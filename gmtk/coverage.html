<html>
  <head>
    <title>pycobertura report</title>
    <meta charset="UTF-8">
    <style>
/*! normalize.css v3.0.2 | MIT License | git.io/normalize */

/**
 * 1. Set default font family to sans-serif.
 * 2. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}

/**
 * Remove default margin.
 */

body {
  margin: 0;
}

/* HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined for any HTML5 element in IE 8/9.
 * Correct `block` display not defined for `details` or `summary` in IE 10/11
 * and Firefox.
 * Correct `block` display not defined for `main` in IE 11.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}

/**
 * 1. Correct `inline-block` display not defined in IE 8/9.
 * 2. Normalize vertical alignment of `progress` in Chrome, Firefox, and Opera.
 */

audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
  display: none;
  height: 0;
}

/**
 * Address `[hidden]` styling not present in IE 8/9/10.
 * Hide the `template` element in IE 8/9/11, Safari, and Firefox < 22.
 */

[hidden],
template {
  display: none;
}

/* Links
   ========================================================================== */

/**
 * Remove the gray background color from active links in IE 10.
 */

a {
  background-color: transparent;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
  outline: 0;
}

/* Text-level semantics
   ========================================================================== */

/**
 * Address styling not present in IE 8/9/10/11, Safari, and Chrome.
 */

abbr[title] {
  border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 4+, Safari, and Chrome.
 */

b,
strong {
  font-weight: bold;
}

/**
 * Address styling not present in Safari and Chrome.
 */

dfn {
  font-style: italic;
}

/**
 * Address variable `h1` font-size and margin within `section` and `article`
 * contexts in Firefox 4+, Safari, and Chrome.
 */

h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

/**
 * Address styling not present in IE 8/9.
 */

mark {
  background: #ff0;
  color: #000;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
  font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}

sup {
  top: -0.5em;
}

sub {
  bottom: -0.25em;
}

/* Embedded content
   ========================================================================== */

/**
 * Remove border when inside `a` element in IE 8/9/10.
 */

img {
  border: 0;
}

/**
 * Correct overflow not hidden in IE 9/10/11.
 */

svg:not(:root) {
  overflow: hidden;
}

/* Grouping content
   ========================================================================== */

/**
 * Address margin not present in IE 8/9 and Safari.
 */

figure {
  margin: 1em 40px;
}

/**
 * Address differences between Firefox and other browsers.
 */

hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

/**
 * Contain overflow in all browsers.
 */

pre {
  overflow: auto;
}

/**
 * Address odd `em`-unit font size rendering in all browsers.
 */

code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

/* Forms
   ========================================================================== */

/**
 * Known limitation: by default, Chrome and Safari on OS X allow very limited
 * styling of `select`, unless a `border` property is set.
 */

/**
 * 1. Correct color not being inherited.
 *    Known issue: affects color of disabled elements.
 * 2. Correct font properties not being inherited.
 * 3. Address margins set differently in Firefox 4+, Safari, and Chrome.
 */

button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}

/**
 * Address `overflow` set to `hidden` in IE 8/9/10/11.
 */

button {
  overflow: visible;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Firefox, IE 8/9/10/11, and Opera.
 * Correct `select` style inheritance in Firefox.
 */

button,
select {
  text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
  cursor: default;
}

/**
 * Remove inner padding and border in Firefox 4+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}

/**
 * Address Firefox 4+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

input {
  line-height: normal;
}

/**
 * It's recommended that you don't attempt to style these elements.
 * Firefox's implementation doesn't respect box-sizing, padding, or width.
 *
 * 1. Address box sizing set to `content-box` in IE 8/9/10.
 * 2. Remove excess padding in IE 8/9/10.
 */

input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}

/**
 * Fix the cursor style for Chrome's increment/decrement buttons. For certain
 * `font-size` values of the `input`, it causes the cursor style of the
 * decrement button to change from `default` to `text`.
 */

input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari and Chrome on OS X.
 * Safari (but not Chrome) clips the cancel button when the search input has
 * padding (and `textfield` appearance).
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct `color` not being inherited in IE 8/9/10/11.
 * 2. Remove padding so people aren't caught out if they zero out fieldsets.
 */

legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}

/**
 * Remove default vertical scrollbar in IE 8/9/10/11.
 */

textarea {
  overflow: auto;
}

/**
 * Don't inherit the `font-weight` (applied by a rule above).
 * NOTE: the default cannot safely be changed in Chrome and Safari on OS X.
 */

optgroup {
  font-weight: bold;
}

/* Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
  border-collapse: collapse;
  border-spacing: 0;
}

td,
th {
  padding: 0;
}
/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/


/* Table of contents
––––––––––––––––––––––––––––––––––––––––––––––––––
- Grid
- Base Styles
- Typography
- Links
- Buttons
- Forms
- Lists
- Code
- Tables
- Spacing
- Utilities
- Clearing
- Media Queries
*/


/* Grid
–––––––––––––––––––––––––––––––––––––––––––––––––– */
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }

/* For devices larger than 400px */
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}

/* For devices larger than 550px */
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}


/* Base Styles
–––––––––––––––––––––––––––––––––––––––––––––––––– */
/* NOTE
html is set to 62.5% so that all the REM measurements throughout Skeleton
are based on 10px sizing. So basically 1.5rem = 15px :) */
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }


/* Typography
–––––––––––––––––––––––––––––––––––––––––––––––––– */
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 4.0rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.6rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.0rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

/* Larger than phablet */
@media (min-width: 550px) {
  h1 { font-size: 5.0rem; }
  h2 { font-size: 4.2rem; }
  h3 { font-size: 3.6rem; }
  h4 { font-size: 3.0rem; }
  h5 { font-size: 2.4rem; }
  h6 { font-size: 1.5rem; }
}

p {
  margin-top: 0; }


/* Links
–––––––––––––––––––––––––––––––––––––––––––––––––– */
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }


/* Buttons
–––––––––––––––––––––––––––––––––––––––––––––––––– */
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }


/* Forms
–––––––––––––––––––––––––––––––––––––––––––––––––– */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }


/* Lists
–––––––––––––––––––––––––––––––––––––––––––––––––– */
ul {
  list-style: circle inside; }
ol {
  list-style: decimal inside; }
ol, ul {
  padding-left: 0;
  margin-top: 0; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li {
  margin-bottom: 1rem; }


/* Code
–––––––––––––––––––––––––––––––––––––––––––––––––– */
code {
  padding: .2rem .5rem;
  margin: 0 .2rem;
  font-size: 90%;
  white-space: nowrap;
  background: #F1F1F1;
  border: 1px solid #E1E1E1;
  border-radius: 4px; }
pre > code {
  display: block;
  padding: 1rem 1.5rem;
  white-space: pre; }


/* Tables
–––––––––––––––––––––––––––––––––––––––––––––––––– */
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }


/* Spacing
–––––––––––––––––––––––––––––––––––––––––––––––––– */
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 2.5rem; }


/* Utilities
–––––––––––––––––––––––––––––––––––––––––––––––––– */
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }


/* Misc
–––––––––––––––––––––––––––––––––––––––––––––––––– */
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }


/* Clearing
–––––––––––––––––––––––––––––––––––––––––––––––––– */

/* Self Clearing Goodness */
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }


/* Media Queries
–––––––––––––––––––––––––––––––––––––––––––––––––– */
/*
Note: The best way to structure the use of media queries is to create the queries
near the relevant code. For example, if you wanted to change the styles for buttons
on small devices, paste the mobile query code up in the buttons section and style it
there.
*/


/* Larger than mobile */
@media (min-width: 400px) {}

/* Larger than phablet (also point when grid becomes active) */
@media (min-width: 550px) {}

/* Larger than tablet */
@media (min-width: 750px) {}

/* Larger than desktop */
@media (min-width: 1000px) {}

/* Larger than Desktop HD */
@media (min-width: 1200px) {}
.hit {background-color: #EAFFEA}
.miss {background-color: #FFECEC}
.container .code {margin-left: 0}
pre {line-height: 1.3}
    </style>
  </head>
  <body>
    <div class="container">
      <table class="u-full-width">
        <thead>
          <tr>
            <th>Filename</th>
            <th>Stmts</th>
            <th>Miss</th>
            <th>Cover</th>
            <th>Missing</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><a href="#src/graph_models/domains/binary_vector_domain.rs">src/graph_models/domains/binary_vector_domain.rs</a></td>
            <td>12</td>
            <td>12</td>
            <td>0.00%</td>
            <td>13-103</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/domains/boolean_vector_domain.rs">src/graph_models/domains/boolean_vector_domain.rs</a></td>
            <td>5</td>
            <td>5</td>
            <td>0.00%</td>
            <td>13-55</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/domains/int_domain.rs">src/graph_models/domains/int_domain.rs</a></td>
            <td>10</td>
            <td>0</td>
            <td>100.00%</td>
            <td></td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/factors/dot_tensor1_factor.rs">src/graph_models/factors/dot_tensor1_factor.rs</a></td>
            <td>16</td>
            <td>7</td>
            <td>56.25%</td>
            <td>38-44</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/factors/group_factor.rs">src/graph_models/factors/group_factor.rs</a></td>
            <td>6</td>
            <td>6</td>
            <td>0.00%</td>
            <td>29-73</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/inferences/belief_propagation/bp.rs">src/graph_models/inferences/belief_propagation/bp.rs</a></td>
            <td>110</td>
            <td>3</td>
            <td>97.27%</td>
            <td>147, 225-232</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/inferences/belief_propagation/bp_dfs.rs">src/graph_models/inferences/belief_propagation/bp_dfs.rs</a></td>
            <td>37</td>
            <td>2</td>
            <td>94.59%</td>
            <td>33, 56</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/inferences/belief_propagation/bp_structure.rs">src/graph_models/inferences/belief_propagation/bp_structure.rs</a></td>
            <td>93</td>
            <td>6</td>
            <td>93.55%</td>
            <td>40, 57, 91, 97-112, 202</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/inferences/brute_force.rs">src/graph_models/inferences/brute_force.rs</a></td>
            <td>49</td>
            <td>5</td>
            <td>89.80%</td>
            <td>77, 94, 109, 117-121</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/models.rs">src/graph_models/models.rs</a></td>
            <td>6</td>
            <td>6</td>
            <td>0.00%</td>
            <td>13-54</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/traits.rs">src/graph_models/traits.rs</a></td>
            <td>2</td>
            <td>0</td>
            <td>100.00%</td>
            <td></td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/weights.rs">src/graph_models/weights.rs</a></td>
            <td>13</td>
            <td>5</td>
            <td>61.54%</td>
            <td>25, 34, 38-47</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/utils/builder.rs">src/graph_models/utils/builder.rs</a></td>
            <td>5</td>
            <td>5</td>
            <td>0.00%</td>
            <td>17-55</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/utils/misc.rs">src/graph_models/utils/misc.rs</a></td>
            <td>54</td>
            <td>1</td>
            <td>98.15%</td>
            <td>11</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/variables/binary_vector_variable.rs">src/graph_models/variables/binary_vector_variable.rs</a></td>
            <td>6</td>
            <td>6</td>
            <td>0.00%</td>
            <td>13-39</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/variables/boolean_vector_variable.rs">src/graph_models/variables/boolean_vector_variable.rs</a></td>
            <td>5</td>
            <td>5</td>
            <td>0.00%</td>
            <td>17-36</td>
          </tr>
          <tr>
            <td><a href="#src/graph_models/variables/int_variable.rs">src/graph_models/variables/int_variable.rs</a></td>
            <td>15</td>
            <td>5</td>
            <td>66.67%</td>
            <td>34-40</td>
          </tr>
          <tr>
            <td><a href="#src/main.rs">src/main.rs</a></td>
            <td>45</td>
            <td>45</td>
            <td>0.00%</td>
            <td>15-86</td>
          </tr>
          <tr>
            <td><a href="#src/utils.rs">src/utils.rs</a></td>
            <td>2</td>
            <td>2</td>
            <td>0.00%</td>
            <td>6-14</td>
          </tr>
          <tr>
            <td><a href="#src/optimization/accumulators.rs">src/optimization/accumulators.rs</a></td>
            <td>13</td>
            <td>13</td>
            <td>0.00%</td>
            <td>10-48</td>
          </tr>
          <tr>
            <td><a href="#src/optimization/batch_example.rs">src/optimization/batch_example.rs</a></td>
            <td>6</td>
            <td>6</td>
            <td>0.00%</td>
            <td>11-57</td>
          </tr>
          <tr>
            <td><a href="#src/optimization/example.rs">src/optimization/example.rs</a></td>
            <td>6</td>
            <td>6</td>
            <td>0.00%</td>
            <td>20-72</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/dense_tensor/indexing.rs">src/tensors/dense_tensor/indexing.rs</a></td>
            <td>49</td>
            <td>28</td>
            <td>42.86%</td>
            <td>19-29, 81-90, 105-202, 226-240</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/dense_tensor/operators.rs">src/tensors/dense_tensor/operators.rs</a></td>
            <td>83</td>
            <td>77</td>
            <td>7.23%</td>
            <td>8-54, 64-74, 85-364, 382-539</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/dense_tensor/serializing.rs">src/tensors/dense_tensor/serializing.rs</a></td>
            <td>24</td>
            <td>24</td>
            <td>0.00%</td>
            <td>14-62</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/dense_tensor/tensor.rs">src/tensors/dense_tensor/tensor.rs</a></td>
            <td>157</td>
            <td>56</td>
            <td>64.33%</td>
            <td>20, 24-26, 36-38, 48-50, 60-62, 84, 120-165, 179-181, 197-211, 221-223, 233, 247, 270-276, 293-326, 356-368, 376, 384, 444, 465</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/tensor_index.rs">src/tensors/tensor_index.rs</a></td>
            <td>71</td>
            <td>62</td>
            <td>12.68%</td>
            <td>22-114, 143-201, 277</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/tensor_type.rs">src/tensors/tensor_type.rs</a></td>
            <td>10</td>
            <td>7</td>
            <td>30.00%</td>
            <td>36, 45, 54-71</td>
          </tr>
          <tr>
            <td><a href="#src/tensors/utils.rs">src/tensors/utils.rs</a></td>
            <td>27</td>
            <td>10</td>
            <td>62.96%</td>
            <td>1-15</td>
          </tr>
          <tr>
            <td><a href="#tests/graph_models/inferences/belief_propagation.rs">tests/graph_models/inferences/belief_propagation.rs</a></td>
            <td>47</td>
            <td>0</td>
            <td>100.00%</td>
            <td></td>
          </tr>
          <tr>
            <td><a href="#tests/graph_models/inferences/brute_force.rs">tests/graph_models/inferences/brute_force.rs</a></td>
            <td>99</td>
            <td>28</td>
            <td>71.72%</td>
            <td>55-66, 161-194</td>
          </tr>
          <tr>
            <td><a href="#tests/tensors/dense_tensor.rs">tests/tensors/dense_tensor.rs</a></td>
            <td>223</td>
            <td>198</td>
            <td>11.21%</td>
            <td>20-362, 389-413</td>
          </tr>
          <tr>
            <td><a href="#tests/tensors/utils.rs">tests/tensors/utils.rs</a></td>
            <td>25</td>
            <td>25</td>
            <td>0.00%</td>
            <td>5-34</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td>TOTAL</td>
            <td>1331</td>
            <td>666</td>
            <td>49.96%</td>
            <td></td>
          </tr>
        </tfoot>
      </table>
<h4 id="src/graph_models/domains/binary_vector_domain.rs">src/graph_models/domains/binary_vector_domain.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std::hash::Hash;
</span><span class="noop">use tensors::*;
</span><span class="noop">use std::collections::HashMap;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="noop">pub struct BinaryVectorValue&lt;U: TensorType=TDefault&gt; {
</span><span class="noop">    pub tensor: DenseTensor&lt;U&gt;,
</span><span class="noop">    pub idx: usize
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Clone for BinaryVectorValue&lt;T&gt; {
</span><span class="miss">    fn clone(&amp;self) -&gt; BinaryVectorValue&lt;T&gt; {
</span><span class="noop">        BinaryVectorValue {
</span><span class="noop">            tensor: self.tensor.clone(),
</span><span class="noop">            idx: self.idx.clone()
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait TBinaryVectorDomain&lt;V: Eq + Hash + Clone, U: TensorType=TDefault&gt;: Domain {
</span><span class="noop">    fn get_category(&amp;self, idx: usize) -&gt; &amp;V;
</span><span class="noop">    fn get_category_index(&amp;self, category: &amp;V) -&gt; usize;
</span><span class="noop">    fn has_value(&amp;self, category: &amp;V) -&gt; bool;
</span><span class="noop">    fn encode_value(&amp;self, category: &amp;V) -&gt; BinaryVectorValue&lt;U&gt;;
</span><span class="noop">    fn get_domain_tensor(&amp;mut self) -&gt; &amp;DenseTensor&lt;U&gt;;
</span><span class="noop">    fn cuda_(&amp;mut self);
</span><span class="noop">    fn cpu_(&amp;mut self);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">pub struct BinaryVectorDomain&lt;V: &#39;static + Eq + Hash + Clone, U: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    pub categories: FnvHashMap&lt;V, usize&gt;,
</span><span class="noop">    inversed_categories: Vec&lt;V&gt;,
</span><span class="noop">    is_sparse: bool,
</span><span class="noop">    domain_tensor: Option&lt;DenseTensor&lt;U&gt;&gt;,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: Eq + Hash + Clone, U: TensorType&gt; BinaryVectorDomain&lt;V, U&gt; {
</span><span class="miss">    pub fn new(cats: Vec&lt;V&gt;) -&gt; BinaryVectorDomain&lt;V, U&gt; {
</span><span class="noop">        let categories: FnvHashMap&lt;V, usize&gt; = cats.iter().enumerate().map(|(idx, v)| (v.clone(), idx)).collect();
</span><span class="noop">
</span><span class="noop">        return BinaryVectorDomain {
</span><span class="noop">            categories,
</span><span class="noop">            inversed_categories: cats,
</span><span class="noop">            is_sparse: false,
</span><span class="noop">            domain_tensor: None,
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: Eq + Hash + Clone, U: TensorType&gt; TBinaryVectorDomain&lt;V, U&gt; for BinaryVectorDomain&lt;V, U&gt; {
</span><span class="miss">    fn get_category(&amp;self, idx: usize) -&gt; &amp;V {
</span><span class="noop">        return &amp;self.inversed_categories[idx];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_category_index(&amp;self, category: &amp;V) -&gt; usize {
</span><span class="noop">        return self.categories[category];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn has_value(&amp;self, category: &amp;V) -&gt; bool {
</span><span class="noop">        return self.categories.contains_key(category);
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn encode_value(&amp;self, category: &amp;V) -&gt; BinaryVectorValue&lt;U&gt; {
</span><span class="noop">        return self.get_value(self.categories[category]);
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_domain_tensor(&amp;mut self) -&gt; &amp;DenseTensor&lt;U&gt; {
</span><span class="noop">        if let None = self.domain_tensor {
</span><span class="noop">            let n_cat: i64 = self.categories.len() as i64;
</span><span class="noop">            let mut domain_tensor = DenseTensor::&lt;U&gt;::create(&amp;[n_cat, n_cat]);
</span><span class="noop">            for i in 0..n_cat {
</span><span class="noop">                domain_tensor.assign(i, self.get_value(i as usize).tensor);
</span><span class="noop">            }
</span><span class="noop">
</span><span class="noop">            self.domain_tensor = Some(domain_tensor);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        return self.domain_tensor.as_ref().unwrap();
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn cuda_(&amp;mut self) {
</span><span class="noop">        self.domain_tensor.as_mut().unwrap().cuda_();
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn cpu_(&amp;mut self) {
</span><span class="noop">        self.domain_tensor.as_mut().unwrap().cpu_();
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: &#39;static + Eq + Hash + Clone, U: &#39;static + TensorType&gt; Domain for BinaryVectorDomain&lt;V, U&gt; {
</span><span class="noop">    type Value = BinaryVectorValue&lt;U&gt;;
</span><span class="noop">
</span><span class="miss">    fn numel(&amp;self) -&gt; usize {
</span><span class="noop">        self.categories.len()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_index(&amp;self, value: &amp;&lt;Self as Domain&gt;::Value) -&gt; usize {
</span><span class="noop">        return value.idx;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_value(&amp;self, index: usize) -&gt; &lt;Self as Domain&gt;::Value {
</span><span class="noop">        let mut tensor = DenseTensor::&lt;U&gt;::zeros(&amp;vec![self.categories.len() as i64]);
</span><span class="noop">        tensor.assign(index as i64, 1.0);
</span><span class="noop">
</span><span class="noop">        return BinaryVectorValue {
</span><span class="noop">            tensor,
</span><span class="noop">            idx: index,
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/domains/boolean_vector_domain.rs">src/graph_models/domains/boolean_vector_domain.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std::hash::Hash;
</span><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::domains::BinaryVectorValue;
</span><span class="noop">use std::collections::HashMap;
</span><span class="noop">use graph_models::traits::Domain;
</span><span class="noop">
</span><span class="noop">pub struct BooleanVectorDomain&lt;T: TensorType=TDefault&gt; {
</span><span class="noop">    values: Vec&lt;BinaryVectorValue&lt;T&gt;&gt;,
</span><span class="noop">    domain_tensor: DenseTensor&lt;T&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; BooleanVectorDomain&lt;T&gt; {
</span><span class="miss">    pub fn new() -&gt; BooleanVectorDomain&lt;T&gt; {
</span><span class="noop">        let mut false_tensor = DenseTensor::&lt;T&gt;::zeros(&amp;vec![2]);
</span><span class="noop">        false_tensor.assign(0, 1.0);
</span><span class="noop">
</span><span class="noop">        let mut true_tensor = DenseTensor::&lt;T&gt;::zeros(&amp;vec![2]);
</span><span class="noop">        true_tensor.assign(1, 1.0);
</span><span class="noop">
</span><span class="noop">        let domain_tensor = DenseTensor::stack_ref(&amp;vec![
</span><span class="noop">            &amp;false_tensor, &amp;true_tensor
</span><span class="noop">        ], 0);
</span><span class="noop">
</span><span class="noop">        return BooleanVectorDomain {
</span><span class="noop">            values: vec![
</span><span class="noop">                BinaryVectorValue {
</span><span class="noop">                    tensor: false_tensor,
</span><span class="noop">                    idx: 0
</span><span class="noop">                },
</span><span class="noop">                BinaryVectorValue {
</span><span class="noop">                    tensor: true_tensor,
</span><span class="noop">                    idx: 1
</span><span class="noop">                },
</span><span class="noop">            ],
</span><span class="noop">            domain_tensor
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_domain_tensor(&amp;self) -&gt; &amp;DenseTensor&lt;T&gt; {
</span><span class="noop">        &amp;self.domain_tensor
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;U: TensorType&gt; Domain for BooleanVectorDomain&lt;U&gt; {
</span><span class="noop">    type Value = BinaryVectorValue&lt;U&gt;;
</span><span class="noop">
</span><span class="miss">    fn numel(&amp;self) -&gt; usize {
</span><span class="noop">        2
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_index(&amp;self, value: &amp;&lt;Self as Domain&gt;::Value) -&gt; usize {
</span><span class="noop">        return value.idx;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_value(&amp;self, idx: usize) -&gt; &lt;Self as Domain&gt;::Value {
</span><span class="noop">        return self.values[idx].clone();
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/domains/int_domain.rs">src/graph_models/domains/int_domain.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::traits::Domain;
</span><span class="noop">
</span><span class="noop">pub struct IntDomain {
</span><span class="noop">    min: i32,
</span><span class="noop">    max: i32,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl IntDomain {
</span><span class="hit">    pub fn new(min: i32, max: i32) -&gt; IntDomain {
</span><span class="hit">        return IntDomain { min, max }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl Domain for IntDomain {
</span><span class="noop">    type Value = i32;
</span><span class="noop">
</span><span class="hit">    fn numel(&amp;self) -&gt; usize {
</span><span class="hit">        (self.max - self.min + 1) as usize
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_index(&amp;self, &amp;value: &amp;&lt;Self as Domain&gt;::Value) -&gt; usize {
</span><span class="hit">        assert!(self.min &lt;= value &amp;&amp; value &lt;= self.max, format!(&#34;Invalid value: {} &lt;= {} &lt;= {}&#34;, self.min, value, self.max));
</span><span class="hit">        return (value - self.min) as usize;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_value(&amp;self, index: usize) -&gt; &lt;Self as Domain&gt;::Value {
</span><span class="hit">        assert!(index &lt;= (self.max - self.min) as usize);
</span><span class="hit">        return self.min + index as i32;
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/factors/dot_tensor1_factor.rs">src/graph_models/factors/dot_tensor1_factor.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use graph_models::weights::Weights;
</span><span class="noop">use graph_models::inferences::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="noop">pub trait DotTensor1Factor&lt;&#39;a, V: &#39;static + Variable, T: &#39;static + TensorType=TDefault&gt;: Factor&lt;&#39;a, V, T&gt;
</span><span class="noop">    where Self: Sized {
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="noop">    fn get_weights(&amp;self) -&gt; &amp;Weights&lt;T&gt;;
</span><span class="noop">    fn get_features_tensor(&amp;self) -&gt; &amp;DenseTensor&lt;T&gt;;
</span><span class="noop">    #[inline]
</span><span class="noop">    fn get_vars_dims(&amp;self) -&gt; &amp;Vec&lt;i64&gt;;
</span><span class="noop">
</span><span class="hit">    fn val2feature_idx(&amp;self, values: &amp;[&amp;V::Value]) -&gt; i64 {
</span><span class="noop">        // Note: you may want to override this function to improve speed
</span><span class="hit">        let vars = self.get_variables();
</span><span class="noop">
</span><span class="noop">        ravel_index(&amp;(0..vars.len())
</span><span class="hit">            .map(|i| vars[i].get_domain().get_index(values[i]) as i64)
</span><span class="noop">            .collect(),
</span><span class="hit">            self.get_vars_dims()
</span><span class="noop">        )
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn impl_get_scores_tensor(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="hit">        self.get_features_tensor().mv(&amp;self.get_weights().get_value())
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn impl_score_assignment(&amp;self, assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;) -&gt; f64 {
</span><span class="hit">        let values: Vec&lt;&amp;V::Value&gt; = self.get_variables().iter().map(|&amp;v| &amp;assignment[&amp;v.get_id()]).collect();
</span><span class="noop">        return self.get_features_tensor()
</span><span class="noop">            .at(slice![self.val2feature_idx(&amp;values) as i64, ;])
</span><span class="hit">            .dot(&amp;self.get_weights().get_value()).get_f64();
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn impl_compute_gradients(&amp;self, target_assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;, inference: &amp;Inference&lt;V, T&gt;) -&gt; Vec&lt;(i64, DenseTensor&lt;T&gt;)&gt; {
</span><span class="miss">        let values: Vec&lt;&amp;V::Value&gt; = self.get_variables().iter().map(|&amp;v| &amp;target_assignment[&amp;v.get_id()]).collect();
</span><span class="miss">        let feature_tensor = self.get_features_tensor();
</span><span class="miss">        let target_idx = self.val2feature_idx(&amp;values) as i64;
</span><span class="miss">        let tensor = inference.log_prob_factor(self);
</span><span class="miss">        return vec![(self.get_weights().id.clone(), feature_tensor.at(slice![target_idx, ;]) -
</span><span class="miss">            tensor.exp().view(&amp;[1, -1]).mm(&amp;feature_tensor).squeeze())];
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/factors/group_factor.rs">src/graph_models/factors/group_factor.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use graph_models::weights::Weights;
</span><span class="noop">use std::collections::HashMap;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">use graph_models::inferences::*;
</span><span class="noop">use graph_models::factors::dot_tensor1_factor::DotTensor1Factor;
</span><span class="noop">use std::collections::HashSet;
</span><span class="noop">use graph_models::utils::get_variables_index;
</span><span class="noop">use utils::RefEquality;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">pub trait SubTensorFactor&lt;&#39;a, V: &#39;static + Variable + Sync, T: TensorType=TDefault&gt; {
</span><span class="noop">    fn score_assignment(&amp;self, assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;) -&gt; f64;
</span><span class="noop">    fn get_scores_tensor(&amp;self) -&gt; &amp;DenseTensor&lt;T&gt;;
</span><span class="noop">    fn compute_gradients(&amp;self, target_assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;, prob_factor: &amp;DenseTensor&lt;T&gt;) -&gt; Vec&lt;(i64, DenseTensor&lt;T&gt;)&gt;;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub struct GroupTensorFactor&lt;&#39;a, V: &#39;static + Variable + Sync, T: TensorType=TDefault&gt; {
</span><span class="noop">    factors: Vec&lt;Box&lt;SubTensorFactor&lt;&#39;a, V, T&gt;&gt;&gt;,
</span><span class="noop">    variables: Vec&lt;&amp;&#39;a V&gt;,
</span><span class="noop">    vars_dims: Vec&lt;i64&gt;,
</span><span class="noop">    variables_set: HashSet&lt;usize&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: &#39;static + Variable + Sync, T: TensorType&gt; GroupTensorFactor&lt;&#39;a, V, T&gt; {
</span><span class="miss">    pub fn new(variables: Vec&lt;&amp;&#39;a V&gt;, factors: Vec&lt;Box&lt;SubTensorFactor&lt;&#39;a, V, T&gt;&gt;&gt;) -&gt; GroupTensorFactor&lt;&#39;a, V, T&gt; {
</span><span class="noop">        let vars_dims: Vec&lt;i64&gt; = variables.iter().map(|v| v.get_domain_size()).collect();
</span><span class="noop">        let variables_set = get_variables_index(&amp;variables);
</span><span class="noop">
</span><span class="noop">        GroupTensorFactor {
</span><span class="noop">            factors,
</span><span class="noop">            variables,
</span><span class="noop">            vars_dims,
</span><span class="noop">            variables_set
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: &#39;static + Variable + Sync, T: TensorType&gt; Factor&lt;&#39;a, V, T&gt; for GroupTensorFactor&lt;&#39;a, V, T&gt; {
</span><span class="miss">    fn get_variables(&amp;self) -&gt; &amp;[&amp;&#39;a V] {
</span><span class="noop">        &amp;self.variables
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn score_assignment&lt;&#39;b&gt;(&amp;self, assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;) -&gt; f64 {
</span><span class="noop">        self.factors.iter().map(|f| f.score_assignment(assignment)).sum()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_scores_tensor(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        if self.factors.len() == 1 {
</span><span class="noop">            self.factors[0].get_scores_tensor().expand(&amp;self.vars_dims).contiguous_().view1()
</span><span class="noop">        } else {
</span><span class="noop">            let mut tensor = DenseTensor::&lt;T&gt;::zeros(&amp;self.vars_dims);
</span><span class="noop">            for f in &amp;self.factors {
</span><span class="noop">                tensor += f.get_scores_tensor();
</span><span class="noop">            }
</span><span class="noop">            tensor.view1()
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn compute_gradients(&amp;self, target_assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;, inference: &amp;Inference&lt;V, T&gt;) -&gt; Vec&lt;(i64, DenseTensor&lt;T&gt;)&gt; {
</span><span class="noop">        let prob_factor = inference.log_prob_factor(self).exp();
</span><span class="noop">        let mut gradients = Vec::with_capacity(self.factors.len());
</span><span class="noop">        for f in &amp;self.factors {
</span><span class="noop">            gradients.append(&amp;mut f.compute_gradients(target_assignment, &amp;prob_factor));
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        return gradients;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn touch(&amp;self, var: &amp;V) -&gt; bool {
</span><span class="noop">        self.variables_set.contains(&amp;var.get_id())
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/inferences/belief_propagation/bp.rs">src/graph_models/inferences/belief_propagation/bp.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
218 &nbsp;
219 &nbsp;
220 &nbsp;
221 &nbsp;
222 &nbsp;
223 &nbsp;
224 &nbsp;
225 &nbsp;
226 &nbsp;
227 &nbsp;
228 &nbsp;
229 &nbsp;
230 &nbsp;
231 &nbsp;
232 &nbsp;
233 &nbsp;
234 &nbsp;
235 &nbsp;
236 &nbsp;
237 &nbsp;
238 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::inferences::InferProb;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use tensors::*;
</span><span class="noop">use rand::prelude::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">use graph_models::inferences::belief_propagation::bp_structure::*;
</span><span class="noop">use graph_models::inferences::belief_propagation::bp_dfs::DFSInfo;
</span><span class="noop">use graph_models::inferences::Inference;
</span><span class="noop">
</span><span class="noop">pub struct BeliefPropagation&lt;&#39;a: &#39;a2, &#39;a2, V: &#39;static + Variable, T: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    pub(super) infer_prob: InferProb,
</span><span class="noop">    pub(super) variables: Vec&lt;BPVariable&gt;,
</span><span class="noop">    pub(super) factors: Vec&lt;BPFactor&lt;T&gt;&gt;,
</span><span class="noop">    pub(super) edges: Vec&lt;BPEdge&lt;T&gt;&gt;,
</span><span class="noop">    pub(super) ex_vars: &amp;&#39;a [V],
</span><span class="noop">    pub(super) ex_factors: &amp;&#39;a2 [Box&lt;Factor&lt;&#39;a, V, T&gt; + &#39;a&gt;],
</span><span class="noop">    pub(super) ex_vars_index: FnvHashMap&lt;usize, usize&gt;,
</span><span class="noop">    pub(super) ex_factors_index: FnvHashMap&lt;usize, usize&gt;,
</span><span class="noop">
</span><span class="noop">    // vector of index of variables (not variable id)
</span><span class="noop">    pub(super) roots: Vec&lt;usize&gt;,
</span><span class="noop">    pub(super) roots_log_z: Vec&lt;DenseTensor&lt;T&gt;&gt;,
</span><span class="noop">    // vector of component index of all nodes in factor graph (including var &amp; factor), the component index start from 1
</span><span class="noop">    // so that we can use usize
</span><span class="noop">    pub(super) connected_components: Vec&lt;usize&gt;,
</span><span class="noop">    pub(super) sending_plans: Vec&lt;Vec&lt;(usize, usize)&gt;&gt;,
</span><span class="noop">    pub(super) has_cycle: bool,
</span><span class="noop">    pub(super) is_inferred: bool
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a: &#39;a2, &#39;a2, V: &#39;static + Variable, T: &#39;static + TensorType&gt; BeliefPropagation&lt;&#39;a, &#39;a2, V, T&gt; {
</span><span class="hit">    pub fn new(infer_prob: InferProb, variables: &amp;&#39;a [V], factors: &amp;&#39;a2 [Box&lt;Factor&lt;&#39;a, V, T&gt; + &#39;a&gt;], seed: u8) -&gt; BeliefPropagation&lt;&#39;a, &#39;a2, V, T&gt; {
</span><span class="hit">        let n_edges: usize = factors.iter().map(|f| f.get_variables().len()).sum();
</span><span class="hit">        let var_index: FnvHashMap&lt;usize, usize&gt; = variables.iter().enumerate().map(|(i, v)| (v.get_id(), i)).collect();
</span><span class="hit">        let fac_index: FnvHashMap&lt;usize, usize&gt; = factors.iter().enumerate().map(|(i, f)| (f.get_id(), i)).collect();
</span><span class="hit">        let _null_component: usize = variables.len() + factors.len() + 99000;
</span><span class="noop">
</span><span class="hit">        let mut bp = BeliefPropagation {
</span><span class="hit">            infer_prob,
</span><span class="hit">            variables: Vec::with_capacity(variables.len()),
</span><span class="hit">            factors: Vec::with_capacity(factors.len()),
</span><span class="hit">            edges: Vec::with_capacity(n_edges),
</span><span class="hit">            ex_vars: variables,
</span><span class="hit">            ex_factors: factors,
</span><span class="hit">            ex_vars_index: var_index,
</span><span class="hit">            ex_factors_index: fac_index,
</span><span class="hit">            roots: Vec::new(),
</span><span class="hit">            roots_log_z: Vec::new(),
</span><span class="hit">            connected_components: vec![_null_component; variables.len() + factors.len()],
</span><span class="hit">            sending_plans: Vec::new(),
</span><span class="noop">            has_cycle: false,
</span><span class="noop">            is_inferred: false,
</span><span class="noop">        };
</span><span class="noop">
</span><span class="hit">        let mut id_counter = 0;
</span><span class="noop">
</span><span class="noop">        // STEP 1: build factor graph: factor, variable &amp; edges
</span><span class="hit">        for factor in factors {
</span><span class="hit">            bp.factors.push(BPFactor::new(id_counter, factor));
</span><span class="hit">            id_counter += 1;
</span><span class="noop">        }
</span><span class="hit">        for var in variables {
</span><span class="hit">            bp.variables.push(BPVariable::new(id_counter, var));
</span><span class="hit">            id_counter += 1;
</span><span class="noop">        }
</span><span class="hit">        for (i, factor) in factors.iter().enumerate() {
</span><span class="hit">            for var in factor.get_variables() {
</span><span class="hit">                let var_idx = bp.ex_vars_index[&amp;var.get_id()];
</span><span class="hit">                bp.add_edge(i, var_idx);
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        // STEP 2: need to find all disconnected components in the graph, because cannot pass message between those components
</span><span class="hit">        let mut dfs = DFSInfo::new(bp.edges.len(), id_counter);
</span><span class="hit">        let (mut n_remained, mut n_connected_nodes, mut root_index) = (0, 0, 1);
</span><span class="noop">
</span><span class="hit">        let mut remained_variables = vec![0; variables.len()];
</span><span class="hit">        let mut rng = StdRng::from_seed([seed; 32]);
</span><span class="hit">        let mut random_idx = 0; //rng.gen_range(0, variables.len());
</span><span class="noop">
</span><span class="hit">        loop {
</span><span class="hit">            let vnode = &amp;bp.variables[random_idx];
</span><span class="hit">            bp.has_cycle = !dfs.dfs_from_var(&amp;bp, -1, vnode, 0) || bp.has_cycle;
</span><span class="noop">            // add roots, connected components, and make sending plans for this connected components
</span><span class="hit">            bp.roots.push(random_idx);
</span><span class="hit">            bp.roots_log_z.push(DenseTensor::&lt;T&gt;::default());
</span><span class="noop">
</span><span class="hit">            n_remained = 0;
</span><span class="hit">            n_connected_nodes = 0;
</span><span class="hit">            for i in 0..factors.len() {
</span><span class="hit">                if dfs.visit_order[i] != -1 {
</span><span class="hit">                    bp.connected_components[i] = root_index;
</span><span class="hit">                    n_connected_nodes += 1;
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            for i in factors.len()..dfs.n_nodes {
</span><span class="hit">                if dfs.visit_order[i] != -1 {
</span><span class="hit">                    bp.connected_components[i] = root_index;
</span><span class="hit">                    n_connected_nodes += 1;
</span><span class="hit">                } else if bp.connected_components[i] == _null_component {
</span><span class="hit">                    remained_variables[n_remained] = i - factors.len();
</span><span class="hit">                    n_remained += 1;
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="noop">
</span><span class="noop">            // add sending plan to send from leaves to root
</span><span class="hit">            let mut sending_plan = Vec::with_capacity(n_connected_nodes - 1);
</span><span class="hit">            for i in 0..dfs.n_nodes {
</span><span class="hit">                if dfs.visit_order[i] &gt; 0 { // 0 is root nodes
</span><span class="hit">                    sending_plan.push((i, dfs.visit_order[i] as usize));
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="hit">            sending_plan.sort_unstable_by(|&amp;a, &amp;b| a.1.cmp(&amp;b.1).reverse());
</span><span class="hit">            for i in 0..(n_connected_nodes - 1) {
</span><span class="hit">                sending_plan[i].1 = dfs.from_edges[sending_plan[i].0] as usize;
</span><span class="noop">            }
</span><span class="hit">            bp.sending_plans.push(sending_plan);
</span><span class="noop">
</span><span class="noop">            // randomly select another one variable to build, also reset tracker
</span><span class="hit">            if n_remained == 0 {
</span><span class="noop">                break;
</span><span class="noop">            }
</span><span class="hit">            random_idx = remained_variables[0]; //rng.gen_range(0, n_remained)];
</span><span class="hit">            dfs.reset();
</span><span class="hit">            root_index += 1;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        bp
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn add_edge(&amp;mut self, factor_idx: usize, var_idx: usize) {
</span><span class="hit">        let edge = BPEdge::new(self.edges.len(), self.variables[var_idx].domain_size, var_idx, factor_idx);
</span><span class="hit">        self.factors[factor_idx].edges.push(edge.id);
</span><span class="hit">        self.variables[var_idx].edges.push(edge.id);
</span><span class="hit">        self.edges.push(edge);
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn update_scores_tensor(&amp;mut self) {
</span><span class="hit">        for (i, factor) in self.ex_factors.iter().enumerate() {
</span><span class="hit">            self.factors[i].score_tensor = factor.get_scores_tensor().view(&amp;self.factors[i].dimensions);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a: &#39;a2, &#39;a2, V: &#39;static + Variable, T: &#39;static + TensorType&gt; Inference&lt;V, T&gt; for BeliefPropagation&lt;&#39;a, &#39;a2, V, T&gt; {
</span><span class="miss">    fn reset_value(&amp;mut self) {
</span><span class="noop">        if !self.is_inferred {
</span><span class="noop">            return;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        for edge in self.edges.iter_mut() {
</span><span class="noop">            edge.reset_value();
</span><span class="noop">        }
</span><span class="noop">        for factor in self.factors.iter_mut() {
</span><span class="noop">            factor.reset_value();
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        self.is_inferred = false;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn infer(&amp;mut self) {
</span><span class="hit">        self.update_scores_tensor();
</span><span class="noop">
</span><span class="hit">        for sending_plain in self.sending_plans.iter() {
</span><span class="noop">            // sending from leaves to root
</span><span class="hit">            for ue_pair in sending_plain.iter() {
</span><span class="hit">                if ue_pair.0 &lt; self.factors.len() {
</span><span class="noop">                    // this is factor node
</span><span class="hit">                    self.factors[ue_pair.0].send_message(&amp;self.infer_prob, &amp;mut self.edges, ue_pair.1);
</span><span class="hit">                    println!(&#34;message2var: {:?} {:?}&#34;, ue_pair, self.edges[ue_pair.1].message2var);
</span><span class="noop">                } else {
</span><span class="hit">                    self.variables[ue_pair.0 - self.factors.len()].send_message(&amp;mut self.edges, ue_pair.1);
</span><span class="hit">                    println!(&#34;message2factor: {:?} {:?}&#34;, ue_pair, self.edges[ue_pair.1].message2factor);
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            if self.infer_prob == InferProb::MARGINAL {
</span><span class="hit">                for ue_pair in sending_plain.iter().rev() {
</span><span class="hit">                    if ue_pair.0 &lt; self.factors.len() {
</span><span class="hit">                        self.variables[self.edges[ue_pair.1].var_idx].send_message(&amp;mut self.edges, ue_pair.1);
</span><span class="hit">                        println!(&#34;message2factor: {:?}&#34;, self.edges[ue_pair.1].message2factor);
</span><span class="noop">                    } else {
</span><span class="hit">                        self.factors[self.edges[ue_pair.1].factor_idx].send_message(&amp;self.infer_prob, &amp;mut self.edges, ue_pair.1);
</span><span class="hit">                        println!(&#34;message2var: {:?}&#34;, self.edges[ue_pair.1].message2var);
</span><span class="noop">                    }
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        for i in 0..self.roots.len() {
</span><span class="hit">            self.roots_log_z[i] = self.variables[self.roots[i]].get_log_z(&amp;self.edges);
</span><span class="hit">            println!(&#34;logZ: {:?}&#34;, self.roots_log_z[i]);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        self.is_inferred = true;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn map(&amp;self) -&gt; FnvHashMap&lt;usize, V::Value&gt; {
</span><span class="hit">        debug_assert!(self.is_inferred &amp;&amp; self.infer_prob == InferProb::MAP);
</span><span class="noop">
</span><span class="hit">        let mut encoded_solution = vec![0; self.variables.len()];
</span><span class="hit">        let mut map_solution: FnvHashMap&lt;usize, V::Value&gt; = Default::default();
</span><span class="noop">
</span><span class="hit">        for i in 0..self.roots.len() {
</span><span class="hit">            println!(&#34;Root: {}, var_id&#34;, self.roots[i]);
</span><span class="hit">            self.variables[self.roots[i]].compute_map(self, &amp;self.sending_plans[i], &amp;mut encoded_solution);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        println!(&#34;solution: {:?}&#34;, encoded_solution);
</span><span class="noop">
</span><span class="hit">        for i in 0..self.variables.len() {
</span><span class="hit">            map_solution.insert(self.ex_vars[i].get_id(), self.ex_vars[i].get_domain().get_value(encoded_solution[i] as usize));
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        map_solution
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn log_z(&amp;self) -&gt; f64 {
</span><span class="hit">        debug_assert!(self.is_inferred &amp;&amp; self.infer_prob == InferProb::MARGINAL);
</span><span class="noop">
</span><span class="hit">        self.roots_log_z.iter().sum::&lt;DenseTensor&lt;T&gt;&gt;().get_f64()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn log_prob_var(&amp;self, var: &amp;V) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        debug_assert!(self.is_inferred);
</span><span class="noop">
</span><span class="noop">        let vidx = self.ex_vars_index[&amp;var.get_id()];
</span><span class="noop">        self.variables[vidx].get_log_belief(&amp;self.edges) - &amp;self.roots_log_z[self.connected_components[vidx + self.factors.len()]]
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn log_prob_factor(&amp;self, factor: &amp;Factor&lt;V, T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        debug_assert!(self.is_inferred);
</span><span class="noop">
</span><span class="noop">        let fidx = self.ex_factors_index[&amp;factor.get_id()];
</span><span class="noop">        self.factors[fidx].get_log_belief(&amp;self.edges) - &amp;self.roots_log_z[self.connected_components[fidx]]
</span><span class="noop">    }
</span><span class="noop">}
</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/inferences/belief_propagation/bp_dfs.rs">src/graph_models/inferences/belief_propagation/bp_dfs.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::inferences::belief_propagation::bp_structure::*;
</span><span class="noop">use tensors::TensorType;
</span><span class="noop">use graph_models::inferences::belief_propagation::bp::BeliefPropagation;
</span><span class="noop">use graph_models::traits::Variable;
</span><span class="noop">
</span><span class="noop">pub struct DFSInfo {
</span><span class="noop">    pub n_nodes: usize,
</span><span class="noop">    pub n_edges: usize,
</span><span class="noop">
</span><span class="noop">    // array of edge id which a node is visited from during DFS, -1 means no edges
</span><span class="noop">    pub from_edges: Vec&lt;i32&gt;,
</span><span class="noop">    // keep order of visiting, -1 mean that a node hasn&#39;t been visited
</span><span class="noop">    pub visit_order: Vec&lt;i32&gt;,
</span><span class="noop">    // keep information whether an edge has been visited or not (0 and 1)
</span><span class="noop">    pub visited_edges: Vec&lt;i32&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl DFSInfo {
</span><span class="hit">    pub fn new(n_edges: usize, n_nodes: usize) -&gt; DFSInfo {
</span><span class="hit">        DFSInfo {
</span><span class="hit">            n_nodes,
</span><span class="hit">            n_edges,
</span><span class="hit">            from_edges: vec![-1; n_nodes],
</span><span class="hit">            visit_order: vec![-1; n_nodes],
</span><span class="hit">            visited_edges: vec![0; n_edges],
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    /// Compute DFS travel path and return true if graph doesn&#39;t have cycle, false otherwise
</span><span class="hit">    pub fn dfs_from_factor&lt;V: Variable, T: TensorType&gt;(&amp;mut self, bp: &amp;BeliefPropagation&lt;V, T&gt;, from_edge_id: i32, node: &amp;BPFactor&lt;T&gt;, order: i32) -&gt; bool {
</span><span class="hit">        if self.visit_order[node.id] != -1 {
</span><span class="noop">            // has cycle
</span><span class="miss">            return false;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        self.visit_order[node.id] = order;
</span><span class="hit">        self.from_edges[node.id] = from_edge_id;
</span><span class="hit">        let mut no_cycle = true;
</span><span class="noop">
</span><span class="hit">        for &amp;eid in &amp;node.edges {
</span><span class="hit">            if self.visited_edges[eid] != 0 {
</span><span class="hit">                continue;
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            self.visited_edges[eid] = 1;
</span><span class="hit">            no_cycle = self.dfs_from_var(bp, eid as i32, &amp;bp.variables[bp.edges[eid].var_idx], order + 1) &amp;&amp; no_cycle;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        return no_cycle;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    /// Compute DFS travel path and return true if graph doesn&#39;t have cycle, false otherwise
</span><span class="hit">    pub fn dfs_from_var&lt;V: Variable, T: TensorType&gt;(&amp;mut self, bp: &amp;BeliefPropagation&lt;V, T&gt;, from_edge_id: i32, node: &amp;BPVariable, order: i32) -&gt; bool {
</span><span class="hit">        if self.visit_order[node.id] != -1 {
</span><span class="noop">            // has cycle
</span><span class="miss">            return false;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        self.visit_order[node.id] = order;
</span><span class="hit">        self.from_edges[node.id] = from_edge_id;
</span><span class="hit">        let mut no_cycle = true;
</span><span class="noop">
</span><span class="hit">        for &amp;eid in &amp;node.edges {
</span><span class="hit">            if self.visited_edges[eid] != 0 {
</span><span class="noop">                // ignored visited path
</span><span class="hit">                continue;
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            self.visited_edges[eid] = 1;
</span><span class="hit">            no_cycle = self.dfs_from_factor(bp, eid as i32, &amp;bp.factors[bp.edges[eid].factor_idx], order + 1) &amp;&amp; no_cycle;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        return no_cycle;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn reset(&amp;mut self) {
</span><span class="hit">        for i in 0..self.n_nodes {
</span><span class="hit">            self.from_edges[i] = -1;
</span><span class="hit">            self.visit_order[i] = -1;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        for i in 0..self.n_edges {
</span><span class="hit">            self.visited_edges[i] = 0;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/inferences/belief_propagation/bp_structure.rs">src/graph_models/inferences/belief_propagation/bp_structure.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::inferences::InferProb;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use tensors::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">use graph_models::inferences::belief_propagation::bp::BeliefPropagation;
</span><span class="noop">
</span><span class="noop">pub struct BPVariable {
</span><span class="noop">    pub id: usize,
</span><span class="noop">    pub domain_size: i64,
</span><span class="noop">    pub edges: Vec&lt;usize&gt;
</span><span class="noop">}
</span><span class="noop">pub struct BPFactor&lt;T: TensorType&gt; {
</span><span class="noop">    pub id: usize,
</span><span class="noop">    pub edges: Vec&lt;usize&gt;,
</span><span class="noop">    pub dimensions: Vec&lt;i64&gt;,
</span><span class="noop">    pub score_tensor: DenseTensor&lt;T&gt;
</span><span class="noop">}
</span><span class="noop">pub struct BPEdge&lt;T: TensorType=TDefault&gt; {
</span><span class="noop">    pub id: usize,
</span><span class="noop">    pub var_idx: usize,
</span><span class="noop">    pub factor_idx: usize,
</span><span class="noop">    pub message2factor: DenseTensor&lt;T&gt;,
</span><span class="noop">    pub message2var: DenseTensor&lt;T&gt;,
</span><span class="noop">    pub trace_message: DenseTensor&lt;TLong&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: &#39;static + TensorType&gt; BPEdge&lt;T&gt; {
</span><span class="hit">    pub fn new(id: usize, var_domain_size: i64, var_idx: usize, factor_idx: usize) -&gt; BPEdge&lt;T&gt; {
</span><span class="hit">        let dims = [var_domain_size];
</span><span class="hit">        BPEdge {
</span><span class="hit">            id,
</span><span class="hit">            var_idx: var_idx,
</span><span class="hit">            factor_idx: factor_idx,
</span><span class="hit">            message2factor: DenseTensor::zeros(&amp;dims),
</span><span class="hit">            message2var: DenseTensor::zeros(&amp;dims),
</span><span class="hit">            trace_message: DenseTensor::zeros(&amp;dims)
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn reset_value(&amp;mut self) {
</span><span class="noop">        self.message2factor.zero_();
</span><span class="noop">        self.message2var.zero_();
</span><span class="noop">        self.trace_message.zero_();
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: &#39;static + TensorType&gt; BPFactor&lt;T&gt; {
</span><span class="hit">    pub fn new&lt;&#39;a, V: &#39;static + Variable&gt;(id: usize, factor: &amp;Box&lt;Factor&lt;&#39;a, V, T&gt; + &#39;a&gt;) -&gt; BPFactor&lt;T&gt; {
</span><span class="hit">        BPFactor {
</span><span class="hit">            id,
</span><span class="hit">            edges: Vec::with_capacity(factor.get_variables().len()),
</span><span class="hit">            dimensions: factor.get_variables().iter().map(|v| v.get_domain_size()).collect(),
</span><span class="hit">            score_tensor: DenseTensor::default(),
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn reset_value(&amp;mut self) {
</span><span class="noop">        self.score_tensor = DenseTensor::default()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn send_message(&amp;self, infer_prob: &amp;InferProb, bp_edges: &amp;mut Vec&lt;BPEdge&lt;T&gt;&gt;, eid: usize) {
</span><span class="hit">        if self.edges.len() == 1 {
</span><span class="hit">            bp_edges[eid].message2var = self.score_tensor.clone_reference();
</span><span class="hit">            return;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        let mut shape = vec![1; self.edges.len()];
</span><span class="hit">        let mut along_dim = 0;
</span><span class="hit">        let mut score_tensor = self.score_tensor.clone();
</span><span class="noop">
</span><span class="hit">        for idx in 0..self.edges.len() {
</span><span class="hit">            if self.edges[idx] == eid {
</span><span class="hit">                along_dim = idx as i64;
</span><span class="hit">                continue;
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            shape[idx] = -1;
</span><span class="hit">            score_tensor += bp_edges[self.edges[idx]].message2factor.view(&amp;shape);
</span><span class="hit">            shape[idx] = 1;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        let along_dim_size = score_tensor.size_in_dim(along_dim);
</span><span class="hit">        if infer_prob == &amp;InferProb::MAP {
</span><span class="noop">            // unbind
</span><span class="noop">            let (message, trace_messages) = score_tensor.unbind(along_dim).into_iter()
</span><span class="hit">                .map(|mut aten| aten.contiguous_().view1().max_in_dim(0, true))
</span><span class="noop">                .unzip();
</span><span class="noop">
</span><span class="hit">            bp_edges[eid].message2var = DenseTensor::&lt;T&gt;::concat(&amp;message, 0);
</span><span class="hit">            bp_edges[eid].trace_message = DenseTensor::&lt;TLong&gt;::stack(&amp;trace_messages, 0);
</span><span class="miss">        } else {
</span><span class="hit">            bp_edges[eid].message2var = score_tensor.swap_axes(0, along_dim).contiguous_().view(&amp;[along_dim_size, -1]).log_sum_exp_2dim(1);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="miss">    pub fn get_log_belief(&amp;self, bp_edges: &amp;Vec&lt;BPEdge&lt;T&gt;&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        let mut shape = vec![1; self.edges.len()];
</span><span class="noop">        shape[0] = -1;
</span><span class="noop">        let mut score_tensor = &amp;self.score_tensor + bp_edges[self.edges[0]].message2factor.view(&amp;shape);
</span><span class="noop">        shape[0] = 1;
</span><span class="noop">
</span><span class="noop">        for i in 1..self.edges.len() {
</span><span class="noop">            shape[i] = -1;
</span><span class="noop">            score_tensor += bp_edges[self.edges[i]].message2factor.view(&amp;shape);
</span><span class="noop">            shape[i] = 1;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        score_tensor
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_log_prob(&amp;self, bp_edges: &amp;Vec&lt;BPEdge&lt;T&gt;&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        let mut log_belief = self.get_log_belief(bp_edges);
</span><span class="noop">        log_belief -= log_belief.log_sum_exp();
</span><span class="noop">        log_belief
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl BPVariable {
</span><span class="hit">    pub fn new&lt;V: Variable&gt;(id: usize, variable: &amp;V) -&gt; BPVariable {
</span><span class="hit">        BPVariable {
</span><span class="hit">            id,
</span><span class="hit">            domain_size: variable.get_domain_size(),
</span><span class="hit">            edges: Vec::new(),
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn send_message&lt;T: TensorType&gt;(&amp;self, bp_edges: &amp;mut Vec&lt;BPEdge&lt;T&gt;&gt;, eid: usize) {
</span><span class="noop">        // when len(edges) = 1 it&#39;s a leaf node, always 0 which is default message
</span><span class="hit">        if self.edges.len() == 2 {
</span><span class="hit">            bp_edges[eid].message2factor = if self.edges[0] == eid {
</span><span class="hit">                bp_edges[self.edges[1]].message2var.clone_reference()
</span><span class="noop">            } else {
</span><span class="hit">                bp_edges[self.edges[0]].message2var.clone_reference()
</span><span class="noop">            };
</span><span class="noop">        } else {
</span><span class="hit">            let mut message2factor = DenseTensor::&lt;T&gt;::zeros_like(&amp;bp_edges[eid].message2var);
</span><span class="hit">            for &amp;e in &amp;self.edges {
</span><span class="hit">                if e != eid {
</span><span class="hit">                    message2factor += &amp;bp_edges[e].message2var;
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            bp_edges[eid].message2factor = message2factor;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn compute_map&lt;V: Variable, T: TensorType&gt;(&amp;self, bp: &amp;BeliefPropagation&lt;V, T&gt;, sending_plan: &amp;Vec&lt;(usize, usize)&gt;, solution: &amp;mut [i64]) {
</span><span class="hit">        let log_prob = self.get_log_belief::&lt;T&gt;(&amp;bp.edges);
</span><span class="hit">        let mut dimensions = [0; 50];
</span><span class="hit">        let mut n_dim = 0;
</span><span class="noop">
</span><span class="hit">        let max_and_arg = log_prob.max_in_dim(0, false);
</span><span class="hit">        solution[self.id - bp.factors.len()] = max_and_arg.1.get_i64();
</span><span class="hit">        println!(&#34;update sol {} with {:?}, {}&#34;, self.id, max_and_arg.1, max_and_arg.1.get_i64());
</span><span class="hit">        println!(&#34;Solution: {:?}&#34;, solution);
</span><span class="noop">
</span><span class="hit">        for ue_pair in sending_plan.iter().rev() {
</span><span class="hit">            if ue_pair.0 &lt; bp.factors.len() {
</span><span class="noop">                // only back-prop through factor nodes to get argmax(y) of f(x, y)
</span><span class="hit">                let edge = &amp;bp.edges[ue_pair.1];
</span><span class="hit">                let factor = &amp;bp.factors[edge.factor_idx];
</span><span class="noop">
</span><span class="hit">                if factor.edges.len() == 1 {
</span><span class="hit">                    continue;
</span><span class="noop">                }
</span><span class="noop">
</span><span class="noop">                // unravel to set trace message correctly!!!
</span><span class="hit">                n_dim = 0;
</span><span class="hit">                for &amp;e in &amp;factor.edges {
</span><span class="hit">                    if e != ue_pair.1 {
</span><span class="hit">                        dimensions[n_dim] = bp.variables[bp.edges[e].var_idx].domain_size;
</span><span class="hit">                        n_dim += 1;
</span><span class="noop">                    }
</span><span class="noop">                }
</span><span class="noop">
</span><span class="hit">                let idx = edge.trace_message.at(solution[edge.var_idx]).get_i64();
</span><span class="hit">                println!(&#34;Edge-trace: {:?}&#34;, edge.trace_message);
</span><span class="hit">                let argmax = unravel_index_ptr(idx, &amp;dimensions, n_dim);
</span><span class="hit">                println!(&#34;idx = {}, dimensions: {:?}&#34;, idx, &amp;dimensions[..n_dim]);
</span><span class="hit">                println!(&#34;argmax = {:?}&#34;, argmax);
</span><span class="hit">                n_dim = 0;
</span><span class="hit">                for &amp;e in &amp;factor.edges {
</span><span class="hit">                    if e != ue_pair.1 {
</span><span class="hit">                        solution[bp.edges[e].var_idx] = argmax[n_dim];
</span><span class="hit">                        n_dim += 1;
</span><span class="noop">                    }
</span><span class="noop">                }
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        println!(&#34;Solution: {:?}&#34;, solution);
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="hit">    pub fn get_log_z&lt;T: TensorType&gt;(&amp;self, bp_edges: &amp;Vec&lt;BPEdge&lt;T&gt;&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="hit">        self.get_log_belief(bp_edges).log_sum_exp()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="noop">    #[allow(dead_code)]
</span><span class="miss">    pub fn get_log_prob&lt;T: TensorType&gt;(&amp;self, bp_edges: &amp;Vec&lt;BPEdge&lt;T&gt;&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        let mut log_belief = self.get_log_belief(bp_edges);
</span><span class="noop">        log_belief -= &amp;log_belief.log_sum_exp();
</span><span class="noop">        log_belief
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="hit">    pub fn get_log_belief&lt;T: TensorType&gt;(&amp;self, bp_edges: &amp;Vec&lt;BPEdge&lt;T&gt;&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="hit">        let mut log_prob = bp_edges[self.edges[0]].message2var.clone();
</span><span class="hit">        for i in 1..self.edges.len() {
</span><span class="hit">            log_prob += &amp;bp_edges[self.edges[i]].message2var;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        log_prob
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/inferences/brute_force.rs">src/graph_models/inferences/brute_force.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::inferences::InferProb;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use tensors::*;
</span><span class="noop">use rand::prelude::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">use graph_models::inferences::Inference;
</span><span class="noop">use std::collections::HashSet;
</span><span class="noop">use std::f64;
</span><span class="noop">use graph_models::utils::misc::iter_assignment;
</span><span class="noop">
</span><span class="noop">pub struct BruteForce&lt;&#39;a: &#39;a2, &#39;a2, V: &#39;static + Variable&gt; {
</span><span class="noop">    variables: &amp;&#39;a [V],
</span><span class="noop">    factors: &amp;&#39;a2 [Box&lt;Factor&lt;&#39;a, V, TDouble&gt; + &#39;a&gt;],
</span><span class="noop">    log_z: Option&lt;f64&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a: &#39;a2, &#39;a2, V: Variable&gt; BruteForce&lt;&#39;a, &#39;a2, V&gt; {
</span><span class="hit">    pub fn new(variables: &amp;&#39;a [V], factors: &amp;&#39;a2 [Box&lt;Factor&lt;&#39;a, V, TDouble&gt; + &#39;a&gt;]) -&gt; BruteForce&lt;&#39;a, &#39;a2, V&gt; {
</span><span class="hit">        BruteForce {
</span><span class="hit">            variables,
</span><span class="hit">            factors,
</span><span class="hit">            log_z: None
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn all_map(&amp;self) -&gt; Vec&lt;FnvHashMap&lt;usize, V::Value&gt;&gt; {
</span><span class="hit">        let mut max_score = -f64::INFINITY;
</span><span class="hit">        let mut map_sols = Vec::new();
</span><span class="noop">
</span><span class="hit">        iter_assignment(&amp;self.variables.iter().collect::&lt;Vec&lt;&amp;V&gt;&gt;(), |current_idx, assignment| {
</span><span class="noop">            let score: f64 = self.factors.iter()
</span><span class="hit">                .map(|f| f.score_assignment(assignment))
</span><span class="noop">                .sum();
</span><span class="noop">
</span><span class="hit">            if score &gt; max_score {
</span><span class="hit">                map_sols = vec![current_idx.clone()];
</span><span class="hit">                max_score = score;
</span><span class="hit">            } else if score == max_score {
</span><span class="hit">                map_sols.push(current_idx.clone());
</span><span class="noop">            }
</span><span class="noop">        });
</span><span class="noop">
</span><span class="hit">        map_sols.iter().map(|current_idx| {
</span><span class="noop">            current_idx.iter().enumerate()
</span><span class="hit">                .map(|(i, &amp;idx)| (self.variables[i].get_id(), self.variables[i].get_domain().get_value(idx as usize)))
</span><span class="noop">                .collect::&lt;FnvHashMap&lt;usize, V::Value&gt;&gt;()
</span><span class="hit">        }).collect()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn marginal_log_prob(&amp;self, vars: &amp;[&amp;V]) -&gt; DenseTensor&lt;TDouble&gt; {
</span><span class="hit">        let _factor_var_ids: HashSet&lt;usize&gt; = vars.iter().map(|v| v.get_id()).collect();
</span><span class="hit">        let mut scores = DenseTensor::&lt;TDouble&gt;::create(
</span><span class="hit">            &amp;vars.iter().map(|v| v.get_domain_size()).collect::&lt;Vec&lt;i64&gt;&gt;());
</span><span class="noop">        let mut remained_variables: Vec&lt;&amp;V&gt; = self.variables.iter()
</span><span class="hit">            .filter(|v| !_factor_var_ids.contains(&amp;v.get_id()))
</span><span class="noop">            .collect();
</span><span class="noop">
</span><span class="hit">        iter_assignment(vars, |current_index, assignment| {
</span><span class="hit">            let mut score = Vec::new();
</span><span class="hit">            if remained_variables.len() &gt; 0 {
</span><span class="hit">                iter_assignment(&amp;remained_variables, |_temp, r_ass| {
</span><span class="hit">                    let mut new_assignment: FnvHashMap&lt;usize, V::Value&gt; = Default::default();
</span><span class="hit">                    for (&amp;k, v) in r_ass.iter() {
</span><span class="hit">                        new_assignment.insert(k, v.clone());
</span><span class="noop">                    }
</span><span class="hit">                    for (&amp;k, v) in assignment.iter() {
</span><span class="hit">                        new_assignment.insert(k, v.clone());
</span><span class="noop">                    }
</span><span class="noop">                    // it&#39;s weird that rust compiler doesn&#39;t support this
</span><span class="noop">//                    r_ass.extend(assignment.iter());
</span><span class="noop">                    score.push(self.factors.iter()
</span><span class="hit">                        .map(|f| f.score_assignment(&amp;new_assignment))
</span><span class="noop">                        .sum());
</span><span class="noop">                });
</span><span class="noop">            } else {
</span><span class="noop">                score.push(self.factors.iter()
</span><span class="miss">                    .map(|f| f.score_assignment(assignment))
</span><span class="noop">                    .sum());
</span><span class="noop">            }
</span><span class="noop">
</span><span class="noop">//            let mm = ;
</span><span class="noop">//            println!(&#34;pre-set score = {:?}&#34;, mm);
</span><span class="hit">            scores.assign(current_index, DenseTensor::&lt;TDouble&gt;::borrow_from_array(&amp;score).log_sum_exp() - self.log_z.unwrap());
</span><span class="noop">//            println!(&#34;scores = {:?}&#34;, scores.at(current_index));
</span><span class="noop">        });
</span><span class="noop">
</span><span class="noop">//        println!(&#34;final results = {:?}&#34;, scores.to_1darray());
</span><span class="noop">
</span><span class="hit">        scores
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a: &#39;a2, &#39;a2, V: Variable&gt; Inference&lt;V, TDouble&gt; for BruteForce&lt;&#39;a, &#39;a2, V&gt; {
</span><span class="miss">    fn reset_value(&amp;mut self) {}
</span><span class="noop">
</span><span class="hit">    fn infer(&amp;mut self) {
</span><span class="hit">        let mut scores: Vec&lt;f64&gt; = Vec::with_capacity(self.variables.len());
</span><span class="hit">        let vars: Vec&lt;&amp;V&gt; = self.variables.iter().collect();
</span><span class="hit">        iter_assignment(&amp;vars, |_, assignment| {
</span><span class="noop">            let score = self.factors.iter()
</span><span class="hit">                .map(|f| f.score_assignment(assignment))
</span><span class="noop">                .sum();
</span><span class="hit">            scores.push(score);
</span><span class="noop">        });
</span><span class="noop">
</span><span class="hit">        self.log_z = Some(DenseTensor::&lt;TDouble&gt;::borrow_from_array(&amp;scores).log_sum_exp().get_f64());
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn map(&amp;self) -&gt; FnvHashMap&lt;usize, &lt;V as Variable&gt;::Value&gt; {
</span><span class="noop">        unimplemented!()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn log_z(&amp;self) -&gt; f64 {
</span><span class="hit">        self.log_z.unwrap()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn log_prob_var(&amp;self, var: &amp;V) -&gt; DenseTensor&lt;TDouble&gt; {
</span><span class="noop">        unimplemented!()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn log_prob_factor(&amp;self, factor: &amp;Factor&lt;V, TDouble&gt;) -&gt; DenseTensor&lt;TDouble&gt; {
</span><span class="noop">        self.marginal_log_prob(factor.get_variables())
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/models.rs">src/graph_models/models.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::traits::FactorTemplate;
</span><span class="noop">use tensors::*;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use graph_models::weights::Weights;
</span><span class="noop">use std::rc::Rc;
</span><span class="noop">
</span><span class="noop">pub struct LogLinearModel&lt;E, V: Variable, T: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    templates: Vec&lt;Box&lt;FactorTemplate&lt;E, V, T&gt;&gt;&gt;,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;E, V: Variable, T: &#39;static + TensorType&gt; LogLinearModel&lt;E, V, T&gt; {
</span><span class="miss">    pub fn new(templates: Vec&lt;Box&lt;FactorTemplate&lt;E, V, T&gt;&gt;&gt;) -&gt; LogLinearModel&lt;E, V, T&gt; {
</span><span class="noop">        LogLinearModel { templates }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_factors&lt;&#39;a: &#39;a1, &#39;a1&gt;(&amp;&#39;a self, example: &amp;&#39;a1 E) -&gt; Vec&lt;Box&lt;Factor&lt;&#39;a1, V, T&gt; + &#39;a1&gt;&gt; {
</span><span class="noop">        let mut factors = Vec::with_capacity(8);
</span><span class="noop">        for template in &amp;self.templates {
</span><span class="noop">            factors.append(&amp;mut template.unroll(&amp;example));
</span><span class="noop">        }
</span><span class="noop">        return factors;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_parameters(&amp;self) -&gt; Vec&lt;&amp;Weights&lt;T&gt;&gt; {
</span><span class="noop">        let mut parameters = Vec::with_capacity(self.templates.len());
</span><span class="noop">        for template in &amp;self.templates {
</span><span class="noop">            for weight in template.get_weights() {
</span><span class="noop">                parameters.push(weight);
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">        return parameters;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn set_parameters(&amp;mut self, new_parameters: &amp;[Weights&lt;T&gt;]) {
</span><span class="noop">        let mut i = 0;
</span><span class="noop">        for template in &amp;self.templates {
</span><span class="noop">            for weights in template.get_weights() {
</span><span class="noop">                assert_eq!(weights.id, new_parameters[i].id);
</span><span class="noop">                weights.copy_(&amp;new_parameters[i]);
</span><span class="noop">                i += 1;
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cuda_(&amp;self) {
</span><span class="noop">        for template in &amp;self.templates {
</span><span class="noop">            for weight in template.get_weights() {
</span><span class="noop">                weight.cuda_();
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cpu_(&amp;self) {
</span><span class="noop">        for template in &amp;self.templates {
</span><span class="noop">            for weight in template.get_weights() {
</span><span class="noop">                weight.cpu_();
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/traits.rs">src/graph_models/traits.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use tensors::*;
</span><span class="noop">use std::collections::HashMap;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">use graph_models::weights::Weights;
</span><span class="noop">use utils::RefEquality;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">use std::rc::Rc;
</span><span class="noop">use graph_models::inferences::*;
</span><span class="noop">
</span><span class="noop">pub trait Domain {
</span><span class="noop">    type Value;
</span><span class="noop">
</span><span class="noop">    fn numel(&amp;self) -&gt; usize;
</span><span class="noop">    fn get_index(&amp;self, value: &amp;Self::Value) -&gt; usize;
</span><span class="noop">    fn get_value(&amp;self, index: usize) -&gt; Self::Value;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait Variable {
</span><span class="noop">    type Value: Clone;
</span><span class="noop">
</span><span class="noop">    /// Return a unique id, which will be used to hash variables
</span><span class="noop">    fn get_id(&amp;self) -&gt; usize;
</span><span class="noop">    fn get_domain_size(&amp;self) -&gt; i64;
</span><span class="noop">    fn get_domain(&amp;self) -&gt; &amp;Domain&lt;Value=Self::Value&gt;;
</span><span class="noop">    fn set_value(&amp;mut self, val: Self::Value) -&gt; &amp;Self;
</span><span class="noop">    fn get_value(&amp;self) -&gt; &amp;Self::Value;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait LabeledVariable: Variable {
</span><span class="noop">    fn get_label_value(&amp;self) -&gt; &amp;Self::Value;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait Factor&lt;&#39;a, V: Variable, T: TensorType=TDefault&gt; {
</span><span class="hit">    fn get_id(&amp;self) -&gt; usize {
</span><span class="hit">        ((self as *const _) as *const usize) as usize
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    fn get_variables(&amp;self) -&gt; &amp;[&amp;&#39;a V];
</span><span class="noop">    fn score_assignment(&amp;self, assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;) -&gt; f64;
</span><span class="noop">    fn get_scores_tensor(&amp;self) -&gt; DenseTensor&lt;T&gt;;
</span><span class="noop">    fn compute_gradients(&amp;self, target_assignment: &amp;FnvHashMap&lt;usize, V::Value&gt;, inference: &amp;Inference&lt;V, T&gt;) -&gt; Vec&lt;(i64, DenseTensor&lt;T&gt;)&gt;;
</span><span class="noop">    fn touch(&amp;self, var: &amp;V)-&gt; bool;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait FactorTemplate&lt;E, V: Variable, T: TensorType=TDefault&gt; {
</span><span class="noop">    fn get_weights(&amp;self) -&gt; &amp;[Weights&lt;T&gt;];
</span><span class="noop">    fn unroll&lt;&#39;a: &#39;a1, &#39;a1&gt;(&amp;&#39;a self, example: &amp;&#39;a1 E) -&gt; Vec&lt;Box&lt;Factor&lt;&#39;a1, V, T&gt; + &#39;a1&gt;&gt;;
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/weights.rs">src/graph_models/weights.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use tensors::*;
</span><span class="noop">use uuid::Uuid;
</span><span class="noop">use std::cell::RefCell;
</span><span class="noop">use std::cell::Ref;
</span><span class="noop">use std::ops::Deref;
</span><span class="noop">
</span><span class="noop">pub struct Weights&lt;T: TensorType=TDefault&gt; {
</span><span class="noop">    pub id: i64,
</span><span class="noop">    val: RefCell&lt;DenseTensor&lt;T&gt;&gt;,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">static mut ID_COUNTER: i64 = 0;
</span><span class="hit">fn get_id() -&gt; i64 {
</span><span class="noop">    unsafe {
</span><span class="hit">        ID_COUNTER += 1;
</span><span class="hit">        ID_COUNTER
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Weights&lt;T&gt; {
</span><span class="hit">    pub fn new(val: DenseTensor&lt;T&gt;) -&gt; Weights&lt;T&gt; {
</span><span class="hit">        Weights { val: RefCell::new(val), id: get_id() }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn new_with_id(val: DenseTensor&lt;T&gt;, id: i64) -&gt; Weights&lt;T&gt; {
</span><span class="noop">        Weights { val: RefCell::new(val), id }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn clone(&amp;self) -&gt; Weights&lt;T&gt; {
</span><span class="hit">        Weights { val: RefCell::new(self.val.borrow().clone()), id: self.id }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline(always)]
</span><span class="miss">    pub fn get_value(&amp;self) -&gt; Ref&lt;DenseTensor&lt;T&gt;&gt; {
</span><span class="hit">        return self.val.borrow();
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn copy_(&amp;self, weight: &amp;Weights&lt;T&gt;) {
</span><span class="noop">        // TODO: fix me!! we should borrow mut instead of immutable
</span><span class="noop">        self.val.borrow_mut().copy_(weight.val.borrow().deref());
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cuda_(&amp;self) {
</span><span class="noop">        self.val.borrow_mut().cuda_();
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cpu_(&amp;self) {
</span><span class="noop">        self.val.borrow_mut().cpu_();
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/utils/builder.rs">src/graph_models/utils/builder.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::traits::Variable;
</span><span class="noop">use std::collections::HashSet;
</span><span class="noop">use num_traits::*;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">use std::hash::Hasher;
</span><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::*;
</span><span class="noop">use std::ops::AddAssign;
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">pub struct ObservedFeaturesBuilder&lt;V: &#39;static + Eq + Hash + Clone, T: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    active_categories: Vec&lt;V&gt;,
</span><span class="noop">    active_values: Vec&lt;T::PrimitiveType&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: &#39;static + Eq + Hash + Clone, T: &#39;static + TensorType&gt; ObservedFeaturesBuilder&lt;V, T&gt; {
</span><span class="miss">    pub fn new() -&gt; ObservedFeaturesBuilder&lt;V, T&gt; {
</span><span class="noop">        ObservedFeaturesBuilder {
</span><span class="noop">            active_categories: Vec::new(),
</span><span class="noop">            active_values: Vec::new(),
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn create_domain(builders: &amp;[ObservedFeaturesBuilder&lt;V, T&gt;]) -&gt; BinaryVectorDomain&lt;V, T&gt; {
</span><span class="noop">        let mut catset: HashSet&lt;&amp;V&gt; = HashSet::new();
</span><span class="noop">
</span><span class="noop">        for builder in builders {
</span><span class="noop">            for cat in &amp;builder.active_categories {
</span><span class="noop">                catset.insert(cat);
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        let cats: Vec&lt;V&gt; = catset.into_iter().map(|v| v.clone()).collect();
</span><span class="noop">        BinaryVectorDomain::new(cats)
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn create_tensor(&amp;self, domain: &amp;BinaryVectorDomain&lt;V, T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        let mut val = DenseTensor::zeros(&amp;vec![domain.numel() as i64]);
</span><span class="noop">        let idx: Vec&lt;i64&gt; = self.active_categories.iter()
</span><span class="noop">            .map(|c| domain.get_category_index(c) as i64).collect();
</span><span class="noop">
</span><span class="noop">        val.assign(&amp;idx, &amp;self.active_values);
</span><span class="noop">        val
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: &#39;static + Eq + Hash + Clone, T: &#39;static + TensorType&gt; AddAssign&lt;(V, f64)&gt; for ObservedFeaturesBuilder&lt;V, T&gt; {
</span><span class="miss">    fn add_assign(&amp;mut self, rhs: (V, f64)) {
</span><span class="noop">        self.active_categories.push(rhs.0);
</span><span class="noop">        self.active_values.push(T::PrimitiveType::from_f64(rhs.1).unwrap());
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: &#39;static + Eq + Hash + Clone, T: &#39;static + TensorType&gt; AddAssign&lt;(V, f32)&gt; for ObservedFeaturesBuilder&lt;V, T&gt; {
</span><span class="miss">    fn add_assign(&amp;mut self, rhs: (V, f32)) {
</span><span class="noop">        self.active_categories.push(rhs.0);
</span><span class="noop">        self.active_values.push(T::PrimitiveType::from_f32(rhs.1).unwrap());
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/utils/misc.rs">src/graph_models/utils/misc.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::traits::Variable;
</span><span class="noop">use std::collections::HashSet;
</span><span class="noop">use num_traits::*;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">use std::hash::Hasher;
</span><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::*;
</span><span class="noop">use std::ops::AddAssign;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="miss">pub fn get_variables_index&lt;&#39;a, V: Variable&gt;(variables: &amp;[&amp;&#39;a V]) -&gt; HashSet&lt;usize&gt; {
</span><span class="noop">    let mut vars_set = HashSet::with_capacity(variables.len());
</span><span class="noop">    for v in variables {
</span><span class="noop">        vars_set.insert(v.get_id());
</span><span class="noop">    }
</span><span class="noop">    vars_set
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">/// Iter all values using counter method and apply a function, the produce
</span><span class="hit">pub fn iter_values&lt;V: &#39;static + Variable, F&gt;(vars: &amp;[&amp;V], mut func: F)
</span><span class="noop">    where F: FnMut(i64, &amp;Vec&lt;i64&gt;, &amp;Vec&lt;V::Value&gt;) -&gt; ()
</span><span class="noop">{
</span><span class="hit">    let max_val_index: Vec&lt;i64&gt; = vars.iter().map(|v| v.get_domain_size()).collect();
</span><span class="hit">    let mut current_val = Vec::with_capacity(vars.len());
</span><span class="hit">    let mut current_ravelled_index: i64 = -1;
</span><span class="hit">    let mut current_val_index: Vec&lt;i64&gt; = vec![0; vars.len()];
</span><span class="hit">    current_val_index[vars.len() - 1] = -1;
</span><span class="noop">
</span><span class="noop">    // set default assignment to index 0
</span><span class="hit">    for (i, var) in vars.iter().enumerate() {
</span><span class="hit">        current_val.push(var.get_domain().get_value(0));
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    let mut no_more_vals = false;
</span><span class="noop">
</span><span class="hit">    loop {
</span><span class="noop">        // iterate through each assignment
</span><span class="hit">        let mut i = current_val_index.len() - 1;
</span><span class="hit">        current_ravelled_index += 1;
</span><span class="hit">        loop {
</span><span class="noop">            // move to next state &amp; set value of variables to next state value
</span><span class="hit">            current_val_index[i] += 1;
</span><span class="hit">            if current_val_index[i] == max_val_index[i] {
</span><span class="hit">                current_val_index[i] = 0;
</span><span class="hit">                current_val[i] = vars[i].get_domain().get_value(current_val_index[i] as usize);
</span><span class="hit">                if i == 0 {
</span><span class="hit">                    no_more_vals = true;
</span><span class="hit">                    break;
</span><span class="noop">                }
</span><span class="noop">
</span><span class="hit">                i -= 1;
</span><span class="noop">            } else {
</span><span class="hit">                current_val[i] = vars[i].get_domain().get_value(current_val_index[i] as usize);
</span><span class="hit">                break;
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        if no_more_vals {
</span><span class="noop">            // already iterated through all values
</span><span class="noop">            break;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        func(current_ravelled_index, &amp;current_val_index, &amp;current_val);
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">/// Iter all assignment using counter method and apply a function
</span><span class="hit">pub fn iter_assignment&lt;V: &#39;static + Variable, F&gt;(vars: &amp;[&amp;V], mut func: F)
</span><span class="noop">    where F: FnMut(&amp;Vec&lt;i64&gt;, &amp;mut FnvHashMap&lt;usize, V::Value&gt;) -&gt; ()
</span><span class="noop">{
</span><span class="hit">    let max_val_index: Vec&lt;i64&gt; = vars.iter().map(|v| v.get_domain_size()).collect();
</span><span class="noop">
</span><span class="hit">    let mut current_val = Vec::with_capacity(vars.len());
</span><span class="hit">    let mut target_assignment: FnvHashMap&lt;usize, V::Value&gt; = Default::default();
</span><span class="noop">
</span><span class="hit">    let mut current_val_index: Vec&lt;i64&gt; = vec![0; vars.len()];
</span><span class="hit">    current_val_index[vars.len() - 1] = -1;
</span><span class="noop">
</span><span class="hit">    let current_val_ptr = &amp;mut current_val as *mut Vec&lt;V::Value&gt;;
</span><span class="noop">
</span><span class="noop">    // set default assignment to index 0
</span><span class="hit">    for (i, var) in vars.iter().enumerate() {
</span><span class="hit">        unsafe { (&amp;mut *current_val_ptr).push(var.get_domain().get_value(0)); }
</span><span class="hit">        target_assignment.insert(var.get_id(), var.get_domain().get_value(0));
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    let mut no_more_vals = false;
</span><span class="noop">
</span><span class="hit">    loop {
</span><span class="noop">        // iterate through each assignment
</span><span class="hit">        let mut i = current_val_index.len() - 1;
</span><span class="hit">        loop {
</span><span class="noop">            // move to next state &amp; set value of variables to next state value
</span><span class="hit">            current_val_index[i] += 1;
</span><span class="hit">            if current_val_index[i] == max_val_index[i] {
</span><span class="hit">                current_val_index[i] = 0;
</span><span class="noop">                unsafe {
</span><span class="hit">                    (&amp;mut *current_val_ptr)[i] = vars[i].get_domain().get_value(current_val_index[i] as usize);
</span><span class="noop">                }
</span><span class="hit">                target_assignment.insert(vars[i].get_id(), vars[i].get_domain().get_value(current_val_index[i] as usize));
</span><span class="hit">                if i == 0 {
</span><span class="noop">                    // already iterated through all values
</span><span class="hit">                    no_more_vals = true;
</span><span class="hit">                    break;
</span><span class="noop">                }
</span><span class="noop">
</span><span class="hit">                i -= 1;
</span><span class="noop">            } else {
</span><span class="noop">                unsafe {
</span><span class="hit">                    (&amp;mut *current_val_ptr)[i] = vars[i].get_domain().get_value(current_val_index[i] as usize);
</span><span class="noop">                }
</span><span class="hit">                target_assignment.insert(vars[i].get_id(), vars[i].get_domain().get_value(current_val_index[i] as usize));
</span><span class="hit">                break;
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        if no_more_vals {
</span><span class="noop">            // already iterated through all values
</span><span class="noop">            break;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="hit">        func(&amp;current_val_index, &amp;mut target_assignment);
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/variables/binary_vector_variable.rs">src/graph_models/variables/binary_vector_variable.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use tensors::*;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">use graph_models::domains::*;
</span><span class="noop">use std::hash::Hasher;
</span><span class="noop">
</span><span class="noop">pub struct BinaryVectorVariable&lt;&#39;a, V: &#39;static + Eq + Hash + Clone, T: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    domain: &amp;&#39;a BinaryVectorDomain&lt;V, T&gt;,
</span><span class="noop">    value: BinaryVectorValue&lt;T&gt;,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: Eq + Hash + Clone, U: TensorType&gt; BinaryVectorVariable&lt;&#39;a, V, U&gt; {
</span><span class="miss">    pub fn set_value_by_category(&amp;mut self, val: &amp;V) -&gt; &amp;Self {
</span><span class="noop">        self.value = self.domain.encode_value(val);
</span><span class="noop">        return self;
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: Eq + Hash + Clone, T: TensorType&gt; Variable for BinaryVectorVariable&lt;&#39;a, V, T&gt; {
</span><span class="noop">    type Value = BinaryVectorValue&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn get_id(&amp;self) -&gt; usize {
</span><span class="noop">        (self as *const _) as usize
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_domain_size(&amp;self) -&gt; i64 {
</span><span class="noop">        self.domain.numel() as i64
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_domain(&amp;self) -&gt; &amp;Domain&lt;Value=Self::Value&gt; {
</span><span class="noop">        self.domain
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn set_value(&amp;mut self, val: &lt;Self as Variable&gt;::Value) -&gt; &amp;Self {
</span><span class="noop">        self.value = val;
</span><span class="noop">        return self;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_value(&amp;self) -&gt; &amp;&lt;Self as Variable&gt;::Value {
</span><span class="noop">        return &amp;self.value;
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/variables/boolean_vector_variable.rs">src/graph_models/variables/boolean_vector_variable.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std::hash::Hash;
</span><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::domains::BinaryVectorValue;
</span><span class="noop">use graph_models::domains::BooleanVectorDomain;
</span><span class="noop">use graph_models::traits::Variable;
</span><span class="noop">use graph_models::traits::Domain;
</span><span class="noop">use std::hash::Hasher;
</span><span class="noop">
</span><span class="noop">pub struct BooleanVectorVariable&lt;&#39;a, U: &#39;static + TensorType + Sized = TDefault&gt; {
</span><span class="noop">    domain: &amp;&#39;a BooleanVectorDomain&lt;U&gt;,
</span><span class="noop">    value: BinaryVectorValue&lt;U&gt;,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, U: TensorType&gt; Variable for BooleanVectorVariable&lt;&#39;a, U&gt; {
</span><span class="noop">    type Value = BinaryVectorValue&lt;U&gt;;
</span><span class="noop">
</span><span class="miss">    fn get_id(&amp;self) -&gt; usize {
</span><span class="noop">        (self as *const _) as usize
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline(always)]
</span><span class="miss">    fn get_domain_size(&amp;self) -&gt; i64 {
</span><span class="noop">        2
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline(always)]
</span><span class="miss">    fn get_domain(&amp;self) -&gt; &amp;Domain&lt;Value=BinaryVectorValue&lt;U&gt;&gt; {
</span><span class="noop">        self.domain
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn set_value(&amp;mut self, val: &lt;Self as Variable&gt;::Value) -&gt; &amp;Self {
</span><span class="noop">        self.value = val;
</span><span class="noop">        return self;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_value(&amp;self) -&gt; &amp;&lt;Self as Variable&gt;::Value {
</span><span class="noop">        return &amp;self.value;
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/graph_models/variables/int_variable.rs">src/graph_models/variables/int_variable.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::domains::IntDomain;
</span><span class="noop">use graph_models::traits::*;
</span><span class="noop">
</span><span class="noop">pub struct IntVariable {
</span><span class="noop">    domain: IntDomain,
</span><span class="noop">    value: i32
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl IntVariable {
</span><span class="hit">    pub fn new(domain: IntDomain, val: i32) -&gt; IntVariable {
</span><span class="hit">        IntVariable {
</span><span class="hit">            domain,
</span><span class="hit">            value: val
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl Variable for IntVariable {
</span><span class="noop">    type Value = i32;
</span><span class="noop">
</span><span class="hit">    fn get_id(&amp;self) -&gt; usize {
</span><span class="hit">        (self as *const _) as usize
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_domain_size(&amp;self) -&gt; i64 {
</span><span class="hit">        self.domain.numel() as i64
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_domain(&amp;self) -&gt; &amp;Domain&lt;Value=&lt;Self as Variable&gt;::Value&gt; {
</span><span class="hit">        &amp;self.domain
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn set_value(&amp;mut self, val: &lt;Self as Variable&gt;::Value) -&gt; &amp;Self {
</span><span class="miss">        self.value = val;
</span><span class="miss">        self
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn get_value(&amp;self) -&gt; &amp;&lt;Self as Variable&gt;::Value {
</span><span class="miss">        &amp;self.value
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/main.rs">src/main.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">extern crate gmtk;
</span><span class="noop">extern crate libc;
</span><span class="noop">extern crate bincode;
</span><span class="noop">extern crate time;
</span><span class="noop">extern crate fnv;
</span><span class="noop">
</span><span class="noop">use gmtk::tensors::*;
</span><span class="noop">use std::time::{Duration, Instant};
</span><span class="noop">use std::collections::HashSet;
</span><span class="noop">use time::precise_time_ns;
</span><span class="noop">use gmtk::graph_models::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="noop">
</span><span class="miss">fn a1st(ys: &amp;[DenseTensor], n_loop: usize) {
</span><span class="miss">    let mut total = 0.0;
</span><span class="miss">    let start = precise_time_ns();
</span><span class="miss">    for _ in 0..n_loop {
</span><span class="miss">        let mut result = DenseTensor::zeros_like(&amp;ys[0]);
</span><span class="miss">        for y in ys {
</span><span class="miss">            result += y;
</span><span class="noop">        }
</span><span class="miss">        total += result.sum().get_f64();
</span><span class="noop">    }
</span><span class="miss">    let end = precise_time_ns();
</span><span class="miss">    println!(&#34;1st approach: {:.5}s -- result: {}&#34;, (end - start) as f64 * 1e-9, total);
</span><span class="noop">}
</span><span class="noop">
</span><span class="miss">fn a2nd(ys: &amp;[DenseTensor], n_loop: usize) {
</span><span class="miss">    let mut total = 0.0;
</span><span class="miss">    let start = precise_time_ns();
</span><span class="miss">    for _ in 0..n_loop {
</span><span class="miss">        let mut result = &amp;ys[0] + &amp;ys[1];
</span><span class="miss">        for i in 2..ys.len() {
</span><span class="miss">            result += &amp;ys[i];
</span><span class="noop">        }
</span><span class="miss">        total += result.sum().get_f64();
</span><span class="noop">    }
</span><span class="miss">    let end = precise_time_ns();
</span><span class="miss">    println!(&#34;2nd approach: {:.5}s -- result: {}&#34;, (end - start) as f64 * 1e-9, total);
</span><span class="noop">}
</span><span class="noop">
</span><span class="miss">fn a3rd(ys: &amp;[DenseTensor], n_loop: usize) {
</span><span class="miss">    let mut total = 0.0;
</span><span class="miss">    let start = precise_time_ns();
</span><span class="miss">    for _ in 0..n_loop {
</span><span class="miss">        let mut result = &amp;ys[0] + &amp;ys[1];
</span><span class="miss">        for i in 2..ys.len() {
</span><span class="miss">            result = &amp;result + &amp;ys[i];
</span><span class="noop">        }
</span><span class="miss">        total += result.sum().get_f64();
</span><span class="noop">    }
</span><span class="miss">    let end = precise_time_ns();
</span><span class="miss">    println!(&#34;3rd approach: {:.5}s -- result: {}&#34;, (end - start) as f64 * 1e-9, total);
</span><span class="noop">}
</span><span class="noop">
</span><span class="miss">fn a4th(ys: &amp;[DenseTensor], n_loop: usize) {
</span><span class="miss">    let mut total = 0.0;
</span><span class="miss">    let start = precise_time_ns();
</span><span class="miss">    for _ in 0..n_loop {
</span><span class="miss">        let mut result = &amp;ys[0] + 0.0;
</span><span class="miss">        for i in 1..ys.len() {
</span><span class="miss">            result = &amp;result + &amp;ys[i];
</span><span class="noop">        }
</span><span class="miss">        total += result.sum().get_f64();
</span><span class="noop">    }
</span><span class="miss">    let end = precise_time_ns();
</span><span class="miss">    println!(&#34;4th approach: {:.5}s -- result: {}&#34;, (end - start) as f64 * 1e-9, total);
</span><span class="noop">}
</span><span class="noop">
</span><span class="miss">fn main() {
</span><span class="noop">//    let ys: Vec&lt;DenseTensor&gt; = (0..10).map(|_i| DenseTensor::create_randn(&amp;[1000])).collect();
</span><span class="noop">//    let n_loop = 100;
</span><span class="noop">//    a1st(&amp;ys, n_loop);
</span><span class="noop">//    a2nd(&amp;ys, n_loop);
</span><span class="noop">//    a3rd(&amp;ys, n_loop);
</span><span class="noop">//    a4th(&amp;ys, n_loop);
</span><span class="noop">//
</span><span class="noop">//    a1st(&amp;ys, n_loop);
</span><span class="noop">//    a2nd(&amp;ys, n_loop);
</span><span class="noop">//    a3rd(&amp;ys, n_loop);
</span><span class="noop">//    a4th(&amp;ys, n_loop);
</span><span class="miss">    let v = DenseTensor::&lt;TFloat&gt;::from_array(&amp;[1.0, 2.0, 3.0]);
</span><span class="miss">    let mut empty = DenseTensor::default();
</span><span class="miss">    empty += v;
</span><span class="miss">    println!(&#34;&gt;&gt;&gt;&gt; FINISH!!!&#34;);
</span><span class="noop">}
</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/utils.rs">src/utils.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std;
</span><span class="noop">
</span><span class="noop">pub struct RefEquality&lt;&#39;a, T: &#39;a&gt;(pub &amp;&#39;a T);
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T&gt; std::hash::Hash for RefEquality&lt;&#39;a, T&gt; {
</span><span class="miss">    fn hash&lt;H&gt;(&amp;self, state: &amp;mut H)
</span><span class="noop">        where H: std::hash::Hasher
</span><span class="noop">    {
</span><span class="noop">        (self.0 as *const T).hash(state)
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, &#39;b, T&gt; PartialEq&lt;RefEquality&lt;&#39;b, T&gt;&gt; for RefEquality&lt;&#39;a, T&gt; {
</span><span class="miss">    fn eq(&amp;self, other: &amp;RefEquality&lt;T&gt;) -&gt; bool {
</span><span class="noop">        self.0 as *const T == other.0 as *const T
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T&gt; Eq for RefEquality&lt;&#39;a, T&gt; {}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/optimization/accumulators.rs">src/optimization/accumulators.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std::collections::HashMap;
</span><span class="noop">use tensors::*;
</span><span class="noop">use std::hash::Hash;
</span><span class="noop">
</span><span class="noop">pub struct ValueAccumulator {
</span><span class="noop">    value: f64
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl ValueAccumulator {
</span><span class="miss">    pub fn new() -&gt; ValueAccumulator {
</span><span class="miss">        ValueAccumulator { value: 0.0 }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn clear(&amp;mut self) {
</span><span class="miss">        self.value = 0.0;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_value(&amp;self) -&gt; f64 {
</span><span class="miss">        self.value
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn accumulate(&amp;mut self, value: f64) {
</span><span class="miss">        self.value += value;
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub struct Tensor1AccumulatorDict&lt;V: Eq + Hash, T: TensorType&gt; {
</span><span class="noop">    tensors: HashMap&lt;V, DenseTensor&lt;T&gt;&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;V: Eq + Hash, T: TensorType&gt; Tensor1AccumulatorDict&lt;V, T&gt; {
</span><span class="miss">    pub fn track_object(&amp;mut self, obj: V, default_tensor: &amp;DenseTensor&lt;T&gt;) {
</span><span class="noop">        self.tensors.insert(obj, DenseTensor::&lt;T&gt;::zeros_like(default_tensor));
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn accumulate(&amp;mut self, obj: &amp;V, value: &amp;DenseTensor&lt;T&gt;) {
</span><span class="noop">        *self.tensors.get_mut(obj).unwrap() += value;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn accumulate_minus(&amp;mut self, obj: &amp;V, value: &amp;DenseTensor&lt;T&gt;) {
</span><span class="noop">        *self.tensors.get_mut(obj).unwrap() -= value;
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_value(&amp;self, obj: &amp;V) -&gt; &amp;DenseTensor&lt;T&gt; {
</span><span class="noop">        return &amp;self.tensors[obj];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn clear(&amp;mut self) {
</span><span class="noop">        for (k, v) in self.tensors.iter_mut() {
</span><span class="noop">            v.zero_();
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/optimization/batch_example.rs">src/optimization/batch_example.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::traits::*;
</span><span class="noop">use optimization::example::NLLExample;
</span><span class="noop">use tensors::*;
</span><span class="noop">use optimization::accumulators::*;
</span><span class="noop">
</span><span class="noop">pub struct BatchNLLExample&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType&gt; {
</span><span class="noop">    examples: Vec&lt;&amp;&#39;a mut NLLExample&lt;&#39;a, V, T&gt;&gt;,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType&gt; BatchNLLExample&lt;&#39;a, V, T&gt; {
</span><span class="miss">    pub fn new(examples: &amp;&#39;a mut [NLLExample&lt;&#39;a, V, T&gt;]) -&gt; BatchNLLExample&lt;&#39;a, V, T&gt; {
</span><span class="noop">        let mut batch_examples = Vec::new();
</span><span class="noop">        for e in examples {
</span><span class="noop">            batch_examples.push(e);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        BatchNLLExample { examples: batch_examples }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn split_random(examples: &amp;&#39;a mut [NLLExample&lt;&#39;a, V, T&gt;], batch_size: usize) -&gt; Vec&lt;BatchNLLExample&lt;&#39;a, V, T&gt;&gt; {
</span><span class="noop">        let mut batch_examples = Vec::new();
</span><span class="noop">
</span><span class="noop">        for chunk in examples.chunks_mut(batch_size) {
</span><span class="noop">            let mut batch_example = Vec::new();
</span><span class="noop">            for e in chunk {
</span><span class="noop">                batch_example.push(e);
</span><span class="noop">            }
</span><span class="noop">            batch_examples.push(BatchNLLExample { examples: batch_example });
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">        batch_examples
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn accumulate_value_and_gradient(&amp;mut self, loss_value: &amp;mut ValueAccumulator, gradient_accum: &amp;mut Tensor1AccumulatorDict&lt;i64, T&gt;) {
</span><span class="noop">        for example in &amp;mut self.examples {
</span><span class="noop">            example.accumulate_value_and_gradient(loss_value, gradient_accum);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub struct ParallelBatchNLLExample&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType&gt; {
</span><span class="noop">    batch_example: BatchNLLExample&lt;&#39;a, V, T&gt;,
</span><span class="noop">    n_threads: u32
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType&gt; ParallelBatchNLLExample&lt;&#39;a, V, T&gt; {
</span><span class="miss">    pub fn new(examples: &amp;&#39;a mut [NLLExample&lt;&#39;a, V, T&gt;], n_threads: u32) -&gt; ParallelBatchNLLExample&lt;&#39;a, V, T&gt; {
</span><span class="noop">        ParallelBatchNLLExample { batch_example: BatchNLLExample::new(examples), n_threads }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn split_random(examples: &amp;&#39;a mut [NLLExample&lt;&#39;a, V, T&gt;], batch_size: usize, n_threads: u32) -&gt; Vec&lt;ParallelBatchNLLExample&lt;&#39;a, V, T&gt;&gt; {
</span><span class="noop">        BatchNLLExample::split_random(examples, batch_size).into_iter()
</span><span class="noop">            .map(|v| ParallelBatchNLLExample { batch_example: v, n_threads }).collect()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn accumulate_value_and_gradient(&amp;mut self, loss_value: &amp;mut ValueAccumulator, gradient_accum: &amp;mut Tensor1AccumulatorDict&lt;String, T&gt;) {
</span><span class="noop">        unimplemented!()
</span><span class="noop">//        let mut pool = Pool::new(self.n_threads);
</span><span class="noop">//        pool.scoped(|scoped| {
</span><span class="noop">//            for example in &amp;mut self.batch_example.examples {
</span><span class="noop">//                scoped.execute(move || {
</span><span class="noop">//                    example.accumulate_value_and_gradient(loss_value, gradient_accum);
</span><span class="noop">//                });
</span><span class="noop">//            }
</span><span class="noop">//        });
</span><span class="noop">//        for example in &amp;mut self.batch_example.examples {
</span><span class="noop">//            example.accumulate_value_and_gradient(loss_value, gradient_accum);
</span><span class="noop">//        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/optimization/example.rs">src/optimization/example.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use graph_models::traits::LabeledVariable;
</span><span class="noop">use graph_models::traits::Factor;
</span><span class="noop">use std::collections::HashMap;
</span><span class="noop">use utils::RefEquality;
</span><span class="noop">use optimization::accumulators::*;
</span><span class="noop">use tensors::*;
</span><span class="noop">use graph_models::inferences::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">/// https://stackoverflow.com/questions/32300132/why-cant-i-store-a-value-and-a-reference-to-that-value-in-the-same-struct
</span><span class="noop">pub struct NLLExample&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    variables: &amp;&#39;a [V],
</span><span class="noop">    target_assignment: FnvHashMap&lt;usize, V::Value&gt;,
</span><span class="noop">    factors: &amp;&#39;a [Box&lt;Factor&lt;&#39;a, V, T&gt;&gt;],
</span><span class="noop">    inference: Box&lt;Inference&lt;V, T&gt;&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType&gt; NLLExample&lt;&#39;a, V, T&gt; {
</span><span class="miss">    pub fn new(variables: &amp;&#39;a [V], factors: &amp;&#39;a [Box&lt;Factor&lt;&#39;a, V, T&gt;&gt;], inference: Box&lt;Inference&lt;V, T&gt;&gt;) -&gt; NLLExample&lt;&#39;a, V, T&gt; {
</span><span class="noop">        let target_assignment: FnvHashMap&lt;usize, V::Value&gt; = variables.iter().map(|v| (v.get_id(), v.get_label_value().clone())).collect();
</span><span class="noop">        NLLExample {
</span><span class="noop">            variables,
</span><span class="noop">            factors,
</span><span class="noop">            target_assignment,
</span><span class="noop">            inference
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn accumulate_value_and_gradient(&amp;mut self, loss_value: &amp;mut ValueAccumulator, gradient_accum: &amp;mut Tensor1AccumulatorDict&lt;i64, T&gt;) {
</span><span class="noop">        self.inference.reset_value();
</span><span class="noop">        self.real_accumulate_value_and_gradient(loss_value, gradient_accum);
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline(always)]
</span><span class="miss">    fn real_accumulate_value_and_gradient(&amp;mut self, loss_value: &amp;mut ValueAccumulator, gradient_accum: &amp;mut Tensor1AccumulatorDict&lt;i64, T&gt;) {
</span><span class="noop">        loss_value.accumulate(self.inference.log_z());
</span><span class="noop">
</span><span class="noop">        for factor in self.factors {
</span><span class="noop">            loss_value.accumulate(-factor.score_assignment(&amp;self.target_assignment));
</span><span class="noop">
</span><span class="noop">            for (weight_id, gradient) in factor.compute_gradients(&amp;self.target_assignment, self.inference.as_ref()) {
</span><span class="noop">                gradient_accum.accumulate_minus(&amp;weight_id, &amp;gradient);
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub struct MAPExample&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType=TDefault&gt; {
</span><span class="noop">    variables: &amp;&#39;a [V],
</span><span class="noop">    target_assignment: HashMap&lt;RefEquality&lt;&#39;a, V&gt;, &amp;&#39;a V::Value&gt;,
</span><span class="noop">    factors: &amp;&#39;a [Box&lt;Factor&lt;&#39;a, V, T&gt;&gt;],
</span><span class="noop">    inference: Box&lt;Inference&lt;V, T&gt;&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, V: &#39;static + LabeledVariable + Sync, T: &#39;static + TensorType&gt; MAPExample&lt;&#39;a, V, T&gt; {
</span><span class="miss">    pub fn new(variables: &amp;&#39;a [V], factors: &amp;&#39;a [Box&lt;Factor&lt;&#39;a, V, T&gt;&gt;], inference: Box&lt;Inference&lt;V, T&gt;&gt;) -&gt; MAPExample&lt;&#39;a, V, T&gt; {
</span><span class="noop">        let target_assignment: HashMap&lt;RefEquality&lt;&#39;a, V&gt;, &amp;&#39;a V::Value&gt; = variables.iter().map(|v| (RefEquality(v), v.get_label_value())).collect();
</span><span class="noop">        MAPExample {
</span><span class="noop">            variables,
</span><span class="noop">            factors,
</span><span class="noop">            target_assignment,
</span><span class="noop">            inference
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_map_assignment(&amp;mut self, loss_value: ValueAccumulator, gradient_accum: Tensor1AccumulatorDict&lt;String, T&gt;) -&gt; FnvHashMap&lt;usize, V::Value&gt; {
</span><span class="noop">        self.inference.reset_value();
</span><span class="noop">        return self.inference.map();
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_target_assignment(&amp;self) -&gt; &amp;HashMap&lt;RefEquality&lt;&#39;a, V&gt;, &amp;&#39;a V::Value&gt; {
</span><span class="noop">        &amp;self.target_assignment
</span><span class="noop">    }
</span><span class="noop">}
</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/dense_tensor/indexing.rs">src/tensors/dense_tensor/indexing.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
218 &nbsp;
219 &nbsp;
220 &nbsp;
221 &nbsp;
222 &nbsp;
223 &nbsp;
224 &nbsp;
225 &nbsp;
226 &nbsp;
227 &nbsp;
228 &nbsp;
229 &nbsp;
230 &nbsp;
231 &nbsp;
232 &nbsp;
233 &nbsp;
234 &nbsp;
235 &nbsp;
236 &nbsp;
237 &nbsp;
238 &nbsp;
239 &nbsp;
240 &nbsp;
241 &nbsp;
242 &nbsp;
243 &nbsp;
244 &nbsp;
245 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use super::c_api::*;
</span><span class="noop">use super::tensor::DenseTensor;
</span><span class="noop">use tensors::AdvancedSlice;
</span><span class="noop">use tensors::tensor_index::*;
</span><span class="noop">use tensors::tensor_type::*;
</span><span class="noop">use std::ops::Index;
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; TensorIndex&lt;i64&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="hit">    fn at(&amp;self, idx: i64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor::new(cten_select(self.tensor, 0, idx));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorIndex&lt;&amp;&#39;a Vec&lt;i64&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="miss">    fn at(&amp;self, idx: &amp;&#39;a Vec&lt;i64&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_index_select_v(self.tensor, 0, idx.len() as i64, idx.as_ptr()));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorIndex&lt;Slice&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="miss">    fn at(&amp;self, idx: Slice) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_slice(self.tensor, 0, idx.start, idx.end, idx.step));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorIndex&lt;Vec&lt;AdvancedSlice&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="hit">    fn at(&amp;self, indices: Vec&lt;AdvancedSlice&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor::new(cten_advance_access_index(self.tensor, indices.len() as i32, indices.as_ptr()));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorIndex&lt;(i64, i64)&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="hit">    fn at(&amp;self, idx: (i64, i64)) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor::new(cten_select_along_dims(self.tensor, 2, [idx.0, idx.1].as_ptr()));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;i64, f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: i64, val: f64) {
</span><span class="hit">        unsafe { cten_select_fill_(self.tensor, 0, idx, val); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;i64, DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: i64, val: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            // our default is non-blocking
</span><span class="hit">            cten_select_copy_(self.tensor, 0, idx, val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;i64, Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: i64, val: Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            // our default is non-blocking
</span><span class="hit">            cten_select_copy_(self.tensor, 0, idx, DenseTensor::&lt;T&gt;::borrow_from_array(&amp;val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;i64, &amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: i64, val: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            // our default is non-blocking
</span><span class="hit">            cten_select_copy_(self.tensor, 0, idx, val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;i64, &amp;&#39;a Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: i64, val: &amp;&#39;a Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            // our default is non-blocking
</span><span class="noop">            cten_select_copy_(self.tensor, 0, idx, DenseTensor::&lt;T&gt;::borrow_from_array(val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;&amp;&#39;a Vec&lt;i64&gt;, f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: &amp;&#39;a Vec&lt;i64&gt;, val: f64) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_index_fill_val_(self.tensor, 0, DenseTensor::&lt;TLong&gt;::borrow_from_array(idx).tensor, val);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;&amp;&#39;a Vec&lt;i64&gt;, DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: &amp;&#39;a Vec&lt;i64&gt;, val: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="hit">            cten_index_fill_tensor_(self.tensor, 0,
</span><span class="hit">                                    DenseTensor::&lt;TLong&gt;::borrow_from_array(idx).tensor, val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;&amp;&#39;a Vec&lt;i64&gt;, Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: &amp;&#39;a Vec&lt;i64&gt;, val: Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_index_fill_tensor_(self.tensor, 0,
</span><span class="noop">                                    DenseTensor::&lt;TLong&gt;::borrow_from_array(idx).tensor,
</span><span class="noop">                                    DenseTensor::&lt;T&gt;::borrow_from_array(&amp;val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, &#39;a2, T: TensorType&gt; TensorAssign&lt;&amp;&#39;a2 Vec&lt;i64&gt;, &amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: &amp;&#39;a2 Vec&lt;i64&gt;, val: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_index_fill_tensor_(self.tensor, 0,
</span><span class="noop">                                    DenseTensor::&lt;TLong&gt;::borrow_from_array(idx).tensor, val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, &#39;a2, T: TensorType&gt; TensorAssign&lt;&amp;&#39;a2 Vec&lt;i64&gt;, &amp;&#39;a Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: &amp;&#39;a2 Vec&lt;i64&gt;, val: &amp;&#39;a Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_index_fill_tensor_(self.tensor, 0,
</span><span class="noop">                                    DenseTensor::&lt;TLong&gt;::borrow_from_array(idx).tensor,
</span><span class="noop">                                    DenseTensor::&lt;T&gt;::borrow_from_array(val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;Slice, f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: Slice, val: f64) {
</span><span class="noop">        unsafe { cten_slice_fill_(self.tensor, 0, idx.start, idx.end, idx.step, val); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;Slice, DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: Slice, val: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_slice_copy_(self.tensor, 0, idx.start, idx.end, idx.step, val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;Slice, Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: Slice, val: Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_slice_copy_(self.tensor, 0, idx.start, idx.end, idx.step, DenseTensor::&lt;T&gt;::borrow_from_array(&amp;val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;Slice, &amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: Slice, val: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_slice_copy_(self.tensor, 0, idx.start, idx.end, idx.step, val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;Slice, &amp;&#39;a Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: Slice, val: &amp;&#39;a Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_slice_copy_(self.tensor, 0, idx.start, idx.end, idx.step, DenseTensor::&lt;T&gt;::borrow_from_array(val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;Vec&lt;AdvancedSlice&gt;, f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, indices: Vec&lt;AdvancedSlice&gt;, val: f64) {
</span><span class="noop">        unsafe {
</span><span class="miss">            let aten = cten_advance_access_index(self.tensor, indices.len() as i32, indices.as_ptr());
</span><span class="miss">            cten_fill_(aten, val);
</span><span class="miss">            cten_drop_tensor(aten);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;Vec&lt;AdvancedSlice&gt;, DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, indices: Vec&lt;AdvancedSlice&gt;, val: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="miss">            let aten = cten_advance_access_index(self.tensor, indices.len() as i32, indices.as_ptr());
</span><span class="miss">            cten_copy_(aten, val.tensor, false);
</span><span class="miss">            cten_drop_tensor(aten);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;Vec&lt;AdvancedSlice&gt;, Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, indices: Vec&lt;AdvancedSlice&gt;, val: Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            let aten = cten_advance_access_index(self.tensor, indices.len() as i32, indices.as_ptr());
</span><span class="noop">            cten_copy_(aten, DenseTensor::&lt;T&gt;::borrow_from_array(&amp;val).tensor, false);
</span><span class="noop">            cten_drop_tensor(aten);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;Vec&lt;AdvancedSlice&gt;, &amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, indices: Vec&lt;AdvancedSlice&gt;, val: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            let aten = cten_advance_access_index(self.tensor, indices.len() as i32, indices.as_ptr());
</span><span class="noop">            cten_copy_(aten, val.tensor, false);
</span><span class="noop">            cten_drop_tensor(aten);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;Vec&lt;AdvancedSlice&gt;, &amp;&#39;a Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, indices: Vec&lt;AdvancedSlice&gt;, val: &amp;&#39;a Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            let aten = cten_advance_access_index(self.tensor, indices.len() as i32, indices.as_ptr());
</span><span class="noop">            cten_copy_(aten, DenseTensor::&lt;T&gt;::borrow_from_array(val).tensor, false);
</span><span class="noop">            cten_drop_tensor(aten);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;(i64, i64), f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: (i64, i64), val: f64) {
</span><span class="noop">        unsafe {
</span><span class="hit">            cten_select_along_dims_fill_(self.tensor, 2, [idx.0, idx.1].as_ptr(), val);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;(i64, i64), DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn assign(&amp;mut self, idx: (i64, i64), val: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="hit">            cten_select_along_dims_copy_(self.tensor, 2, [idx.0, idx.1].as_ptr(), val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; TensorAssign&lt;(i64, i64), Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: (i64, i64), val: Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_select_along_dims_copy_(self.tensor, 2, [idx.0, idx.1].as_ptr(), DenseTensor::&lt;T&gt;::borrow_from_array(&amp;val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;(i64, i64), &amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: (i64, i64), val: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_select_along_dims_copy_(self.tensor, 2, [idx.0, idx.1].as_ptr(), val.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; TensorAssign&lt;(i64, i64), &amp;&#39;a Vec&lt;T::PrimitiveType&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn assign(&amp;mut self, idx: (i64, i64), val: &amp;&#39;a Vec&lt;T::PrimitiveType&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_select_along_dims_copy_(self.tensor, 2, [idx.0, idx.1].as_ptr(), DenseTensor::&lt;T&gt;::borrow_from_array(val).tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/dense_tensor/operators.rs">src/tensors/dense_tensor/operators.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
218 &nbsp;
219 &nbsp;
220 &nbsp;
221 &nbsp;
222 &nbsp;
223 &nbsp;
224 &nbsp;
225 &nbsp;
226 &nbsp;
227 &nbsp;
228 &nbsp;
229 &nbsp;
230 &nbsp;
231 &nbsp;
232 &nbsp;
233 &nbsp;
234 &nbsp;
235 &nbsp;
236 &nbsp;
237 &nbsp;
238 &nbsp;
239 &nbsp;
240 &nbsp;
241 &nbsp;
242 &nbsp;
243 &nbsp;
244 &nbsp;
245 &nbsp;
246 &nbsp;
247 &nbsp;
248 &nbsp;
249 &nbsp;
250 &nbsp;
251 &nbsp;
252 &nbsp;
253 &nbsp;
254 &nbsp;
255 &nbsp;
256 &nbsp;
257 &nbsp;
258 &nbsp;
259 &nbsp;
260 &nbsp;
261 &nbsp;
262 &nbsp;
263 &nbsp;
264 &nbsp;
265 &nbsp;
266 &nbsp;
267 &nbsp;
268 &nbsp;
269 &nbsp;
270 &nbsp;
271 &nbsp;
272 &nbsp;
273 &nbsp;
274 &nbsp;
275 &nbsp;
276 &nbsp;
277 &nbsp;
278 &nbsp;
279 &nbsp;
280 &nbsp;
281 &nbsp;
282 &nbsp;
283 &nbsp;
284 &nbsp;
285 &nbsp;
286 &nbsp;
287 &nbsp;
288 &nbsp;
289 &nbsp;
290 &nbsp;
291 &nbsp;
292 &nbsp;
293 &nbsp;
294 &nbsp;
295 &nbsp;
296 &nbsp;
297 &nbsp;
298 &nbsp;
299 &nbsp;
300 &nbsp;
301 &nbsp;
302 &nbsp;
303 &nbsp;
304 &nbsp;
305 &nbsp;
306 &nbsp;
307 &nbsp;
308 &nbsp;
309 &nbsp;
310 &nbsp;
311 &nbsp;
312 &nbsp;
313 &nbsp;
314 &nbsp;
315 &nbsp;
316 &nbsp;
317 &nbsp;
318 &nbsp;
319 &nbsp;
320 &nbsp;
321 &nbsp;
322 &nbsp;
323 &nbsp;
324 &nbsp;
325 &nbsp;
326 &nbsp;
327 &nbsp;
328 &nbsp;
329 &nbsp;
330 &nbsp;
331 &nbsp;
332 &nbsp;
333 &nbsp;
334 &nbsp;
335 &nbsp;
336 &nbsp;
337 &nbsp;
338 &nbsp;
339 &nbsp;
340 &nbsp;
341 &nbsp;
342 &nbsp;
343 &nbsp;
344 &nbsp;
345 &nbsp;
346 &nbsp;
347 &nbsp;
348 &nbsp;
349 &nbsp;
350 &nbsp;
351 &nbsp;
352 &nbsp;
353 &nbsp;
354 &nbsp;
355 &nbsp;
356 &nbsp;
357 &nbsp;
358 &nbsp;
359 &nbsp;
360 &nbsp;
361 &nbsp;
362 &nbsp;
363 &nbsp;
364 &nbsp;
365 &nbsp;
366 &nbsp;
367 &nbsp;
368 &nbsp;
369 &nbsp;
370 &nbsp;
371 &nbsp;
372 &nbsp;
373 &nbsp;
374 &nbsp;
375 &nbsp;
376 &nbsp;
377 &nbsp;
378 &nbsp;
379 &nbsp;
380 &nbsp;
381 &nbsp;
382 &nbsp;
383 &nbsp;
384 &nbsp;
385 &nbsp;
386 &nbsp;
387 &nbsp;
388 &nbsp;
389 &nbsp;
390 &nbsp;
391 &nbsp;
392 &nbsp;
393 &nbsp;
394 &nbsp;
395 &nbsp;
396 &nbsp;
397 &nbsp;
398 &nbsp;
399 &nbsp;
400 &nbsp;
401 &nbsp;
402 &nbsp;
403 &nbsp;
404 &nbsp;
405 &nbsp;
406 &nbsp;
407 &nbsp;
408 &nbsp;
409 &nbsp;
410 &nbsp;
411 &nbsp;
412 &nbsp;
413 &nbsp;
414 &nbsp;
415 &nbsp;
416 &nbsp;
417 &nbsp;
418 &nbsp;
419 &nbsp;
420 &nbsp;
421 &nbsp;
422 &nbsp;
423 &nbsp;
424 &nbsp;
425 &nbsp;
426 &nbsp;
427 &nbsp;
428 &nbsp;
429 &nbsp;
430 &nbsp;
431 &nbsp;
432 &nbsp;
433 &nbsp;
434 &nbsp;
435 &nbsp;
436 &nbsp;
437 &nbsp;
438 &nbsp;
439 &nbsp;
440 &nbsp;
441 &nbsp;
442 &nbsp;
443 &nbsp;
444 &nbsp;
445 &nbsp;
446 &nbsp;
447 &nbsp;
448 &nbsp;
449 &nbsp;
450 &nbsp;
451 &nbsp;
452 &nbsp;
453 &nbsp;
454 &nbsp;
455 &nbsp;
456 &nbsp;
457 &nbsp;
458 &nbsp;
459 &nbsp;
460 &nbsp;
461 &nbsp;
462 &nbsp;
463 &nbsp;
464 &nbsp;
465 &nbsp;
466 &nbsp;
467 &nbsp;
468 &nbsp;
469 &nbsp;
470 &nbsp;
471 &nbsp;
472 &nbsp;
473 &nbsp;
474 &nbsp;
475 &nbsp;
476 &nbsp;
477 &nbsp;
478 &nbsp;
479 &nbsp;
480 &nbsp;
481 &nbsp;
482 &nbsp;
483 &nbsp;
484 &nbsp;
485 &nbsp;
486 &nbsp;
487 &nbsp;
488 &nbsp;
489 &nbsp;
490 &nbsp;
491 &nbsp;
492 &nbsp;
493 &nbsp;
494 &nbsp;
495 &nbsp;
496 &nbsp;
497 &nbsp;
498 &nbsp;
499 &nbsp;
500 &nbsp;
501 &nbsp;
502 &nbsp;
503 &nbsp;
504 &nbsp;
505 &nbsp;
506 &nbsp;
507 &nbsp;
508 &nbsp;
509 &nbsp;
510 &nbsp;
511 &nbsp;
512 &nbsp;
513 &nbsp;
514 &nbsp;
515 &nbsp;
516 &nbsp;
517 &nbsp;
518 &nbsp;
519 &nbsp;
520 &nbsp;
521 &nbsp;
522 &nbsp;
523 &nbsp;
524 &nbsp;
525 &nbsp;
526 &nbsp;
527 &nbsp;
528 &nbsp;
529 &nbsp;
530 &nbsp;
531 &nbsp;
532 &nbsp;
533 &nbsp;
534 &nbsp;
535 &nbsp;
536 &nbsp;
537 &nbsp;
538 &nbsp;
539 &nbsp;
540 &nbsp;
541 &nbsp;
542 &nbsp;
543 &nbsp;
544 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use super::tensor::DenseTensor;
</span><span class="noop">use tensors::tensor_type::TensorType;
</span><span class="noop">use super::c_api::*;
</span><span class="noop">use std::ops::*;
</span><span class="noop">use tensors::tensor_index::TensorAssign;
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; PartialEq for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn eq(&amp;self, other: &amp;DenseTensor&lt;T&gt;) -&gt; bool {
</span><span class="noop">        unsafe {
</span><span class="miss">            return cten_equal(self.tensor, other.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Eq for DenseTensor&lt;T&gt; {}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Neg for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn neg(self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_neg(self.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Neg for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn neg(self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_neg(self.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">/// ********* OpsAssign definition goes here
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; AddAssign&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn add_assign(&amp;mut self, val: f64) {
</span><span class="noop">        unsafe { cten_add_v_(self.tensor, val); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; SubAssign&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn sub_assign(&amp;mut self, val: f64) {
</span><span class="miss">        unsafe { cten_sub_v_(self.tensor, val); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; MulAssign&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn mul_assign(&amp;mut self, val: f64) {
</span><span class="miss">        unsafe { cten_mul_v_(self.tensor, val); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; DivAssign&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn div_assign(&amp;mut self, val: f64) {
</span><span class="miss">        unsafe { cten_div_v_(self.tensor, val); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; AddAssign&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn add_assign(&amp;mut self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="hit">        unsafe { cten_add_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; SubAssign&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn sub_assign(&amp;mut self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="miss">        unsafe { cten_sub_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; MulAssign&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn mul_assign(&amp;mut self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe { cten_mul_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; DivAssign&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn div_assign(&amp;mut self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe { cten_div_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; AddAssign&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn add_assign(&amp;mut self, tensor: DenseTensor&lt;T&gt;) {
</span><span class="hit">        unsafe { cten_add_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; SubAssign&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn sub_assign(&amp;mut self, tensor: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe { cten_sub_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; MulAssign&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn mul_assign(&amp;mut self, tensor: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe { cten_mul_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; DivAssign&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn div_assign(&amp;mut self, tensor: DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe { cten_div_t_(self.tensor, tensor.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">/// ********* Ops definition goes here
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, &#39;b, T: TensorType&gt; Add&lt;&amp;&#39;b DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: &amp;&#39;b DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_add_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, &#39;b, T: TensorType&gt; Sub&lt;&amp;&#39;b DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: &amp;&#39;b DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_sub_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, &#39;b, T: TensorType&gt; Mul&lt;&amp;&#39;b DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: &amp;&#39;b DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, &#39;b, T: TensorType&gt; Div&lt;&amp;&#39;b DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: &amp;&#39;b DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_div_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Add&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sub&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_sub_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Mul&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Div&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_div_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Add&lt;DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sub&lt;DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_sub_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Mul&lt;DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Div&lt;DenseTensor&lt;T&gt;&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_div_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Add&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Sub&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_sub_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Mul&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Div&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_div_t(self.tensor, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Add&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_add_v(tensor.tensor, self));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sub&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_rsub_v(tensor.tensor, self));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Mul&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(tensor.tensor, self));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Div&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_rdiv(self, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Add&lt;DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_v(tensor.tensor, self));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Sub&lt;DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_rsub_v(tensor.tensor, self));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Mul&lt;DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(tensor.tensor, self));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Div&lt;DenseTensor&lt;T&gt;&gt; for f64 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_rdiv(self, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Add&lt;f64&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_add_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sub&lt;f64&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_sub_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Mul&lt;f64&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_mul_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Div&lt;f64&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_div_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Add&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Sub&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="hit">    fn sub(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor::new(cten_sub_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Mul&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Div&lt;f64&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor::new(cten_div_v(self.tensor, val));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Add&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_v(tensor.tensor, self as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sub&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_rsub_v(tensor.tensor, self as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Mul&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(tensor.tensor, self as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Div&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: &amp;&#39;a DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_rdiv(self as f64, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Add&lt;DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_v(tensor.tensor, self as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Sub&lt;DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_rsub_v(tensor.tensor, self as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Mul&lt;DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(tensor.tensor, self as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Div&lt;DenseTensor&lt;T&gt;&gt; for f32 {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, tensor: DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_rdiv(self as f64, tensor.tensor));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Add&lt;f32&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sub&lt;f32&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_sub_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Mul&lt;f32&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Div&lt;f32&gt; for &amp;&#39;a DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_div_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Add&lt;f32&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn add(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_add_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Sub&lt;f32&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn sub(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_sub_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Mul&lt;f32&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn mul(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_mul_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">impl&lt;T: TensorType&gt; Div&lt;f32&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    type Output = DenseTensor&lt;T&gt;;
</span><span class="noop">
</span><span class="miss">    fn div(self, val: f32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor::new(cten_div_v(self.tensor, val as f64));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/dense_tensor/serializing.rs">src/tensors/dense_tensor/serializing.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std;
</span><span class="noop">use serde::ser::{Serialize, Serializer, SerializeStruct};
</span><span class="noop">use tensors::TensorType;
</span><span class="noop">use tensors::DenseTensor;
</span><span class="noop">use super::c_api::*;
</span><span class="noop">use tensors::TFloat;
</span><span class="noop">use serde::Deserialize;
</span><span class="noop">use serde::de::Visitor;
</span><span class="noop">use serde::Deserializer;
</span><span class="noop">use std::fmt;
</span><span class="noop">use serde::de;
</span><span class="noop">
</span><span class="noop">impl Serialize for DenseTensor&lt;TFloat&gt; {
</span><span class="miss">    fn serialize&lt;S&gt;(&amp;self, serializer: S) -&gt; Result&lt;S::Ok, S::Error&gt;
</span><span class="noop">        where S: Serializer {
</span><span class="noop">
</span><span class="miss">        let data = unsafe {
</span><span class="miss">            let _tensor1 = cten_contiguous(self.tensor);
</span><span class="miss">            let _tensor = cten_view1(_tensor1);
</span><span class="miss">            cten_drop_tensor(_tensor1);
</span><span class="miss">            let ptr = cten_get_data_ptr(_tensor) as *const f32;
</span><span class="miss">            std::slice::from_raw_parts(ptr, cten_numel(self.tensor) as usize)
</span><span class="noop">        };
</span><span class="noop">
</span><span class="miss">        let dtype = unsafe { cten_get_dtype(self.tensor) };
</span><span class="miss">        let shape: &amp;[i64] = self.size();
</span><span class="noop">
</span><span class="miss">        let mut state = serializer.serialize_struct(&#34;DenseTensor&#34;, 3)?;
</span><span class="miss">        state.serialize_field(&#34;dtype&#34;, &amp;dtype)?;
</span><span class="miss">        state.serialize_field(&#34;shape&#34;, &amp;shape)?;
</span><span class="miss">        state.serialize_field(&#34;data&#34;, data)?;
</span><span class="miss">        state.end()
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;de&gt; Deserialize&lt;&#39;de&gt; for DenseTensor&lt;TFloat&gt; {
</span><span class="miss">    fn deserialize&lt;D&gt;(deserializer: D) -&gt; Result&lt;DenseTensor&lt;TFloat&gt;, D::Error&gt;
</span><span class="noop">    where D: Deserializer&lt;&#39;de&gt; {
</span><span class="noop">        struct TensorVisitor;
</span><span class="noop">
</span><span class="noop">        impl&lt;&#39;de&gt; Visitor&lt;&#39;de&gt; for TensorVisitor {
</span><span class="noop">            type Value = DenseTensor&lt;TFloat&gt;;
</span><span class="noop">
</span><span class="miss">            fn expecting(&amp;self, formatter: &amp;mut fmt::Formatter) -&gt; fmt::Result {
</span><span class="miss">                formatter.write_str(&#34;struct DenseTensor&#34;)
</span><span class="noop">            }
</span><span class="noop">
</span><span class="miss">            fn visit_seq&lt;V&gt;(self, mut seq: V) -&gt; Result&lt;DenseTensor&lt;TFloat&gt;, V::Error&gt;
</span><span class="noop">                where
</span><span class="noop">                    V: de::SeqAccess&lt;&#39;de&gt;,
</span><span class="noop">            {
</span><span class="miss">                let dtype: i32 = seq.next_element()?.ok_or_else(|| de::Error::invalid_length(0, &amp;self))?;
</span><span class="miss">                assert_eq!(dtype, TFloat::get_dtype());
</span><span class="miss">                let shape: Vec&lt;i64&gt; = seq.next_element()?.ok_or_else(|| de::Error::invalid_length(1, &amp;self))?;
</span><span class="miss">                let data: Vec&lt;f32&gt; = seq.next_element()?.ok_or_else(|| de::Error::invalid_length(2, &amp;self))?;
</span><span class="noop">
</span><span class="miss">                Ok(DenseTensor::&lt;TFloat&gt;::from_ndarray(&amp;data, &amp;shape))
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        const FIELDS: &amp;&#39;static [&amp;&#39;static str] = &amp;[&#34;dtype&#34;, &#34;shape&#34;, &#34;tensor&#34;];
</span><span class="miss">        deserializer.deserialize_struct(&#34;DenseTensor&#34;, FIELDS, TensorVisitor)
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/dense_tensor/tensor.rs">src/tensors/dense_tensor/tensor.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
218 &nbsp;
219 &nbsp;
220 &nbsp;
221 &nbsp;
222 &nbsp;
223 &nbsp;
224 &nbsp;
225 &nbsp;
226 &nbsp;
227 &nbsp;
228 &nbsp;
229 &nbsp;
230 &nbsp;
231 &nbsp;
232 &nbsp;
233 &nbsp;
234 &nbsp;
235 &nbsp;
236 &nbsp;
237 &nbsp;
238 &nbsp;
239 &nbsp;
240 &nbsp;
241 &nbsp;
242 &nbsp;
243 &nbsp;
244 &nbsp;
245 &nbsp;
246 &nbsp;
247 &nbsp;
248 &nbsp;
249 &nbsp;
250 &nbsp;
251 &nbsp;
252 &nbsp;
253 &nbsp;
254 &nbsp;
255 &nbsp;
256 &nbsp;
257 &nbsp;
258 &nbsp;
259 &nbsp;
260 &nbsp;
261 &nbsp;
262 &nbsp;
263 &nbsp;
264 &nbsp;
265 &nbsp;
266 &nbsp;
267 &nbsp;
268 &nbsp;
269 &nbsp;
270 &nbsp;
271 &nbsp;
272 &nbsp;
273 &nbsp;
274 &nbsp;
275 &nbsp;
276 &nbsp;
277 &nbsp;
278 &nbsp;
279 &nbsp;
280 &nbsp;
281 &nbsp;
282 &nbsp;
283 &nbsp;
284 &nbsp;
285 &nbsp;
286 &nbsp;
287 &nbsp;
288 &nbsp;
289 &nbsp;
290 &nbsp;
291 &nbsp;
292 &nbsp;
293 &nbsp;
294 &nbsp;
295 &nbsp;
296 &nbsp;
297 &nbsp;
298 &nbsp;
299 &nbsp;
300 &nbsp;
301 &nbsp;
302 &nbsp;
303 &nbsp;
304 &nbsp;
305 &nbsp;
306 &nbsp;
307 &nbsp;
308 &nbsp;
309 &nbsp;
310 &nbsp;
311 &nbsp;
312 &nbsp;
313 &nbsp;
314 &nbsp;
315 &nbsp;
316 &nbsp;
317 &nbsp;
318 &nbsp;
319 &nbsp;
320 &nbsp;
321 &nbsp;
322 &nbsp;
323 &nbsp;
324 &nbsp;
325 &nbsp;
326 &nbsp;
327 &nbsp;
328 &nbsp;
329 &nbsp;
330 &nbsp;
331 &nbsp;
332 &nbsp;
333 &nbsp;
334 &nbsp;
335 &nbsp;
336 &nbsp;
337 &nbsp;
338 &nbsp;
339 &nbsp;
340 &nbsp;
341 &nbsp;
342 &nbsp;
343 &nbsp;
344 &nbsp;
345 &nbsp;
346 &nbsp;
347 &nbsp;
348 &nbsp;
349 &nbsp;
350 &nbsp;
351 &nbsp;
352 &nbsp;
353 &nbsp;
354 &nbsp;
355 &nbsp;
356 &nbsp;
357 &nbsp;
358 &nbsp;
359 &nbsp;
360 &nbsp;
361 &nbsp;
362 &nbsp;
363 &nbsp;
364 &nbsp;
365 &nbsp;
366 &nbsp;
367 &nbsp;
368 &nbsp;
369 &nbsp;
370 &nbsp;
371 &nbsp;
372 &nbsp;
373 &nbsp;
374 &nbsp;
375 &nbsp;
376 &nbsp;
377 &nbsp;
378 &nbsp;
379 &nbsp;
380 &nbsp;
381 &nbsp;
382 &nbsp;
383 &nbsp;
384 &nbsp;
385 &nbsp;
386 &nbsp;
387 &nbsp;
388 &nbsp;
389 &nbsp;
390 &nbsp;
391 &nbsp;
392 &nbsp;
393 &nbsp;
394 &nbsp;
395 &nbsp;
396 &nbsp;
397 &nbsp;
398 &nbsp;
399 &nbsp;
400 &nbsp;
401 &nbsp;
402 &nbsp;
403 &nbsp;
404 &nbsp;
405 &nbsp;
406 &nbsp;
407 &nbsp;
408 &nbsp;
409 &nbsp;
410 &nbsp;
411 &nbsp;
412 &nbsp;
413 &nbsp;
414 &nbsp;
415 &nbsp;
416 &nbsp;
417 &nbsp;
418 &nbsp;
419 &nbsp;
420 &nbsp;
421 &nbsp;
422 &nbsp;
423 &nbsp;
424 &nbsp;
425 &nbsp;
426 &nbsp;
427 &nbsp;
428 &nbsp;
429 &nbsp;
430 &nbsp;
431 &nbsp;
432 &nbsp;
433 &nbsp;
434 &nbsp;
435 &nbsp;
436 &nbsp;
437 &nbsp;
438 &nbsp;
439 &nbsp;
440 &nbsp;
441 &nbsp;
442 &nbsp;
443 &nbsp;
444 &nbsp;
445 &nbsp;
446 &nbsp;
447 &nbsp;
448 &nbsp;
449 &nbsp;
450 &nbsp;
451 &nbsp;
452 &nbsp;
453 &nbsp;
454 &nbsp;
455 &nbsp;
456 &nbsp;
457 &nbsp;
458 &nbsp;
459 &nbsp;
460 &nbsp;
461 &nbsp;
462 &nbsp;
463 &nbsp;
464 &nbsp;
465 &nbsp;
466 &nbsp;
467 &nbsp;
468 &nbsp;
469 &nbsp;
470 &nbsp;
471 &nbsp;
472 &nbsp;
473 &nbsp;
474 &nbsp;
475 &nbsp;
476 &nbsp;
477 &nbsp;
478 &nbsp;
479 &nbsp;
480 &nbsp;
481 &nbsp;
482 &nbsp;
483 &nbsp;
484 &nbsp;
485 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std;
</span><span class="noop">use super::c_api::*;
</span><span class="noop">use num_traits::*;
</span><span class="noop">use tensors::tensor_type::*;
</span><span class="noop">use std::marker::PhantomData;
</span><span class="noop">use std::fmt;
</span><span class="noop">use std::ffi::CStr;
</span><span class="noop">use libc::c_void;
</span><span class="noop">use std::iter::Sum;
</span><span class="noop">
</span><span class="noop">pub struct DenseTensor&lt;T: TensorType=TDefault&gt; {
</span><span class="noop">    /// A Rust wrapper for DenseTensor
</span><span class="noop">    pub(super) tensor: *const ATensor,
</span><span class="noop">    _marker: std::marker::PhantomData&lt;*const T&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">
</span><span class="noop">    #[inline(always)]
</span><span class="miss">    pub(super) fn new(tensor: *const ATensor) -&gt; DenseTensor&lt;T&gt; {
</span><span class="hit">        return DenseTensor { tensor, _marker: PhantomData };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn manual_seed(seed: u64) {
</span><span class="noop">        unsafe {
</span><span class="miss">            cten_manual_seed(seed, DEFAULT_BACKEND);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn create(shape: &amp;[i64]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_create(shape.len(), shape.as_ptr(), T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn create_randn(shape: &amp;[i64]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_create_randn(shape.len(), shape.as_ptr(), T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn zeros(shape: &amp;[i64]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_zeros(shape.len(), shape.as_ptr(), T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn ones(shape: Vec&lt;i64&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_ones(shape.len(), shape.as_ptr(), T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn zeros_like(tensor: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_zeros_like(tensor.tensor, T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn ones_like(tensor: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_ones_like(tensor.tensor, T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn from_array(data: &amp;[T::PrimitiveType]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_from_array(data.len() as i64, data.as_ptr() as *const c_void, T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn borrow_from_array(data: &amp;[T::PrimitiveType]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor::new(cten_borrow_from_array(data.len() as i64, data.as_ptr() as *const c_void, T::get_dtype(), Backend::CPU as i32));
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn from_ndarray(data: &amp;[T::PrimitiveType], shape: &amp;[i64]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_from_array_and_view(data.len() as i64, shape.len(), shape.as_ptr(), data.as_ptr() as *const c_void, T::get_dtype(), DEFAULT_BACKEND), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn stack_ref(tensors: &amp;Vec&lt;&amp;DenseTensor&lt;T&gt;&gt;, dim: i64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            let ptr: Vec&lt;*const ATensor&gt; = tensors.iter().map(|t| t.tensor).collect();
</span><span class="noop">            let tensor = cten_stack(tensors.len(), ptr.as_ptr(), dim);
</span><span class="noop">            return DenseTensor { tensor, _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn stack(tensors: &amp;Vec&lt;DenseTensor&lt;T&gt;&gt;, dim: i64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            let ptr: Vec&lt;*const ATensor&gt; = tensors.iter().map(|t| t.tensor).collect();
</span><span class="hit">            let tensor = cten_stack(tensors.len(), ptr.as_ptr(), dim);
</span><span class="hit">            return DenseTensor { tensor, _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn concat(tensors: &amp;Vec&lt;DenseTensor&lt;T&gt;&gt;, dim: i64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            let ptr: Vec&lt;*const ATensor&gt; = tensors.iter().map(|t| t.tensor).collect();
</span><span class="hit">            let tensor = cten_concat(tensors.len(), ptr.as_ptr(), dim);
</span><span class="hit">            return DenseTensor { tensor, _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn clone(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_clone(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn clone_reference(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_create_reference(self.tensor), _marker: PhantomData }
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn zero_(&amp;mut self) {
</span><span class="miss">        unsafe { cten_zero_(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    /// Copies the elements from src into self tensor.
</span><span class="miss">    pub fn copy_(&amp;self, another: &amp;DenseTensor&lt;T&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            cten_copy_(self.tensor, another.tensor, false);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    /// Sum all elements in tensor
</span><span class="miss">    pub fn sum(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_sum(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    /// Returns the sum of each row of the input tensor in the given dimension dim.
</span><span class="miss">    pub fn sum_along_dim(&amp;self, dim: i32, keepdim: bool) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_sum_along_dim(self.tensor, dim, keepdim), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn pow(&amp;self, exp: i32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_pow_tensor(self.tensor, exp), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn exp(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_exp_tensor(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn log(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_log_tensor(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn max(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_max(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn max_in_dim(&amp;self, dim: i32, keepdim: bool) -&gt; (DenseTensor&lt;T&gt;, DenseTensor&lt;TLong&gt;) {
</span><span class="noop">        unsafe {
</span><span class="hit">            let res = cten_max_in_dim(self.tensor, dim, keepdim);
</span><span class="hit">            return (
</span><span class="hit">                DenseTensor { tensor: res.first, _marker: PhantomData },
</span><span class="hit">                DenseTensor { tensor: res.second, _marker: PhantomData }
</span><span class="noop">            );
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn transpose(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_transpose(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn swap_axes(&amp;self, dim1: i64, dim2: i64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_swap_axes(self.tensor, dim1, dim2), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn dot(&amp;self, another: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_dot(self.tensor, another.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn outer(&amp;self, another: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_outer(self.tensor, another.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn matmul(&amp;self, another: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_matmul(self.tensor, another.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn mm(&amp;self, another: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_mm(self.tensor, another.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn mv(&amp;self, another: &amp;DenseTensor&lt;T&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_mv(self.tensor, another.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn expand(&amp;self, shape: &amp;Vec&lt;i64&gt;) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_expand(self.tensor, shape.len(), shape.as_ptr()), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn view(&amp;self, shape: &amp;[i64]) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_view(self.tensor, shape.len(), shape.as_ptr()), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn view_(&amp;mut self, shape: Vec&lt;i64&gt;) {
</span><span class="noop">        unsafe {
</span><span class="noop">            let _tensor = cten_view(self.tensor, shape.len(), shape.as_ptr());
</span><span class="noop">            cten_drop_tensor(self.tensor);
</span><span class="noop">            self.tensor = _tensor;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn view1(&amp;self) -&gt; Self {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_view1(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn view1_(mut self) -&gt; Self {
</span><span class="noop">        unsafe {
</span><span class="noop">            let _tensor = cten_view1(self.tensor);
</span><span class="noop">            cten_drop_tensor(self.tensor);
</span><span class="noop">            self.tensor = _tensor;
</span><span class="noop">        }
</span><span class="noop">
</span><span class="noop">        self
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn unbind(&amp;self, dim: i64) -&gt; Vec&lt;DenseTensor&lt;T&gt;&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            let dim_size = cten_size_in_dim(self.tensor, dim);
</span><span class="hit">            let mut vec = Vec::with_capacity(dim_size as usize);
</span><span class="noop">
</span><span class="hit">            for i in 0..dim_size {
</span><span class="hit">                vec.push(DenseTensor { tensor: cten_select(self.tensor, dim, i), _marker: PhantomData });
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            return vec;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn sigmoid(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_sigmoid(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn sigmoid_(&amp;self) {
</span><span class="noop">        unsafe { cten_sigmoid_(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn log_sum_exp(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_log_sum_exp(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    /// Default of dim should be 1
</span><span class="hit">    pub fn log_sum_exp_2dim(&amp;self, dim: i32) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            return DenseTensor { tensor: cten_log_sum_exp_2dim(self.tensor, dim), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cuda(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor { tensor: cten_cuda(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cuda_(&amp;mut self) {
</span><span class="noop">        unsafe {
</span><span class="noop">            let _tensor = cten_cuda(self.tensor);
</span><span class="noop">            cten_drop_tensor(self.tensor);
</span><span class="noop">            self.tensor = _tensor;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cpu(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor { tensor: cten_cpu(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn cpu_(&amp;mut self) {
</span><span class="noop">        unsafe {
</span><span class="noop">            let _tensor = cten_cpu(self.tensor);
</span><span class="noop">            cten_drop_tensor(self.tensor);
</span><span class="noop">            self.tensor = _tensor;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn is_cuda(&amp;self) -&gt; bool {
</span><span class="noop">        unsafe { return cten_is_cuda(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn is_contiguous(&amp;self) -&gt; bool {
</span><span class="miss">        unsafe { return cten_is_contiguous(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn contiguous_(&amp;mut self) -&gt; &amp;mut Self {
</span><span class="noop">        unsafe {
</span><span class="hit">            let tensor = cten_contiguous(self.tensor);
</span><span class="hit">            cten_drop_tensor(self.tensor);
</span><span class="hit">            self.tensor = tensor;
</span><span class="noop">        }
</span><span class="hit">        self
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="hit">    pub fn numel(&amp;self) -&gt; i64 {
</span><span class="hit">        unsafe { cten_numel(self.tensor) }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn size(&amp;self) -&gt; &amp;[i64] {
</span><span class="noop">        unsafe {
</span><span class="hit">            let shape = cten_size(self.tensor);
</span><span class="hit">            return std::slice::from_raw_parts(shape.dimensions, shape.ndim);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="hit">    pub fn size_in_dim(&amp;self, dim: i64) -&gt; i64 {
</span><span class="hit">        unsafe { cten_size_in_dim(self.tensor, dim) }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="noop">    #[inline]
</span><span class="miss">    pub fn ndim(&amp;self) -&gt; i64 {
</span><span class="noop">        unsafe {
</span><span class="miss">            return cten_ndim(self.tensor);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn squeeze(&amp;self) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="miss">            return DenseTensor { tensor: cten_squeeze(self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_i32(&amp;self) -&gt; i32 {
</span><span class="noop">        unsafe { return cten_get_i32(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn get_i64(&amp;self) -&gt; i64 {
</span><span class="hit">        unsafe { return cten_get_i64(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn get_f32(&amp;self) -&gt; f32 {
</span><span class="noop">        unsafe { return cten_get_f32(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn get_f64(&amp;self) -&gt; f64 {
</span><span class="hit">        unsafe { return cten_get_f64(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn rdiv(&amp;self, val: f64) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            return DenseTensor { tensor: cten_rdiv(val, self.tensor), _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn to_1darray(&amp;self) -&gt; Vec&lt;T::PrimitiveType&gt; {
</span><span class="noop">        // TODO: improve this function, currently very inefficiently
</span><span class="noop">        unsafe {
</span><span class="hit">            assert_eq!(cten_ndim(self.tensor), 1);
</span><span class="hit">            let numel = cten_numel(self.tensor);
</span><span class="noop">
</span><span class="hit">            let mut vec: Vec&lt;T::PrimitiveType&gt; = Vec::with_capacity(numel as usize);
</span><span class="hit">            for i in 0..numel {
</span><span class="hit">                vec.push(T::PrimitiveType::from_f64(cten_unsafe_select_scalar(self.tensor, i)).unwrap());
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            return vec;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn to_2darray(&amp;self) -&gt; Vec&lt;Vec&lt;T::PrimitiveType&gt;&gt; {
</span><span class="noop">        // TODO: improve this function, currently very inefficiently
</span><span class="noop">        unsafe {
</span><span class="hit">            let _tensor1 = cten_contiguous(self.tensor);
</span><span class="hit">            let _tensor = cten_view1(_tensor1);
</span><span class="hit">            cten_drop_tensor(_tensor1);
</span><span class="noop">
</span><span class="hit">            let shape = self.size();
</span><span class="hit">            let d0 = shape[0];
</span><span class="hit">            let d1 = shape[1];
</span><span class="hit">            assert_eq!(shape.len(), 2);
</span><span class="noop">
</span><span class="hit">            let mut vec: Vec&lt;Vec&lt;T::PrimitiveType&gt;&gt; = Vec::with_capacity(d0 as usize);
</span><span class="hit">            for i in 0..d0 {
</span><span class="hit">                let mut v = Vec::with_capacity(d1 as usize);
</span><span class="hit">                for j in 0..d1 {
</span><span class="hit">                    v.push(T::PrimitiveType::from_f64(cten_unsafe_select_scalar(_tensor, i * d1 + j)).unwrap());
</span><span class="noop">                }
</span><span class="hit">                vec.push(v);
</span><span class="noop">            }
</span><span class="noop">
</span><span class="hit">            cten_drop_tensor(_tensor);
</span><span class="hit">            return vec;
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; fmt::Debug for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {
</span><span class="noop">        // TODO: FIX ME!! memory leak because not cleaning C_string
</span><span class="noop">        unsafe {
</span><span class="hit">            let c_buf = cten_to_string(self.tensor);
</span><span class="hit">            let content = CStr::from_ptr(c_buf).to_str().unwrap();
</span><span class="hit">            return write!(f, &#34;{}&#34;, content);
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; fmt::Display for DenseTensor&lt;T&gt; {
</span><span class="miss">    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {
</span><span class="noop">        write!(f, &#34;DenseTensor(size={:?}, type={:?})&#34;, self.size(), T::get_scalar_type())
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Drop for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn drop(&amp;mut self) {
</span><span class="hit">        unsafe { cten_drop_tensor(self.tensor); }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Default for DenseTensor&lt;T&gt; {
</span><span class="hit">    fn default() -&gt; DenseTensor&lt;T&gt; {
</span><span class="hit">        DenseTensor::zeros(&amp;vec![0])
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;T: TensorType&gt; Sum&lt;DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    // note this function is not safe, and should be used with care, sum will
</span><span class="noop">    // only produce new tensor if you are sum more than 1 tensor, otherwise
</span><span class="noop">    // it return a new reference to the first one.
</span><span class="miss">    fn sum&lt;I: Iterator&lt;Item=DenseTensor&lt;T&gt;&gt;&gt;(iter: I) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="noop">            let ptr: Vec&lt;*const ATensor&gt; = iter.map(|t| t.tensor).collect();
</span><span class="noop">            let tensor = cten_sum_tensors(ptr.len(), ptr.as_ptr());
</span><span class="noop">            return DenseTensor { tensor, _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a, T: TensorType&gt; Sum&lt;&amp;&#39;a DenseTensor&lt;T&gt;&gt; for DenseTensor&lt;T&gt; {
</span><span class="noop">    // note this function is not safe, and should be used with care, sum will
</span><span class="noop">    // only produce new tensor if you are sum more than 1 tensor, otherwise
</span><span class="noop">    // it return a new reference to the first one.
</span><span class="hit">    fn sum&lt;I: Iterator&lt;Item=&amp;&#39;a DenseTensor&lt;T&gt;&gt;&gt;(iter: I) -&gt; DenseTensor&lt;T&gt; {
</span><span class="noop">        unsafe {
</span><span class="hit">            let ptr: Vec&lt;*const ATensor&gt; = iter.map(|t| t.tensor).collect();
</span><span class="hit">            let tensor = cten_sum_tensors(ptr.len(), ptr.as_ptr());
</span><span class="hit">            return DenseTensor { tensor, _marker: PhantomData };
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/tensor_index.rs">src/tensors/tensor_index.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
218 &nbsp;
219 &nbsp;
220 &nbsp;
221 &nbsp;
222 &nbsp;
223 &nbsp;
224 &nbsp;
225 &nbsp;
226 &nbsp;
227 &nbsp;
228 &nbsp;
229 &nbsp;
230 &nbsp;
231 &nbsp;
232 &nbsp;
233 &nbsp;
234 &nbsp;
235 &nbsp;
236 &nbsp;
237 &nbsp;
238 &nbsp;
239 &nbsp;
240 &nbsp;
241 &nbsp;
242 &nbsp;
243 &nbsp;
244 &nbsp;
245 &nbsp;
246 &nbsp;
247 &nbsp;
248 &nbsp;
249 &nbsp;
250 &nbsp;
251 &nbsp;
252 &nbsp;
253 &nbsp;
254 &nbsp;
255 &nbsp;
256 &nbsp;
257 &nbsp;
258 &nbsp;
259 &nbsp;
260 &nbsp;
261 &nbsp;
262 &nbsp;
263 &nbsp;
264 &nbsp;
265 &nbsp;
266 &nbsp;
267 &nbsp;
268 &nbsp;
269 &nbsp;
270 &nbsp;
271 &nbsp;
272 &nbsp;
273 &nbsp;
274 &nbsp;
275 &nbsp;
276 &nbsp;
277 &nbsp;
278 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use std::fmt;
</span><span class="noop">
</span><span class="noop">#[repr(C)]
</span><span class="noop">pub struct AdvancedSlice {
</span><span class="noop">    pub idx: i64,
</span><span class="noop">    pub start: i64,
</span><span class="noop">    pub end: i64,
</span><span class="noop">    pub step: i64,
</span><span class="noop">    pub is_slice: bool,
</span><span class="noop">    pub is_select_all: bool,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub struct Slice {
</span><span class="noop">    pub start: i64,
</span><span class="noop">    pub end: i64,
</span><span class="noop">    pub step: i64,
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">const MAX_END: i64 = 9223372036854775807;
</span><span class="noop">
</span><span class="noop">impl Slice {
</span><span class="miss">    pub fn from(start: i64) -&gt; Slice {
</span><span class="miss">        return Slice {
</span><span class="miss">            start,
</span><span class="noop">            end: MAX_END,
</span><span class="noop">            step: 1,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn from_wstep(start: i64, step: i64) -&gt; Slice {
</span><span class="miss">        return Slice {
</span><span class="miss">            start,
</span><span class="noop">            end: MAX_END,
</span><span class="miss">            step,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn to(end: i64) -&gt; Slice {
</span><span class="miss">        return Slice {
</span><span class="noop">            start: 0,
</span><span class="miss">            end,
</span><span class="noop">            step: 1,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn to_wstep(end: i64, step: i64) -&gt; Slice {
</span><span class="miss">        return Slice {
</span><span class="noop">            start: 0,
</span><span class="miss">            end,
</span><span class="miss">            step
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn step(step: i64) -&gt; Slice {
</span><span class="miss">        return Slice {
</span><span class="noop">            start: 0,
</span><span class="noop">            end: MAX_END,
</span><span class="miss">            step
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn between(start: i64, end: i64) -&gt; Slice {
</span><span class="miss">        return Slice {
</span><span class="miss">            start,
</span><span class="miss">            end,
</span><span class="noop">            step: 1,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn slice(start: i64, end: i64, step: i64) -&gt; Slice {
</span><span class="miss">        return Slice { start, end, step };
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl AdvancedSlice {
</span><span class="noop">
</span><span class="miss">    pub fn step(step: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="noop">            start: 0,
</span><span class="noop">            end: 0,
</span><span class="miss">            step,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        }
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn from(start: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="miss">            start,
</span><span class="noop">            end: MAX_END,
</span><span class="noop">            step: 1,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn to(end: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="noop">            start: 0,
</span><span class="miss">            end: end,
</span><span class="noop">            step: 1,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn between(start: i64, end: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="miss">            start,
</span><span class="miss">            end,
</span><span class="noop">            step: 1,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn all() -&gt; AdvancedSlice {
</span><span class="hit">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="noop">            start: 0,
</span><span class="noop">            end: 0,
</span><span class="noop">            step: 0,
</span><span class="noop">            is_slice: false,
</span><span class="noop">            is_select_all: true,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    pub fn at(idx: i64) -&gt; AdvancedSlice {
</span><span class="hit">        return AdvancedSlice {
</span><span class="hit">            idx,
</span><span class="noop">            start: 0,
</span><span class="noop">            end: 0,
</span><span class="noop">            step: 0,
</span><span class="noop">            is_slice: false,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn from_wstep(start: i64, step: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="miss">            start,
</span><span class="noop">            end: MAX_END,
</span><span class="miss">            step,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn to_wstep(end: i64, step: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="noop">            start: 0,
</span><span class="miss">            end,
</span><span class="miss">            step,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    pub fn slice(start: i64, end: i64, step: i64) -&gt; AdvancedSlice {
</span><span class="miss">        return AdvancedSlice {
</span><span class="noop">            idx: 0,
</span><span class="miss">            start,
</span><span class="miss">            end,
</span><span class="miss">            step,
</span><span class="noop">            is_slice: true,
</span><span class="noop">            is_select_all: false,
</span><span class="noop">        };
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl fmt::Debug for AdvancedSlice {
</span><span class="miss">    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {
</span><span class="miss">        if self.is_select_all {
</span><span class="miss">            return write!(f, &#34;(:)&#34;);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="miss">        if self.is_slice {
</span><span class="miss">            if self.end == MAX_END {
</span><span class="miss">                return write!(f, &#34;({}::{})&#34;, self.start, self.step);
</span><span class="noop">            }
</span><span class="noop">
</span><span class="miss">            return write!(f, &#34;({}:{}:{})&#34;, self.start, self.end, self.step);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="miss">        return write!(f, &#34;({})&#34;, self.idx);
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl fmt::Debug for Slice {
</span><span class="miss">    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {
</span><span class="miss">        if self.end == MAX_END {
</span><span class="miss">            return write!(f, &#34;({}::{})&#34;, self.start, self.step);
</span><span class="noop">        }
</span><span class="noop">
</span><span class="miss">        return write!(f, &#34;({}:{}:{})&#34;, self.start, self.end, self.step);
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait TensorIndex&lt;Idx: Sized&gt; {
</span><span class="noop">    type Output: Sized;
</span><span class="noop">    fn at(&amp;self, index: Idx) -&gt; Self::Output;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait TensorAssign&lt;Idx: Sized, Value: Sized&gt; {
</span><span class="noop">    fn assign(&amp;mut self, idx: Idx, val: Value);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[macro_export]
</span><span class="noop">macro_rules! _slice_ {
</span><span class="noop">    ($array:ident @ $v:expr) =&gt; { $array.push(AdvancedSlice::at($v)) };
</span><span class="hit">    ($array:ident @ ;) =&gt; { $array.push(AdvancedSlice::all()) };
</span><span class="noop">    ($array:ident @ ;;$s:expr) =&gt; { $array.push(AdvancedSlice::step($e)) };
</span><span class="noop">    ($array:ident @ $f:expr;) =&gt; { $array.push(AdvancedSlice::from($f)) };
</span><span class="noop">    ($array:ident @ $f:expr;;$s:expr) =&gt; { $array.push(AdvancedSlice::from_wstep($f, $s)) };
</span><span class="noop">    ($array:ident @ ;$e:expr) =&gt; { $array.push(AdvancedSlice::to($e)) };
</span><span class="noop">    ($array:ident @ ;$e:expr;$s:expr) =&gt; { $array.push(AdvancedSlice::to_wstep($e, $s)) };
</span><span class="noop">    ($array:ident @ $f:expr;$e:expr) =&gt; { $array.push(AdvancedSlice::between($f, $e)) };
</span><span class="noop">    ($array:ident @ $f:expr;$e:expr;$s:expr) =&gt; { $array.push(AdvancedSlice::slice($f, $e, $s)) };
</span><span class="noop">
</span><span class="noop">    ($array:ident @ $v:expr, $($tail:tt)+) =&gt; {
</span><span class="hit">        $array.push(AdvancedSlice::at($v));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ ;, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::all());
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ ;;$s:expr, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::step($e));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ $f:expr;, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::from($f));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ $f:expr;;$s:expr, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::from_wstep($f, $s));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ ;$e:expr, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::to($e));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ ;$e:expr;$s:expr, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::to_wstep($e, $s));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ $f:expr;$e:expr, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::between($f, $e));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">    ($array:ident @ $f:expr;$e:expr;$s:expr, $($tail:tt)+) =&gt; {
</span><span class="noop">        $array.push(AdvancedSlice::slice($f, $e, $s));
</span><span class="noop">        _slice_!($array @ $($tail)+);
</span><span class="noop">    };
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[macro_export]
</span><span class="noop">macro_rules! slice {
</span><span class="noop">    (;;$s:expr) =&gt; { Slice::step($e) };
</span><span class="noop">    ($f:expr;) =&gt; { Slice::from($f) };
</span><span class="noop">    ($f:expr;;$s:expr) =&gt; { Slice::from_wstep($f, $s) };
</span><span class="noop">    (;$e:expr) =&gt; { Slice::to($e) };
</span><span class="noop">    (;$e:expr;$s:expr) =&gt; { Slice::to_wstep($e, $s) };
</span><span class="noop">    ($f:expr;$e:expr) =&gt; { Slice::between($f, $e) };
</span><span class="noop">    ($f:expr;$e:expr;$s:expr) =&gt; { Slice::slice($f, $e, $s) };
</span><span class="noop">    ( $($tail:tt)+ ) =&gt; {{
</span><span class="hit">        let mut slices = Vec::new();
</span><span class="noop">        _slice_!(slices @ $($tail)*);
</span><span class="hit">        slices
</span><span class="miss">    }};
</span><span class="noop">}
</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/tensor_type.rs">src/tensors/tensor_type.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use num_traits::cast::FromPrimitive;
</span><span class="noop">use num_traits::cast::NumCast;
</span><span class="noop">use num_traits::Num;
</span><span class="noop">use std::fmt::Debug;
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">#[repr(C)] pub enum ScalarType {
</span><span class="noop">    Char = 0,
</span><span class="noop">    Short = 1,
</span><span class="noop">    Int = 2,
</span><span class="noop">    Long = 3,
</span><span class="noop">    Half = 4,
</span><span class="noop">    Float = 5,
</span><span class="noop">    Double = 6
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">#[repr(C)] pub enum Backend {
</span><span class="noop">    CPU = 0,
</span><span class="noop">    CUDA = 1
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">pub trait TensorType: Debug {
</span><span class="noop">    type PrimitiveType: Num + NumCast + FromPrimitive;
</span><span class="noop">
</span><span class="noop">    fn get_dtype() -&gt; i32;
</span><span class="noop">    fn get_scalar_type() -&gt; ScalarType;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">pub struct TFloat;
</span><span class="noop">impl TensorType for TFloat {
</span><span class="noop">    type PrimitiveType = f32;
</span><span class="noop">
</span><span class="hit">    fn get_dtype() -&gt; i32 { ScalarType::Float as i32 }
</span><span class="miss">    fn get_scalar_type() -&gt; ScalarType { ScalarType::Float }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">pub struct TDouble;
</span><span class="noop">impl TensorType for TDouble {
</span><span class="noop">    type PrimitiveType = f64;
</span><span class="noop">
</span><span class="hit">    fn get_dtype() -&gt; i32 { ScalarType::Double as i32 }
</span><span class="miss">    fn get_scalar_type() -&gt; ScalarType { ScalarType::Double }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">pub struct TLong;
</span><span class="noop">impl TensorType for TLong {
</span><span class="noop">    type PrimitiveType = i64;
</span><span class="noop">
</span><span class="hit">    fn get_dtype() -&gt; i32 { ScalarType::Long as i32 }
</span><span class="miss">    fn get_scalar_type() -&gt; ScalarType { ScalarType::Long }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[derive(Debug)]
</span><span class="noop">pub struct TInt;
</span><span class="noop">impl TensorType for TInt {
</span><span class="noop">    type PrimitiveType = i32;
</span><span class="noop">
</span><span class="miss">    fn get_dtype() -&gt; i32 { ScalarType::Int as i32 }
</span><span class="miss">    fn get_scalar_type() -&gt; ScalarType { ScalarType::Int }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">/// Specify default tensor type and default backend!
</span><span class="noop">pub type TDefault = TFloat;
</span><span class="noop">
</span><span class="noop">pub static mut DEFAULT_BACKEND: i32 = Backend::CPU as i32;
</span><span class="miss">pub fn set_default_backend(backend: Backend) {
</span><span class="miss">    unsafe { DEFAULT_BACKEND = backend as i32; }
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="src/tensors/utils.rs">src/tensors/utils.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="miss">pub fn unravel_index(mut ravelled_dim: i64, dims: &amp;Vec&lt;i64&gt;) -&gt; Vec&lt;i64&gt; {
</span><span class="miss">    let mut index = Vec::with_capacity(dims.len());
</span><span class="miss">    let mut sizes = vec![0; dims.len()];
</span><span class="noop">
</span><span class="miss">    sizes[dims.len() - 1] = 1;
</span><span class="miss">    for i in (0..(dims.len() - 1)).rev() {
</span><span class="miss">        sizes[i] = dims[i + 1] * sizes[i + 1];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    for i in 0..dims.len() {
</span><span class="miss">        index.push(ravelled_dim / sizes[i]);
</span><span class="miss">        ravelled_dim = ravelled_dim % sizes[i];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    index
</span><span class="noop">}
</span><span class="noop">
</span><span class="hit">pub fn unravel_index_ptr(mut ravelled_dim: i64, dims: &amp;[i64], n_dim: usize) -&gt; Vec&lt;i64&gt; {
</span><span class="hit">    let mut index = Vec::with_capacity(n_dim);
</span><span class="hit">    let mut sizes = vec![0; n_dim];
</span><span class="noop">
</span><span class="hit">    sizes[n_dim - 1] = 1;
</span><span class="hit">    for i in (0..(n_dim - 1)).rev() {
</span><span class="hit">        sizes[i] = dims[i + 1] * sizes[i + 1];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    for i in 0..n_dim {
</span><span class="hit">        index.push(ravelled_dim / sizes[i]);
</span><span class="hit">        ravelled_dim = ravelled_dim % sizes[i];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    index
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">/// Formular: index: [a1, a2, a3, ..., a_n], dims: [d1, d2, d3, ..., d_n]
</span><span class="noop">/// Output: a_n + a_(n-1) * d_(n) + a_(n-2) * d_(n) * d_(n-1) + ... + a1 * d_n * ... * d2
</span><span class="hit">pub fn ravel_index(index: &amp;Vec&lt;i64&gt;, dims: &amp;Vec&lt;i64&gt;) -&gt; i64 {
</span><span class="hit">    let mut ravelled_idx = index[index.len() - 1];
</span><span class="hit">    let mut accum_dimsize = 1;
</span><span class="noop">
</span><span class="hit">    for i in (0..(index.len() - 1)).rev() {
</span><span class="hit">        accum_dimsize *= dims[i + 1];
</span><span class="hit">        ravelled_idx += accum_dimsize * index[i];
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    ravelled_idx
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="tests/graph_models/inferences/belief_propagation.rs">tests/graph_models/inferences/belief_propagation.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use gmtk::graph_models::*;
</span><span class="noop">use gmtk::tensors::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">use graph_models::inferences::brute_force::CustomIntFactor;
</span><span class="noop">
</span><span class="hit">pub fn get_factors_and_vars&lt;F&gt;(func: F)
</span><span class="noop">    where F: for &lt;&#39;a&gt; Fn(&amp;&#39;a [IntVariable], &amp;[Box&lt;Factor&lt;&#39;a, IntVariable, TDouble&gt; + &#39;a&gt;]) -&gt; ()
</span><span class="noop">{
</span><span class="noop">    // Return list of factors (exp class: exp{w.dot(x)}) and its variable (int variable)
</span><span class="hit">    let examples = [
</span><span class="noop">    	// simplest: P(x1, x2) = 1 / Z f(x1, x2)
</span><span class="noop">        (
</span><span class="hit">        	vec![(0, 3, 2), (5, 7, 5)],
</span><span class="hit">        	vec![
</span><span class="noop">	        	(vec![0, 1], vec![1.0, 1.0]),
</span><span class="noop">	    	]
</span><span class="noop">	    ),
</span><span class="noop">	    // long chain
</span><span class="noop">	    (
</span><span class="hit">	    	vec![(0, 2, 1), (5, 8, 6), (9, 10, 9), (11, 15, 11)],
</span><span class="hit">	    	vec![
</span><span class="noop">	        	(vec![0, 1], vec![1.0, 1.0]),
</span><span class="noop">	        	(vec![1, 2], vec![1.0, 1.0]),
</span><span class="noop">	        	(vec![1, 3], vec![1.0, 1.0])
</span><span class="noop">	    	]
</span><span class="noop">	    ),
</span><span class="noop">	    // disconnect: P(x1, x2, x3, x4) = 1 / Z f(x1, x2) f(x3, x4)
</span><span class="noop">	    (
</span><span class="hit">	    	vec![(0, 3, 3), (5, 7, 7), (3, 5, 4), (9, 13, 10)],
</span><span class="hit">	    	vec![
</span><span class="noop">	        	(vec![0, 1], vec![1.0, 1.0]),
</span><span class="noop">	        	(vec![2, 3], vec![2.0, 1.0]),
</span><span class="noop">	    	]
</span><span class="noop">	    ),
</span><span class="noop">	    // factor of one variable: P(x1, x2) = 1 / Z f(x1) f(x1, x2)
</span><span class="noop">	    (
</span><span class="hit">	    	vec![(0, 3, 2), (5, 7, 5)],
</span><span class="hit">	    	vec![
</span><span class="noop">	        	(vec![0], vec![2.0]),
</span><span class="noop">	        	(vec![0, 1], vec![1.0, 1.0]),
</span><span class="noop">	    	]
</span><span class="noop">	    ),
</span><span class="noop">	    // just have more factors: P(x1, x2, x3, x4) = 1 / Z f(x1, x2) f(x3, x4) f(x1, x4)
</span><span class="noop">	    (
</span><span class="hit">	    	vec![(0, 3, 1), (5, 7, 6), (3, 5, 5), (9, 13, 12)],
</span><span class="hit">	    	vec![
</span><span class="noop">	        	(vec![0, 1], vec![1.0, 1.0]),
</span><span class="noop">	        	(vec![2, 3], vec![1.0, 1.0 / 5.0]),
</span><span class="noop">	        	(vec![0, 3], vec![2.0, -1.0])
</span><span class="noop">	    	]
</span><span class="noop">	    ),
</span><span class="noop">	    // test factors have more than 2 variables: P(x1, x2, x3, x4, x5, x6) = 1 / Z f(x1, x2, x3, x4, x5, x6)
</span><span class="noop">	    (
</span><span class="hit">	    	vec![(-1, 1, 0), (-1, 1, -1), (-1, 1, 1), (-1, 1, 0), (-1, 1, -1), (-1, 1, 1)],
</span><span class="hit">	    	vec![
</span><span class="noop">	        	(vec![0, 1, 2, 3, 4, 5], vec![1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),
</span><span class="noop">	    	]
</span><span class="noop">	    ),
</span><span class="noop">	    // factors have more than 2 variables + some factors:
</span><span class="noop">	    // P(x1, x2, x3, x4, x5, x6) = 1 / Z f(x1, x2, x3, x4, x5, x6) f(x5, x7) f(x7, x8)
</span><span class="noop">	    (
</span><span class="hit">	    	vec![(-1, 1, 1), (-1, 1, 0), (-1, 1, -1), (-1, 1, 1), (-1, 1, 0), (-1, 1, 1), (2, 3, 2), (1, 2, 2)],
</span><span class="hit">	    	vec![
</span><span class="noop">	        	(vec![0, 1, 2, 3, 4, 5], vec![1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),
</span><span class="noop">	        	(vec![4, 6], vec![2.0, 1.0]),
</span><span class="noop">	        	(vec![6, 7], vec![1.5, 3.2]),
</span><span class="noop">	    	]
</span><span class="noop">	    )
</span><span class="noop">    ];
</span><span class="noop">
</span><span class="hit">    for example in &amp;examples {
</span><span class="hit">        let vars = example.0.iter().map(|&amp;(min, max, val)| {
</span><span class="hit">            IntVariable::new(IntDomain::new(min, max), val)
</span><span class="hit">        }).collect::&lt;Vec&lt;IntVariable&gt;&gt;();
</span><span class="hit">        let ref_vars = &amp;vars;
</span><span class="noop">
</span><span class="hit">        let factors = example.1.iter().map(|&amp;(ref var_idxs, ref weights)| {
</span><span class="hit">            let vars: Vec&lt;&amp;IntVariable&gt; = var_idxs.iter().map(|&amp;i| { &amp;ref_vars[i] }).collect();
</span><span class="hit">			let x: Box&lt;Factor&lt;IntVariable, TDouble&gt;&gt; = Box::new(CustomIntFactor::new(
</span><span class="hit">                vars,
</span><span class="hit">                Weights::new(DenseTensor::&lt;TDouble&gt;::from_array(&amp;weights))
</span><span class="noop">            ));
</span><span class="hit">            x
</span><span class="hit">		}).collect::&lt;Vec&lt;Box&lt;Factor&lt;IntVariable, TDouble&gt;&gt;&gt;&gt;();
</span><span class="noop">
</span><span class="hit">        println!(&#34;ONE EXAMPLE&#34;);
</span><span class="hit">        func(ref_vars, &amp;factors);
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="hit">pub fn get_assignment_score&lt;&#39;a&gt;(assignment: &amp;FnvHashMap&lt;usize, i32&gt;, factors: &amp;[Box&lt;Factor&lt;&#39;a, IntVariable, TDouble&gt; + &#39;a&gt;]) -&gt; f64 {
</span><span class="hit">    factors.iter().map(|f| f.score_assignment(assignment)).sum()
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="hit">pub fn test_simple_acyclic_model() {
</span><span class="hit">    get_factors_and_vars(|vars, factors| {
</span><span class="hit">        let mut brute_force = BruteForce::new(vars, factors);
</span><span class="hit">        let mut bp = BeliefPropagation::new(InferProb::MARGINAL, vars, factors, 120);
</span><span class="hit">        let mut map_bp = BeliefPropagation::new(InferProb::MAP, vars, factors, 120);
</span><span class="noop">
</span><span class="hit">        brute_force.infer();
</span><span class="hit">        bp.infer();
</span><span class="hit">        map_bp.infer();
</span><span class="noop">
</span><span class="hit">        let map_sols = brute_force.all_map();
</span><span class="hit">        let bp_map_sol = map_bp.map();
</span><span class="noop">
</span><span class="hit">        let map_score = get_assignment_score(&amp;map_sols[0], factors);
</span><span class="hit">        assert!(brute_force.log_z() - bp.log_z() &lt; 1e-9);
</span><span class="hit">        println!(&#34;{:?}&#34;, bp_map_sol);
</span><span class="hit">        assert_eq!(map_score, get_assignment_score(&amp;bp_map_sol, factors));
</span><span class="noop">    });
</span><span class="noop">
</span><span class="hit">	println!(&#34;FINISH!!&#34;);
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="tests/graph_models/inferences/brute_force.rs">tests/graph_models/inferences/brute_force.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use gmtk::graph_models::*;
</span><span class="noop">use gmtk::tensors::*;
</span><span class="noop">use fnv::FnvHashMap;
</span><span class="noop">
</span><span class="noop">pub struct CustomIntFactor&lt;&#39;a&gt; {
</span><span class="noop">    variables: Vec&lt;&amp;&#39;a IntVariable&gt;,
</span><span class="noop">    weights: Weights&lt;TDouble&gt;,
</span><span class="noop">    vars_dims: Vec&lt;i64&gt;,
</span><span class="noop">    features_tensor: DenseTensor&lt;TDouble&gt;
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a&gt; CustomIntFactor&lt;&#39;a&gt; {
</span><span class="hit">    pub fn new(variables: Vec&lt;&amp;&#39;a IntVariable&gt;, weights: Weights&lt;TDouble&gt;) -&gt; CustomIntFactor&lt;&#39;a&gt; {
</span><span class="hit">        let vars_dims = variables.iter().map(|v| v.get_domain_size()).collect();
</span><span class="hit">        let mut factor = CustomIntFactor {
</span><span class="hit">            variables, weights, vars_dims,
</span><span class="hit">            features_tensor: DenseTensor::&lt;TDouble&gt;::default()
</span><span class="noop">        };
</span><span class="noop">
</span><span class="hit">        factor.update_features_tensor();
</span><span class="hit">        factor
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn update_features_tensor(&amp;mut self) {
</span><span class="hit">        let mut features_tensor = DenseTensor::&lt;TDouble&gt;::create(
</span><span class="hit">            &amp;[
</span><span class="hit">                self.vars_dims.iter().fold(1, |a, b| a * b),
</span><span class="hit">                self.weights.get_value().numel()
</span><span class="noop">            ]
</span><span class="noop">        );
</span><span class="noop">
</span><span class="hit">        iter_values(&amp;self.variables, |idx, unravel_idx, values| {
</span><span class="hit">            features_tensor.assign(idx, values.iter().map(|&amp;v| v as f64).collect::&lt;Vec&lt;f64&gt;&gt;());
</span><span class="noop">        });
</span><span class="noop">
</span><span class="hit">        self.features_tensor = features_tensor;
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a&gt; Factor&lt;&#39;a, IntVariable, TDouble&gt; for CustomIntFactor&lt;&#39;a&gt; {
</span><span class="hit">    fn get_variables(&amp;self) -&gt; &amp;[&amp;&#39;a IntVariable] {
</span><span class="hit">        &amp;self.variables
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn score_assignment(&amp;self, assignment: &amp;FnvHashMap&lt;usize, i32&gt;) -&gt; f64 {
</span><span class="hit">        self.impl_score_assignment(assignment)
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_scores_tensor(&amp;self) -&gt; DenseTensor&lt;TDouble&gt; {
</span><span class="hit">        self.impl_get_scores_tensor()
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn compute_gradients(&amp;self, target_assignment: &amp;FnvHashMap&lt;usize, i32&gt;, inference: &amp;Inference&lt;IntVariable, TDouble&gt;) -&gt; Vec&lt;(i64, DenseTensor&lt;TDouble&gt;)&gt; {
</span><span class="miss">        self.impl_compute_gradients(target_assignment, inference)
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    fn touch(&amp;self, var: &amp;IntVariable) -&gt; bool {
</span><span class="miss">        for v in &amp;self.variables {
</span><span class="miss">            if v.get_id() == var.get_id() {
</span><span class="miss">                return true;
</span><span class="noop">            }
</span><span class="noop">        }
</span><span class="noop">
</span><span class="miss">        return false;
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">impl&lt;&#39;a&gt; DotTensor1Factor&lt;&#39;a, IntVariable, TDouble&gt; for CustomIntFactor&lt;&#39;a&gt; {
</span><span class="hit">    fn get_weights(&amp;self) -&gt; &amp;Weights&lt;TDouble&gt; {
</span><span class="hit">        &amp;self.weights
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_features_tensor(&amp;self) -&gt; &amp;DenseTensor&lt;TDouble&gt; {
</span><span class="hit">        &amp;self.features_tensor
</span><span class="noop">    }
</span><span class="noop">
</span><span class="hit">    fn get_vars_dims(&amp;self) -&gt; &amp;Vec&lt;i64&gt; {
</span><span class="hit">        &amp;self.vars_dims
</span><span class="noop">    }
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="hit">pub fn test_simple_model() {
</span><span class="noop">    // Test a prob. distribution P(x1, x2) = 1 / Z f(x1, x2)
</span><span class="noop">    // f(x1, x2) = exp{x1 + x2}
</span><span class="hit">    let x1 = IntVariable::new(IntDomain::new(0, 3), 1);
</span><span class="hit">    let x2 = IntVariable::new(IntDomain::new(5, 7), 6);
</span><span class="noop">
</span><span class="hit">    let vars = vec![x1, x2];
</span><span class="hit">    let weights = Weights::new(DenseTensor::&lt;TDouble&gt;::from_array(&amp;[1.0, 1.0]));
</span><span class="hit">    let f1 = Box::new(CustomIntFactor::new(vars.iter().collect(), weights));
</span><span class="hit">    let factors: Vec&lt;Box&lt;Factor&lt;IntVariable, TDouble&gt;&gt;&gt; = vec![f1];
</span><span class="noop">
</span><span class="hit">    let mut brute_force = BruteForce::new(&amp;vars, &amp;factors);
</span><span class="hit">    brute_force.infer();
</span><span class="noop">
</span><span class="hit">    let log_z = brute_force.log_z();
</span><span class="hit">    assert_eq!(log_z, 10.847795663005575);
</span><span class="noop">
</span><span class="hit">    let prob_x1 = brute_force.marginal_log_prob(&amp;[&amp;vars[0]]);
</span><span class="hit">    assert_eq!(prob_x1.size(), &amp;[vars[0].get_domain_size()]);
</span><span class="noop">    // must be a valid distribution
</span><span class="hit">    assert!((prob_x1.log_sum_exp().get_f64() - 0.0).abs() &lt; 1e-9);
</span><span class="hit">    assert_eq!(prob_x1.to_1darray(), [-3.440189698561195, -2.440189698561195, -1.4401896985611948, -0.44018969856119483]);
</span><span class="noop">
</span><span class="hit">    let prob_x2 = brute_force.marginal_log_prob(&amp;[&amp;vars[1]]);
</span><span class="hit">    assert_eq!(prob_x2.size(), &amp;[vars[1].get_domain_size()]);
</span><span class="noop">    // must be a valid distribution
</span><span class="hit">    assert!((prob_x2.log_sum_exp().get_f64() - 0.0).abs() &lt; 1e-9);
</span><span class="hit">    assert_eq!(prob_x2.to_1darray(), [-2.4076059644443806, -1.4076059644443806, -0.4076059644443806]);
</span><span class="noop">
</span><span class="hit">    assert_eq!(brute_force.all_map(), [[(vars[0].get_id(), 3), (vars[1].get_id(), 7)]
</span><span class="noop">        .iter().cloned()
</span><span class="noop">        .collect::&lt;FnvHashMap&lt;usize, i32&gt;&gt;()]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="hit">pub fn test_long_chain_model() {
</span><span class="noop">    // Test a prob. distribution P(x1, x2, x3, x4) = 1 / Z f(x1, x2) f(x1, x3) f(x3, x4)
</span><span class="noop">    // f(x, y) = exp(x1 + x2)
</span><span class="hit">    let x1 = IntVariable::new(IntDomain::new(0, 2), 1);
</span><span class="hit">    let x2 = IntVariable::new(IntDomain::new(5, 8), 6);
</span><span class="hit">    let x3 = IntVariable::new(IntDomain::new(9, 10), 9);
</span><span class="hit">    let x4 = IntVariable::new(IntDomain::new(11, 15), 11);
</span><span class="noop">    
</span><span class="hit">    let vars = [x1, x2, x3, x4];
</span><span class="hit">    let weights = Weights::new(DenseTensor::&lt;TDouble&gt;::from_array(&amp;[1.0, 1.0]));
</span><span class="noop">
</span><span class="hit">    let f1 = Box::new(CustomIntFactor::new(vec![&amp;vars[0], &amp;vars[1]], weights.clone()));
</span><span class="hit">    let f2 = Box::new(CustomIntFactor::new(vec![&amp;vars[0], &amp;vars[2]], weights.clone()));
</span><span class="hit">    let f3 = Box::new(CustomIntFactor::new(vec![&amp;vars[2], &amp;vars[3]], weights.clone()));
</span><span class="hit">    let factors: [Box&lt;Factor&lt;IntVariable, TDouble&gt;&gt;; 3] = [f1, f2, f3];
</span><span class="noop">
</span><span class="hit">    let mut brute_force = BruteForce::new(&amp;vars, &amp;factors);
</span><span class="hit">    brute_force.infer();
</span><span class="noop">
</span><span class="hit">    let log_z = brute_force.log_z();
</span><span class="hit">    assert_eq!(log_z, 48.16196373404166);
</span><span class="noop">
</span><span class="hit">    let prob_x1 = brute_force.marginal_log_prob(&amp;[&amp;vars[0]]);
</span><span class="hit">    assert_eq!(prob_x1.size(), &amp;[vars[0].get_domain_size()]);
</span><span class="noop">    // must be a valid distribution
</span><span class="hit">    assert!((prob_x1.log_sum_exp().get_f64() - 0.0).abs() &lt; 1e-7);
</span><span class="hit">    assert_eq!(prob_x1.to_1darray(), [-4.142931628499902, -2.142931628499902, -0.14293162849990182]);
</span><span class="noop">
</span><span class="hit">    let prob_x2 = brute_force.marginal_log_prob(&amp;[&amp;vars[1]]);
</span><span class="hit">    assert_eq!(prob_x2.size(), &amp;[vars[1].get_domain_size()]);
</span><span class="noop">    // must be a valid distribution
</span><span class="hit">    assert!((prob_x2.log_sum_exp().get_f64() - 0.0).abs() &lt; 1e-7);
</span><span class="hit">    assert_eq!(prob_x2.to_1darray(), [-3.4401896985611984, -2.4401896985611984, -1.4401896985611984, -0.4401896985611984]);
</span><span class="noop">
</span><span class="hit">    assert_eq!(brute_force.all_map(), [[
</span><span class="noop">            (vars[0].get_id(), 2), (vars[1].get_id(), 8),
</span><span class="noop">            (vars[2].get_id(), 10), (vars[3].get_id(), 15)
</span><span class="noop">        ].iter().cloned()
</span><span class="noop">        .collect::&lt;FnvHashMap&lt;usize, i32&gt;&gt;()]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="miss">pub fn test_simple_cycle_model() {
</span><span class="noop">    // Test a prob. distribution P(x1, x2) = 1 / Z f(x1, x2) f(x2, x3, x4) f(x1, x4)
</span><span class="noop">    //  f(x1, x2) = exp{x1 + x2}
</span><span class="noop">    //  f(x2, x3, x4) = exp{x2 + x3 + x4 / 5}
</span><span class="noop">    //  f(x1, x4) = exp{2*x1 - x4}
</span><span class="noop">    
</span><span class="miss">    let x1 = IntVariable::new(IntDomain::new(0, 3), 2);
</span><span class="miss">    let x2 = IntVariable::new(IntDomain::new(5, 7), 7);
</span><span class="miss">    let x3 = IntVariable::new(IntDomain::new(3, 5), 3);
</span><span class="miss">    let x4 = IntVariable::new(IntDomain::new(9, 13), 10);
</span><span class="noop">
</span><span class="miss">    let vars = [x1, x2, x3, x4];
</span><span class="noop">
</span><span class="miss">    let f1 = Box::new(CustomIntFactor::new(vec![&amp;vars[0], &amp;vars[1]], Weights::new(DenseTensor::&lt;TDouble&gt;::from_array(&amp;[1.0, 1.0]))));
</span><span class="miss">    let f2 = Box::new(CustomIntFactor::new(vec![&amp;vars[1], &amp;vars[2], &amp;vars[3]], Weights::new(DenseTensor::&lt;TDouble&gt;::from_array(&amp;[1.0, 1.0, 1.0 / 5.0]))));
</span><span class="miss">    let f3 = Box::new(CustomIntFactor::new(vec![&amp;vars[0], &amp;vars[3]], Weights::new(DenseTensor::&lt;TDouble&gt;::from_array(&amp;[2.0, -1.0]))));
</span><span class="miss">    let factors: [Box&lt;Factor&lt;IntVariable, TDouble&gt;&gt;; 3] = [f1, f2, f3];
</span><span class="noop">
</span><span class="miss">    let brute_force = BruteForce::new(&amp;vars, &amp;factors);
</span><span class="miss">    let log_z = brute_force.log_z();
</span><span class="miss">    assert_eq!(log_z, 21.979732862);
</span><span class="noop">
</span><span class="miss">    for var in &amp;vars {
</span><span class="noop">        // margin with one variable
</span><span class="miss">        let prob_x = brute_force.marginal_log_prob(&amp;[var]);
</span><span class="miss">        assert_eq!(prob_x.size(), &amp;[var.get_domain_size()]);
</span><span class="miss">        assert!((prob_x.log_sum_exp().get_f64() - 0.0).abs() &lt; 1e-7);
</span><span class="noop">    }
</span><span class="noop">
</span><span class="miss">    let prob_x = brute_force.marginal_log_prob(&amp;[&amp;vars[0], &amp;vars[1]]);
</span><span class="miss">    assert_eq!(prob_x.size(), &amp;[vars[0].get_domain_size(), vars[1].get_domain_size()]);
</span><span class="miss">    assert!((prob_x.log_sum_exp().get_f64() - 0.0).abs() &lt; 1e-7);
</span><span class="noop">
</span><span class="miss">    brute_force.all_map();
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="tests/tensors/dense_tensor.rs">tests/tensors/dense_tensor.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
36 &nbsp;
37 &nbsp;
38 &nbsp;
39 &nbsp;
40 &nbsp;
41 &nbsp;
42 &nbsp;
43 &nbsp;
44 &nbsp;
45 &nbsp;
46 &nbsp;
47 &nbsp;
48 &nbsp;
49 &nbsp;
50 &nbsp;
51 &nbsp;
52 &nbsp;
53 &nbsp;
54 &nbsp;
55 &nbsp;
56 &nbsp;
57 &nbsp;
58 &nbsp;
59 &nbsp;
60 &nbsp;
61 &nbsp;
62 &nbsp;
63 &nbsp;
64 &nbsp;
65 &nbsp;
66 &nbsp;
67 &nbsp;
68 &nbsp;
69 &nbsp;
70 &nbsp;
71 &nbsp;
72 &nbsp;
73 &nbsp;
74 &nbsp;
75 &nbsp;
76 &nbsp;
77 &nbsp;
78 &nbsp;
79 &nbsp;
80 &nbsp;
81 &nbsp;
82 &nbsp;
83 &nbsp;
84 &nbsp;
85 &nbsp;
86 &nbsp;
87 &nbsp;
88 &nbsp;
89 &nbsp;
90 &nbsp;
91 &nbsp;
92 &nbsp;
93 &nbsp;
94 &nbsp;
95 &nbsp;
96 &nbsp;
97 &nbsp;
98 &nbsp;
99 &nbsp;
100 &nbsp;
101 &nbsp;
102 &nbsp;
103 &nbsp;
104 &nbsp;
105 &nbsp;
106 &nbsp;
107 &nbsp;
108 &nbsp;
109 &nbsp;
110 &nbsp;
111 &nbsp;
112 &nbsp;
113 &nbsp;
114 &nbsp;
115 &nbsp;
116 &nbsp;
117 &nbsp;
118 &nbsp;
119 &nbsp;
120 &nbsp;
121 &nbsp;
122 &nbsp;
123 &nbsp;
124 &nbsp;
125 &nbsp;
126 &nbsp;
127 &nbsp;
128 &nbsp;
129 &nbsp;
130 &nbsp;
131 &nbsp;
132 &nbsp;
133 &nbsp;
134 &nbsp;
135 &nbsp;
136 &nbsp;
137 &nbsp;
138 &nbsp;
139 &nbsp;
140 &nbsp;
141 &nbsp;
142 &nbsp;
143 &nbsp;
144 &nbsp;
145 &nbsp;
146 &nbsp;
147 &nbsp;
148 &nbsp;
149 &nbsp;
150 &nbsp;
151 &nbsp;
152 &nbsp;
153 &nbsp;
154 &nbsp;
155 &nbsp;
156 &nbsp;
157 &nbsp;
158 &nbsp;
159 &nbsp;
160 &nbsp;
161 &nbsp;
162 &nbsp;
163 &nbsp;
164 &nbsp;
165 &nbsp;
166 &nbsp;
167 &nbsp;
168 &nbsp;
169 &nbsp;
170 &nbsp;
171 &nbsp;
172 &nbsp;
173 &nbsp;
174 &nbsp;
175 &nbsp;
176 &nbsp;
177 &nbsp;
178 &nbsp;
179 &nbsp;
180 &nbsp;
181 &nbsp;
182 &nbsp;
183 &nbsp;
184 &nbsp;
185 &nbsp;
186 &nbsp;
187 &nbsp;
188 &nbsp;
189 &nbsp;
190 &nbsp;
191 &nbsp;
192 &nbsp;
193 &nbsp;
194 &nbsp;
195 &nbsp;
196 &nbsp;
197 &nbsp;
198 &nbsp;
199 &nbsp;
200 &nbsp;
201 &nbsp;
202 &nbsp;
203 &nbsp;
204 &nbsp;
205 &nbsp;
206 &nbsp;
207 &nbsp;
208 &nbsp;
209 &nbsp;
210 &nbsp;
211 &nbsp;
212 &nbsp;
213 &nbsp;
214 &nbsp;
215 &nbsp;
216 &nbsp;
217 &nbsp;
218 &nbsp;
219 &nbsp;
220 &nbsp;
221 &nbsp;
222 &nbsp;
223 &nbsp;
224 &nbsp;
225 &nbsp;
226 &nbsp;
227 &nbsp;
228 &nbsp;
229 &nbsp;
230 &nbsp;
231 &nbsp;
232 &nbsp;
233 &nbsp;
234 &nbsp;
235 &nbsp;
236 &nbsp;
237 &nbsp;
238 &nbsp;
239 &nbsp;
240 &nbsp;
241 &nbsp;
242 &nbsp;
243 &nbsp;
244 &nbsp;
245 &nbsp;
246 &nbsp;
247 &nbsp;
248 &nbsp;
249 &nbsp;
250 &nbsp;
251 &nbsp;
252 &nbsp;
253 &nbsp;
254 &nbsp;
255 &nbsp;
256 &nbsp;
257 &nbsp;
258 &nbsp;
259 &nbsp;
260 &nbsp;
261 &nbsp;
262 &nbsp;
263 &nbsp;
264 &nbsp;
265 &nbsp;
266 &nbsp;
267 &nbsp;
268 &nbsp;
269 &nbsp;
270 &nbsp;
271 &nbsp;
272 &nbsp;
273 &nbsp;
274 &nbsp;
275 &nbsp;
276 &nbsp;
277 &nbsp;
278 &nbsp;
279 &nbsp;
280 &nbsp;
281 &nbsp;
282 &nbsp;
283 &nbsp;
284 &nbsp;
285 &nbsp;
286 &nbsp;
287 &nbsp;
288 &nbsp;
289 &nbsp;
290 &nbsp;
291 &nbsp;
292 &nbsp;
293 &nbsp;
294 &nbsp;
295 &nbsp;
296 &nbsp;
297 &nbsp;
298 &nbsp;
299 &nbsp;
300 &nbsp;
301 &nbsp;
302 &nbsp;
303 &nbsp;
304 &nbsp;
305 &nbsp;
306 &nbsp;
307 &nbsp;
308 &nbsp;
309 &nbsp;
310 &nbsp;
311 &nbsp;
312 &nbsp;
313 &nbsp;
314 &nbsp;
315 &nbsp;
316 &nbsp;
317 &nbsp;
318 &nbsp;
319 &nbsp;
320 &nbsp;
321 &nbsp;
322 &nbsp;
323 &nbsp;
324 &nbsp;
325 &nbsp;
326 &nbsp;
327 &nbsp;
328 &nbsp;
329 &nbsp;
330 &nbsp;
331 &nbsp;
332 &nbsp;
333 &nbsp;
334 &nbsp;
335 &nbsp;
336 &nbsp;
337 &nbsp;
338 &nbsp;
339 &nbsp;
340 &nbsp;
341 &nbsp;
342 &nbsp;
343 &nbsp;
344 &nbsp;
345 &nbsp;
346 &nbsp;
347 &nbsp;
348 &nbsp;
349 &nbsp;
350 &nbsp;
351 &nbsp;
352 &nbsp;
353 &nbsp;
354 &nbsp;
355 &nbsp;
356 &nbsp;
357 &nbsp;
358 &nbsp;
359 &nbsp;
360 &nbsp;
361 &nbsp;
362 &nbsp;
363 &nbsp;
364 &nbsp;
365 &nbsp;
366 &nbsp;
367 &nbsp;
368 &nbsp;
369 &nbsp;
370 &nbsp;
371 &nbsp;
372 &nbsp;
373 &nbsp;
374 &nbsp;
375 &nbsp;
376 &nbsp;
377 &nbsp;
378 &nbsp;
379 &nbsp;
380 &nbsp;
381 &nbsp;
382 &nbsp;
383 &nbsp;
384 &nbsp;
385 &nbsp;
386 &nbsp;
387 &nbsp;
388 &nbsp;
389 &nbsp;
390 &nbsp;
391 &nbsp;
392 &nbsp;
393 &nbsp;
394 &nbsp;
395 &nbsp;
396 &nbsp;
397 &nbsp;
398 &nbsp;
399 &nbsp;
400 &nbsp;
401 &nbsp;
402 &nbsp;
403 &nbsp;
404 &nbsp;
405 &nbsp;
406 &nbsp;
407 &nbsp;
408 &nbsp;
409 &nbsp;
410 &nbsp;
411 &nbsp;
412 &nbsp;
413 &nbsp;
414 &nbsp;
415 &nbsp;
416 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use gmtk::tensors::*;
</span><span class="noop">use bincode;
</span><span class="noop">
</span><span class="noop">/// Premises:
</span><span class="noop">///     + from_array &amp; from_ndarray return correct tensors
</span><span class="noop">///     + to_array &amp; to_2d_array: return correct value content of a tensor (value content need to have correct type!!)
</span><span class="noop">///     + size: return correct dimensions of a tensor
</span><span class="noop">#[test]
</span><span class="hit">fn premise() {
</span><span class="hit">    let tensor = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);
</span><span class="hit">    assert_eq!(tensor.size(), vec![6].as_slice());
</span><span class="hit">    assert_eq!(tensor.to_1darray(), vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);
</span><span class="noop">
</span><span class="hit">    let tensor = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[2, 3]);
</span><span class="hit">    assert_eq!(tensor.size(), vec![2, 3].as_slice());
</span><span class="hit">    assert_eq!(tensor.to_2darray(), vec![vec![1.0, 2.0, 3.0], vec![4.0, 5.0, 6.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_create() {
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::create(&amp;vec![5, 30, 200]);
</span><span class="miss">    assert_eq!(tensor.size(), vec![5, 30, 200].as_slice());
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_create_randn() {
</span><span class="miss">    DenseTensor::&lt;TDefault&gt;::manual_seed(120);
</span><span class="noop">
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::create_randn(&amp;[2, 3]);
</span><span class="miss">    assert_eq!(tensor.size(), vec![2, 3].as_slice());
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![
</span><span class="noop">        vec![-1.1915583610534668, 0.3627992570400238, -0.37679386138916016],
</span><span class="noop">        vec![0.5234138369560242, -0.7340877652168274, -0.6237930059432983]
</span><span class="noop">    ]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_zeros() {
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::zeros(&amp;vec![2, 3]);
</span><span class="miss">    assert_eq!(tensor.size(), vec![2, 3].as_slice());
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![vec![0.0, 0.0, 0.0], vec![0.0, 0.0, 0.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_ones() {
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::ones(vec![2, 3]);
</span><span class="miss">    assert_eq!(tensor.size(), vec![2, 3].as_slice());
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![vec![1.0, 1.0, 1.0], vec![1.0, 1.0, 1.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_zeros_like() {
</span><span class="miss">    let aten = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 6.0, 7.0], &amp;[3, 2]);
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::zeros_like(&amp;aten);
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![vec![0.0, 0.0], vec![0.0, 0.0], vec![0.0, 0.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_ones_like() {
</span><span class="miss">    let aten = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 6.0, 7.0], &amp;[3, 2]);
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::ones_like(&amp;aten);
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![vec![1.0, 1.0], vec![1.0, 1.0], vec![1.0, 1.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_stack() {
</span><span class="miss">    let tensors = vec![
</span><span class="miss">        DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 3.0, 4.0]),
</span><span class="miss">        DenseTensor::&lt;TDefault&gt;::from_array(&amp;[5.0, 7.0, 9.0])];
</span><span class="noop">
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::stack(&amp;tensors, 0);
</span><span class="miss">    assert_eq!(tensor.size(), vec![2, 3].as_slice());
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![
</span><span class="noop">        vec![2.0, 3.0, 4.0],
</span><span class="noop">        vec![5.0, 7.0, 9.0]
</span><span class="noop">    ]);
</span><span class="noop">
</span><span class="miss">    let tensor = DenseTensor::&lt;TDefault&gt;::stack(&amp;tensors, 1);
</span><span class="miss">    assert_eq!(tensor.size(), vec![3, 2].as_slice());
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![
</span><span class="noop">        vec![2.0, 5.0], vec![3.0, 7.0], vec![4.0, 9.0]
</span><span class="noop">    ]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_concat() {
</span><span class="miss">    let vals = vec![DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 3.0, 4.0]), DenseTensor::&lt;TDefault&gt;::from_array(&amp;[5.0, 7.0, 9.0])];
</span><span class="miss">    assert_eq!(DenseTensor::&lt;TDefault&gt;::concat(&amp;vals, 0).to_1darray(), vec![2.0, 3.0, 4.0, 5.0, 7.0, 9.0]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_zero_() {
</span><span class="miss">    let mut tensor = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(tensor.size(), vec![3, 2].as_slice());
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![vec![1.0, 2.0], vec![3.0, 4.0], vec![5.0, 6.0]]);
</span><span class="miss">    tensor.zero_();
</span><span class="miss">    assert_eq!(tensor.to_2darray(), vec![vec![0.0, 0.0], vec![0.0, 0.0], vec![0.0, 0.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_equal() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.1], &amp;[3, 2]);
</span><span class="miss">    assert_ne!(v, u);
</span><span class="miss">    let u2 = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v, u2);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_clone() {
</span><span class="miss">    let mut v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    let u = v.clone();
</span><span class="miss">    assert_ne!(&amp;v as *const _, &amp;u as *const _);
</span><span class="miss">    assert_eq!(v, u);
</span><span class="miss">    v.zero_();
</span><span class="miss">    assert_eq!(u.to_2darray(), vec![vec![1.0, 2.0], vec![3.0, 4.0], vec![5.0, 6.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_sum() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.sum().get_f64(), 21.0);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_sum_along_dim() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.sum_along_dim(1, false), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[3.0, 7.0, 11.0], &amp;[3]));
</span><span class="miss">    assert_eq!(v.sum_along_dim(0, false), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[9.0, 12.0], &amp;[2]));
</span><span class="miss">    assert_eq!(v.sum_along_dim(1, true), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[3.0, 7.0, 11.0], &amp;[3, 1]));
</span><span class="miss">    assert_eq!(v.sum_along_dim(0, true), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[9.0, 12.0], &amp;[1, 2]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_pow() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 3.0]);
</span><span class="miss">    assert_eq!(v.pow(2), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[4.0, 9.0], &amp;[2]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_exp() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 3.0]);
</span><span class="miss">    assert_eq!(v.exp(), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0_f32.exp(), 3.0_f32.exp()], &amp;[2]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_log() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 3.0]);
</span><span class="miss">    assert_eq!(v.exp().log(), v);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_max() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 9.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.max().get_f64(), 9.0);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_max_in_dim() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 2.0, 3.0, 9.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    let (max_val, idx_val) = v.max_in_dim(1, false);
</span><span class="miss">    assert_eq!(max_val, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 9.0, 6.0], &amp;[3]));
</span><span class="miss">    assert_eq!(idx_val, DenseTensor::&lt;TLong&gt;::from_ndarray(&amp;[0, 1, 1], &amp;[3]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_swap_axes() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 2.0, 3.0, 9.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.swap_axes(0, 1).to_2darray(), vec![vec![5.0, 3.0, 5.0], vec![2.0, 9.0, 6.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_transpose() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 2.0, 3.0, 9.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.transpose().to_2darray(), vec![vec![5.0, 3.0, 5.0], vec![2.0, 9.0, 6.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_dot() {
</span><span class="miss">    assert_eq!(DenseTensor::&lt;TDefault&gt;::from_array(&amp;[3.0, 2.0, 4.0]).dot(
</span><span class="miss">        &amp;DenseTensor::&lt;TDefault&gt;::from_array(&amp;[1.0, 2.0, 0.5])).get_f64(), 9.0);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_outer() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 3.0]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[4.0, 2.0]);
</span><span class="miss">    assert_eq!(v.outer(&amp;u).to_2darray(), vec![
</span><span class="noop">        vec![8.0, 4.0], vec![12.0, 6.0]
</span><span class="noop">    ]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_matmul() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[3.0, 2.0, 4.0]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_array(&amp;[1.0, 2.0, 0.5]);
</span><span class="noop">    // both are one dim
</span><span class="miss">    assert_eq!(v.matmul(&amp;u).get_f64(), 9.0);
</span><span class="noop">
</span><span class="noop">    // both are 2-dim
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[2, 3]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.matmul(&amp;u), v.mm(&amp;u));
</span><span class="noop">
</span><span class="noop">    // TODO: add more test cases
</span><span class="noop">    //    + one of them is 1-dim
</span><span class="noop">    //    + at least one is 1-dim and at least one is N-dim
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_mm() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[2, 3]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.mm(&amp;u).to_2darray(), vec![vec![31.0, 40.0], vec![71.0, 92.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_mv() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[2, 3]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 4.0], &amp;[3]);
</span><span class="miss">    assert_eq!(v.mv(&amp;u), DenseTensor::&lt;TDefault&gt;::from_array(&amp;[24.0, 55.0]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_expand() {
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[4.0, 9.0], &amp;[2, 1]);
</span><span class="miss">    let v = u.expand(&amp;vec![2, 2]);
</span><span class="miss">    assert_eq!(v.to_2darray(), vec![vec![4.0, 4.0], vec![9.0, 9.0]]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_view() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[2, 3]);
</span><span class="miss">    assert_eq!(v.view(&amp;vec![-1, 2]), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[3, 2]));
</span><span class="miss">    assert_eq!(v.view(&amp;vec![-1]),  DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[6]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_unbind() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0, 5.0, 7.0, 9.0], &amp;[2, 3]);
</span><span class="miss">    assert_eq!(v.unbind(1), vec![
</span><span class="noop">        DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 5.0], &amp;[2]),
</span><span class="noop">        DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[3.0, 7.0], &amp;[2]),
</span><span class="noop">        DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[4.0, 9.0], &amp;[2]),
</span><span class="noop">    ]);
</span><span class="miss">    assert_eq!(v.unbind(0), vec![
</span><span class="noop">        DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0], &amp;[3]),
</span><span class="noop">        DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 7.0, 9.0], &amp;[3])
</span><span class="noop">    ]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_sigmoid() {
</span><span class="miss">    let sigmoid_func = |x: f64| 1.0 / (1.0 + (-x).exp());
</span><span class="miss">    let u = DenseTensor::&lt;TDouble&gt;::from_ndarray(&amp;[1.0, 0.5], &amp;[2]);
</span><span class="miss">    assert_eq!(u.sigmoid(), DenseTensor::&lt;TDouble&gt;::from_ndarray(&amp;[sigmoid_func(1.0), sigmoid_func(0.5)], &amp;[2]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_log_sum_exp() {
</span><span class="miss">    let v = DenseTensor::&lt;TDouble&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0], &amp;[3]);
</span><span class="miss">    assert_eq!(v.log_sum_exp().get_f64(), 3.4076059644443806);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_log_sum_exp_2dim() {
</span><span class="miss">    let v = DenseTensor::&lt;TDouble&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 1.0, 2.0, 3.0], &amp;[2, 3]);
</span><span class="miss">    assert_eq!(v.log_sum_exp_2dim(1).to_1darray(), vec![3.4076059644443806, 3.4076059644443806]);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_contiguous() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 2.0, 3.0, 9.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert!(!v.transpose().is_contiguous());
</span><span class="miss">    v.transpose().contiguous_();
</span><span class="miss">    assert!(v.is_contiguous());
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_ndim() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 2.0, 3.0, 9.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.size().len(), v.ndim() as usize);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_squeeze() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0], &amp;[1, 3]);
</span><span class="miss">    assert_eq!(v.squeeze(), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0], &amp;[3]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_operators() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0, 3.0, 4.0], &amp;[3]);
</span><span class="miss">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0, 7.0, 9.0], &amp;[3]);
</span><span class="noop">
</span><span class="noop">    // test operators: +, -, *, /
</span><span class="miss">    assert_eq!(&amp;v + &amp;u, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[7.0, 10.0, 13.0], &amp;[3]));
</span><span class="miss">    assert_eq!(&amp;v + 5.0, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[7.0, 8.0, 9.0], &amp;[3]));
</span><span class="miss">    assert_eq!(5.0 + &amp;v, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[7.0, 8.0, 9.0], &amp;[3]));
</span><span class="miss">    assert_eq!(-&amp;v, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[-2.0, -3.0, -4.0], &amp;[3]));
</span><span class="miss">    assert_eq!(&amp;v - &amp;u, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[-3.0, -4.0, -5.0], &amp;[3]));
</span><span class="miss">    assert_eq!(&amp;v - 5.0, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[-3.0, -2.0, -1.0], &amp;[3]));
</span><span class="miss">    assert_eq!(5.0 - &amp;v, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[3.0, 2.0, 1.0], &amp;[3]));
</span><span class="miss">    assert_eq!(&amp;v * 2.0, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[4.0, 6.0, 8.0], &amp;[3]));
</span><span class="miss">    assert_eq!(&amp;v * 2.0 / 2.0, v);
</span><span class="miss">    assert_eq!(&amp;u / &amp;v, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[5.0 / 2.0, 7.0 / 3.0, 9.0 / 4.0], &amp;[3]));
</span><span class="miss">    assert_eq!(2.0 / &amp;v, DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[2.0 / 2.0, 2.0 / 3.0, 2.0 / 4.0], &amp;[3]));
</span><span class="noop">
</span><span class="noop">    // test operator +=, -=, *=, /=
</span><span class="miss">    let mut k = DenseTensor::&lt;TDefault&gt;::zeros_like(&amp;v);
</span><span class="miss">    k += &amp;v;
</span><span class="miss">    k += &amp;u;
</span><span class="miss">    assert_eq!(k, &amp;v + &amp;u);
</span><span class="miss">    k -= &amp;u;
</span><span class="miss">    assert_eq!(k, v);
</span><span class="miss">    k *= 2.0;
</span><span class="miss">    assert_eq!(k, &amp;v * 2.0);
</span><span class="miss">    k /= 2.0;
</span><span class="miss">    assert_eq!(k, v);
</span><span class="miss">    k -= 2.0;
</span><span class="miss">    assert_eq!(k, &amp;v - 2.0);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_indexing() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    assert_eq!(v.at(1), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[3.0, 4.0], &amp;[2]));
</span><span class="miss">    assert_eq!(v.at((1, 1)).get_f64(), 4.0);
</span><span class="noop">
</span><span class="noop">    // test slice
</span><span class="miss">    assert_eq!(v.at(slice![2;3]).to_2darray(), vec![vec![5.0, 6.0]]);
</span><span class="miss">    assert_eq!(v.at(slice![2;3, 1;2]).to_2darray(), vec![vec![6.0]]);
</span><span class="miss">    assert_eq!(v.at(slice![0;3;2, ;]).to_2darray(), vec![vec![1.0, 2.0], vec![5.0, 6.0]]);
</span><span class="miss">    assert_eq!(v.at(slice![0;-1;1, ;]), v.at(slice![0;2;1, ;]));
</span><span class="miss">    assert_eq!(v.at(slice![0;;2, ;]), v.at(slice![0;3;2, ;]));
</span><span class="noop">
</span><span class="noop">    // test mixed
</span><span class="miss">    assert_eq!(v.at(slice![2, 1;2]), DenseTensor::&lt;TDefault&gt;::from_array(&amp;[6.0]));
</span><span class="miss">    assert_eq!(v.at(slice![1;3, 1]), DenseTensor::&lt;TDefault&gt;::from_array(&amp;[4.0, 6.0]));
</span><span class="miss">    assert_eq!(v.at(slice![0;3;2, 0]), DenseTensor::&lt;TDefault&gt;::from_array(&amp;[1.0, 5.0]));
</span><span class="miss">    assert_eq!(v.at(slice![0;3;2, 1]), DenseTensor::&lt;TDefault&gt;::from_array(&amp;[2.0, 6.0]));
</span><span class="noop">
</span><span class="noop">    // test list of values
</span><span class="miss">    assert_eq!(v.at(&amp;vec![1, 0]), DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[3.0, 4.0, 1.0, 2.0], &amp;[2, 2]));
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="hit">fn test_assign_indexing() {
</span><span class="hit">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="hit">    let u = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[8.0, 3.0, 4.0, 1.0, 9.0, 2.0], &amp;[3, 2]);
</span><span class="noop">
</span><span class="noop">    // test assign to single value or broadcast
</span><span class="hit">    let mut mv = v.clone();
</span><span class="hit">    mv.assign((0, 0), 5.0);
</span><span class="hit">    assert_eq!(mv.to_2darray(), vec![vec![5.0, 2.0], vec![3.0, 4.0], vec![5.0, 6.0]]);
</span><span class="hit">    mv.assign((1, -1), 11.0);
</span><span class="hit">    assert_eq!(mv.to_2darray(), vec![vec![5.0, 2.0], vec![3.0, 11.0], vec![5.0, 6.0]]);
</span><span class="hit">    mv.assign(0, 5.0);
</span><span class="hit">    assert_eq!(mv.to_2darray(), vec![vec![5.0, 5.0], vec![3.0, 11.0], vec![5.0, 6.0]]);
</span><span class="hit">    mv.assign(0, u.at(0));
</span><span class="hit">    assert_eq!(mv.to_2darray(), vec![vec![8.0, 3.0], vec![3.0, 11.0], vec![5.0, 6.0]]);
</span><span class="hit">    mv.assign(0, &amp;v.at(0));
</span><span class="hit">    assert_eq!(mv.to_2darray(), vec![vec![1.0, 2.0], vec![3.0, 11.0], vec![5.0, 6.0]]);
</span><span class="hit">    mv.assign((0, 0), u.at((0, 0)));
</span><span class="hit">    assert_eq!(mv.to_2darray(), vec![vec![8.0, 2.0], vec![3.0, 11.0], vec![5.0, 6.0]]);
</span><span class="noop">
</span><span class="noop">    // test assign to a vector of values
</span><span class="hit">    let mut mv = v.clone();
</span><span class="hit">    mv.assign(&amp;vec![0], u.at(0));
</span><span class="miss">    assert_eq!(mv.to_2darray(), vec![vec![8.0, 3.0], vec![3.0, 4.0], vec![5.0, 6.0]]);
</span><span class="noop">
</span><span class="noop">    // test assign to slice
</span><span class="miss">    let mut mv = v.clone();
</span><span class="miss">    mv.assign(slice![0;3;2, ;], 5.0);
</span><span class="miss">    assert_eq!(mv.to_2darray(), vec![vec![5.0, 5.0], vec![3.0, 4.0], vec![5.0, 5.0]]);
</span><span class="miss">    mv.assign(slice![0;3;2, ;], u.at(slice![0;3;2, ;]));
</span><span class="miss">    assert_eq!(mv.to_2darray(), vec![vec![8.0, 3.0], vec![3.0, 4.0], vec![9.0, 2.0]]);
</span><span class="noop">
</span><span class="noop">    // test assign mix
</span><span class="miss">    let mut mv = v.clone();
</span><span class="miss">    mv.assign(slice![0;3;2, 1], 100.0);
</span><span class="miss">    assert_eq!(mv.to_2darray(), vec![vec![1.0, 100.0], vec![3.0, 4.0], vec![5.0, 100.0]]);
</span><span class="noop">
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_pickling() {
</span><span class="miss">    let v = DenseTensor::&lt;TDefault&gt;::from_ndarray(&amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], &amp;[3, 2]);
</span><span class="miss">    let dumped_v: Vec&lt;u8&gt; = bincode::serialize(&amp;v).unwrap();
</span><span class="noop">
</span><span class="miss">    let u = bincode::deserialize(&amp;dumped_v).unwrap();
</span><span class="miss">    assert_ne!(&amp;v as *const _, &amp;u as *const _);
</span><span class="miss">    assert_eq!(v, u);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">
</span></pre>
      </td>
    </tr>
  </tbody>
</table>
<h4 id="tests/tensors/utils.rs">tests/tensors/utils.rs</h4>
<table class="code u-max-full-width">
  <tbody>
    <tr>
      <td class="lineno">
        <pre>1 &nbsp;
2 &nbsp;
3 &nbsp;
4 &nbsp;
5 &nbsp;
6 &nbsp;
7 &nbsp;
8 &nbsp;
9 &nbsp;
10 &nbsp;
11 &nbsp;
12 &nbsp;
13 &nbsp;
14 &nbsp;
15 &nbsp;
16 &nbsp;
17 &nbsp;
18 &nbsp;
19 &nbsp;
20 &nbsp;
21 &nbsp;
22 &nbsp;
23 &nbsp;
24 &nbsp;
25 &nbsp;
26 &nbsp;
27 &nbsp;
28 &nbsp;
29 &nbsp;
30 &nbsp;
31 &nbsp;
32 &nbsp;
33 &nbsp;
34 &nbsp;
35 &nbsp;
</pre>
      </td>
      <td class="source">
        <pre><span class="noop">use gmtk::tensors::utils::*;
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="noop">#[allow(unused_mut)]
</span><span class="miss">fn test_unravel_index() {
</span><span class="miss">    assert_eq!(unravel_index(0, &amp;vec![7]), vec![0]);
</span><span class="miss">    assert_eq!(unravel_index(5, &amp;vec![7]), vec![5]);
</span><span class="miss">    assert_eq!(unravel_index(0, &amp;vec![7, 6]), vec![0, 0]);
</span><span class="miss">    assert_eq!(unravel_index(22, &amp;vec![7, 6]), vec![3, 4]);
</span><span class="miss">    assert_eq!(unravel_index(41, &amp;vec![7, 6]), vec![6, 5]);
</span><span class="miss">    assert_eq!(unravel_index(37, &amp;vec![7, 6]), vec![6, 1]);
</span><span class="miss">    assert_eq!(unravel_index(41, &amp;vec![5, 7, 3]), vec![1, 6, 2]);
</span><span class="miss">    assert_eq!(unravel_index(20, &amp;vec![5, 7, 3]), vec![0, 6, 2]);
</span><span class="miss">    assert_eq!(unravel_index(10, &amp;vec![5, 7, 3]), vec![0, 3, 1]);
</span><span class="miss">    assert_eq!(unravel_index(0, &amp;vec![5, 7, 3]), vec![0, 0, 0]);
</span><span class="noop">
</span><span class="noop">    // test passed variables won&#39;t change its value
</span><span class="miss">    let mut idx = 37;
</span><span class="miss">    assert_eq!(unravel_index(idx, &amp;vec![7, 6]), vec![6, 1]);
</span><span class="miss">    assert_eq!(idx, 37);
</span><span class="noop">}
</span><span class="noop">
</span><span class="noop">#[test]
</span><span class="miss">fn test_ravel_index() {
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![0], &amp;vec![7]), 0);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![5], &amp;vec![7]), 5);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![0, 0], &amp;vec![7, 6]), 0);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![3, 4], &amp;vec![7, 6]), 22);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![6, 5], &amp;vec![7, 6]), 41);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![6, 1], &amp;vec![7, 6]), 37);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![1, 6, 2], &amp;vec![5, 7, 3]), 41);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![0, 6, 2], &amp;vec![5, 7, 3]), 20);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![0, 3, 1], &amp;vec![5, 7, 3]), 10);
</span><span class="miss">    assert_eq!(ravel_index(&amp;vec![0, 0, 0], &amp;vec![5, 7, 3]), 0);
</span><span class="noop">}</span></pre>
      </td>
    </tr>
  </tbody>
</table>
    </div>
  </body>
</html>